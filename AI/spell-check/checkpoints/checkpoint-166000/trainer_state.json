{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9944574977112937,
  "eval_steps": 2000,
  "global_step": 166000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.345078266987161e-05,
      "grad_norm": 0.17248782515525818,
      "learning_rate": 0.0,
      "loss": 0.6409,
      "step": 1
    },
    {
      "epoch": 0.0011725391334935803,
      "grad_norm": 0.3558568060398102,
      "learning_rate": 1.1490658954115531e-07,
      "loss": 0.9152,
      "step": 50
    },
    {
      "epoch": 0.0023450782669871607,
      "grad_norm": 0.19745972752571106,
      "learning_rate": 2.3215821152192604e-07,
      "loss": 0.8444,
      "step": 100
    },
    {
      "epoch": 0.0035176174004807412,
      "grad_norm": 0.2833110988140106,
      "learning_rate": 3.4940983350269683e-07,
      "loss": 0.8999,
      "step": 150
    },
    {
      "epoch": 0.004690156533974321,
      "grad_norm": 0.22171325981616974,
      "learning_rate": 4.666614554834675e-07,
      "loss": 0.8989,
      "step": 200
    },
    {
      "epoch": 0.0058626956674679015,
      "grad_norm": 0.19217538833618164,
      "learning_rate": 5.839130774642383e-07,
      "loss": 0.8732,
      "step": 250
    },
    {
      "epoch": 0.0070352348009614825,
      "grad_norm": 0.23765762150287628,
      "learning_rate": 7.01164699445009e-07,
      "loss": 0.9229,
      "step": 300
    },
    {
      "epoch": 0.008207773934455062,
      "grad_norm": 0.31069889664649963,
      "learning_rate": 8.184163214257797e-07,
      "loss": 0.8568,
      "step": 350
    },
    {
      "epoch": 0.009380313067948643,
      "grad_norm": 0.2578216791152954,
      "learning_rate": 9.356679434065505e-07,
      "loss": 0.9242,
      "step": 400
    },
    {
      "epoch": 0.010552852201442224,
      "grad_norm": 0.23753759264945984,
      "learning_rate": 1.0529195653873213e-06,
      "loss": 0.941,
      "step": 450
    },
    {
      "epoch": 0.011725391334935803,
      "grad_norm": 0.3635745942592621,
      "learning_rate": 1.170171187368092e-06,
      "loss": 0.8405,
      "step": 500
    },
    {
      "epoch": 0.012897930468429384,
      "grad_norm": 0.22814543545246124,
      "learning_rate": 1.2874228093488628e-06,
      "loss": 0.9078,
      "step": 550
    },
    {
      "epoch": 0.014070469601922965,
      "grad_norm": 0.20508722960948944,
      "learning_rate": 1.4046744313296334e-06,
      "loss": 0.8251,
      "step": 600
    },
    {
      "epoch": 0.015243008735416544,
      "grad_norm": 0.23806500434875488,
      "learning_rate": 1.5219260533104043e-06,
      "loss": 0.8742,
      "step": 650
    },
    {
      "epoch": 0.016415547868910123,
      "grad_norm": 0.35483255982398987,
      "learning_rate": 1.639177675291175e-06,
      "loss": 0.9315,
      "step": 700
    },
    {
      "epoch": 0.017588087002403704,
      "grad_norm": 0.35561221837997437,
      "learning_rate": 1.7564292972719458e-06,
      "loss": 0.8488,
      "step": 750
    },
    {
      "epoch": 0.018760626135897285,
      "grad_norm": 0.2902124226093292,
      "learning_rate": 1.8736809192527164e-06,
      "loss": 0.872,
      "step": 800
    },
    {
      "epoch": 0.019933165269390866,
      "grad_norm": 0.22340093553066254,
      "learning_rate": 1.990932541233487e-06,
      "loss": 0.8798,
      "step": 850
    },
    {
      "epoch": 0.021105704402884447,
      "grad_norm": 0.23337022960186005,
      "learning_rate": 2.108184163214258e-06,
      "loss": 0.8915,
      "step": 900
    },
    {
      "epoch": 0.022278243536378025,
      "grad_norm": 0.27889588475227356,
      "learning_rate": 2.2254357851950285e-06,
      "loss": 0.8218,
      "step": 950
    },
    {
      "epoch": 0.023450782669871606,
      "grad_norm": 0.3134601414203644,
      "learning_rate": 2.3426874071757993e-06,
      "loss": 0.8557,
      "step": 1000
    },
    {
      "epoch": 0.024623321803365187,
      "grad_norm": 0.3214090168476105,
      "learning_rate": 2.4599390291565698e-06,
      "loss": 0.8572,
      "step": 1050
    },
    {
      "epoch": 0.025795860936858768,
      "grad_norm": 0.35062697529792786,
      "learning_rate": 2.5771906511373406e-06,
      "loss": 0.927,
      "step": 1100
    },
    {
      "epoch": 0.02696840007035235,
      "grad_norm": 0.3494292199611664,
      "learning_rate": 2.6944422731181115e-06,
      "loss": 0.8236,
      "step": 1150
    },
    {
      "epoch": 0.02814093920384593,
      "grad_norm": 0.2725541293621063,
      "learning_rate": 2.8116938950988823e-06,
      "loss": 0.8063,
      "step": 1200
    },
    {
      "epoch": 0.029313478337339507,
      "grad_norm": 0.25898507237434387,
      "learning_rate": 2.9289455170796527e-06,
      "loss": 0.8489,
      "step": 1250
    },
    {
      "epoch": 0.03048601747083309,
      "grad_norm": 0.3096117377281189,
      "learning_rate": 3.0461971390604236e-06,
      "loss": 0.8476,
      "step": 1300
    },
    {
      "epoch": 0.03165855660432667,
      "grad_norm": 0.18626651167869568,
      "learning_rate": 3.163448761041195e-06,
      "loss": 0.8139,
      "step": 1350
    },
    {
      "epoch": 0.03283109573782025,
      "grad_norm": 0.41553041338920593,
      "learning_rate": 3.2807003830219653e-06,
      "loss": 0.8506,
      "step": 1400
    },
    {
      "epoch": 0.03400363487131383,
      "grad_norm": 0.35737189650535583,
      "learning_rate": 3.397952005002736e-06,
      "loss": 0.7946,
      "step": 1450
    },
    {
      "epoch": 0.03517617400480741,
      "grad_norm": 0.29790017008781433,
      "learning_rate": 3.5152036269835065e-06,
      "loss": 0.8162,
      "step": 1500
    },
    {
      "epoch": 0.03634871313830099,
      "grad_norm": 0.49319374561309814,
      "learning_rate": 3.632455248964278e-06,
      "loss": 0.8732,
      "step": 1550
    },
    {
      "epoch": 0.03752125227179457,
      "grad_norm": 0.3901229202747345,
      "learning_rate": 3.7497068709450482e-06,
      "loss": 0.7551,
      "step": 1600
    },
    {
      "epoch": 0.03869379140528815,
      "grad_norm": 0.35692378878593445,
      "learning_rate": 3.866958492925819e-06,
      "loss": 0.806,
      "step": 1650
    },
    {
      "epoch": 0.03986633053878173,
      "grad_norm": 0.3562120199203491,
      "learning_rate": 3.9842101149065895e-06,
      "loss": 0.8024,
      "step": 1700
    },
    {
      "epoch": 0.04103886967227531,
      "grad_norm": 0.604220449924469,
      "learning_rate": 4.101461736887361e-06,
      "loss": 0.8369,
      "step": 1750
    },
    {
      "epoch": 0.042211408805768895,
      "grad_norm": 0.25186899304389954,
      "learning_rate": 4.218713358868131e-06,
      "loss": 0.7635,
      "step": 1800
    },
    {
      "epoch": 0.04338394793926247,
      "grad_norm": 0.40235817432403564,
      "learning_rate": 4.335964980848902e-06,
      "loss": 0.814,
      "step": 1850
    },
    {
      "epoch": 0.04455648707275605,
      "grad_norm": 0.38814517855644226,
      "learning_rate": 4.453216602829673e-06,
      "loss": 0.7214,
      "step": 1900
    },
    {
      "epoch": 0.045729026206249634,
      "grad_norm": 0.44633910059928894,
      "learning_rate": 4.570468224810443e-06,
      "loss": 0.7241,
      "step": 1950
    },
    {
      "epoch": 0.04690156533974321,
      "grad_norm": 0.5766168236732483,
      "learning_rate": 4.6877198467912146e-06,
      "loss": 0.712,
      "step": 2000
    },
    {
      "epoch": 0.04690156533974321,
      "eval_loss": 1.2957117557525635,
      "eval_runtime": 211.7497,
      "eval_samples_per_second": 47.226,
      "eval_steps_per_second": 11.806,
      "step": 2000
    },
    {
      "epoch": 0.048074104473236796,
      "grad_norm": 0.27791839838027954,
      "learning_rate": 4.804971468771984e-06,
      "loss": 0.7286,
      "step": 2050
    },
    {
      "epoch": 0.049246643606730374,
      "grad_norm": 0.38338953256607056,
      "learning_rate": 4.922223090752755e-06,
      "loss": 0.7632,
      "step": 2100
    },
    {
      "epoch": 0.05041918274022396,
      "grad_norm": 0.3329769968986511,
      "learning_rate": 5.039474712733527e-06,
      "loss": 0.7414,
      "step": 2150
    },
    {
      "epoch": 0.051591721873717536,
      "grad_norm": 0.3040897846221924,
      "learning_rate": 5.156726334714297e-06,
      "loss": 0.7557,
      "step": 2200
    },
    {
      "epoch": 0.05276426100721111,
      "grad_norm": 0.30323126912117004,
      "learning_rate": 5.2739779566950675e-06,
      "loss": 0.686,
      "step": 2250
    },
    {
      "epoch": 0.0539368001407047,
      "grad_norm": 0.31511348485946655,
      "learning_rate": 5.391229578675839e-06,
      "loss": 0.6972,
      "step": 2300
    },
    {
      "epoch": 0.055109339274198275,
      "grad_norm": 0.4897419512271881,
      "learning_rate": 5.508481200656609e-06,
      "loss": 0.6951,
      "step": 2350
    },
    {
      "epoch": 0.05628187840769186,
      "grad_norm": 0.4097015857696533,
      "learning_rate": 5.6257328226373805e-06,
      "loss": 0.7238,
      "step": 2400
    },
    {
      "epoch": 0.05745441754118544,
      "grad_norm": 0.43235307931900024,
      "learning_rate": 5.74298444461815e-06,
      "loss": 0.7349,
      "step": 2450
    },
    {
      "epoch": 0.058626956674679015,
      "grad_norm": 0.2418988049030304,
      "learning_rate": 5.860236066598921e-06,
      "loss": 0.6898,
      "step": 2500
    },
    {
      "epoch": 0.0597994958081726,
      "grad_norm": 0.3093021512031555,
      "learning_rate": 5.977487688579693e-06,
      "loss": 0.7081,
      "step": 2550
    },
    {
      "epoch": 0.06097203494166618,
      "grad_norm": 0.24662448465824127,
      "learning_rate": 6.094739310560463e-06,
      "loss": 0.6828,
      "step": 2600
    },
    {
      "epoch": 0.06214457407515976,
      "grad_norm": 0.3761790096759796,
      "learning_rate": 6.2119909325412334e-06,
      "loss": 0.6883,
      "step": 2650
    },
    {
      "epoch": 0.06331711320865334,
      "grad_norm": 0.5269337296485901,
      "learning_rate": 6.329242554522004e-06,
      "loss": 0.6727,
      "step": 2700
    },
    {
      "epoch": 0.06448965234214692,
      "grad_norm": 0.3073654770851135,
      "learning_rate": 6.446494176502775e-06,
      "loss": 0.6626,
      "step": 2750
    },
    {
      "epoch": 0.0656621914756405,
      "grad_norm": 0.3154309391975403,
      "learning_rate": 6.563745798483546e-06,
      "loss": 0.6948,
      "step": 2800
    },
    {
      "epoch": 0.06683473060913409,
      "grad_norm": 0.2412647008895874,
      "learning_rate": 6.680997420464316e-06,
      "loss": 0.665,
      "step": 2850
    },
    {
      "epoch": 0.06800726974262766,
      "grad_norm": 0.2531448006629944,
      "learning_rate": 6.798249042445087e-06,
      "loss": 0.6869,
      "step": 2900
    },
    {
      "epoch": 0.06917980887612124,
      "grad_norm": 0.4014500677585602,
      "learning_rate": 6.9155006644258585e-06,
      "loss": 0.6513,
      "step": 2950
    },
    {
      "epoch": 0.07035234800961482,
      "grad_norm": 0.579472541809082,
      "learning_rate": 7.032752286406629e-06,
      "loss": 0.6901,
      "step": 3000
    },
    {
      "epoch": 0.0715248871431084,
      "grad_norm": 0.36082637310028076,
      "learning_rate": 7.150003908387399e-06,
      "loss": 0.6946,
      "step": 3050
    },
    {
      "epoch": 0.07269742627660199,
      "grad_norm": 0.3791721761226654,
      "learning_rate": 7.26725553036817e-06,
      "loss": 0.6734,
      "step": 3100
    },
    {
      "epoch": 0.07386996541009556,
      "grad_norm": 0.4407157599925995,
      "learning_rate": 7.384507152348941e-06,
      "loss": 0.6436,
      "step": 3150
    },
    {
      "epoch": 0.07504250454358914,
      "grad_norm": 0.23751245439052582,
      "learning_rate": 7.5017587743297115e-06,
      "loss": 0.6325,
      "step": 3200
    },
    {
      "epoch": 0.07621504367708272,
      "grad_norm": 0.2853377163410187,
      "learning_rate": 7.619010396310483e-06,
      "loss": 0.6533,
      "step": 3250
    },
    {
      "epoch": 0.0773875828105763,
      "grad_norm": 0.2518289089202881,
      "learning_rate": 7.736262018291253e-06,
      "loss": 0.5917,
      "step": 3300
    },
    {
      "epoch": 0.07856012194406989,
      "grad_norm": 0.3931322395801544,
      "learning_rate": 7.853513640272024e-06,
      "loss": 0.6405,
      "step": 3350
    },
    {
      "epoch": 0.07973266107756347,
      "grad_norm": 0.30834612250328064,
      "learning_rate": 7.970765262252796e-06,
      "loss": 0.6466,
      "step": 3400
    },
    {
      "epoch": 0.08090520021105704,
      "grad_norm": 0.17868559062480927,
      "learning_rate": 8.088016884233565e-06,
      "loss": 0.6413,
      "step": 3450
    },
    {
      "epoch": 0.08207773934455062,
      "grad_norm": 0.3068073093891144,
      "learning_rate": 8.205268506214337e-06,
      "loss": 0.6146,
      "step": 3500
    },
    {
      "epoch": 0.0832502784780442,
      "grad_norm": 0.34030789136886597,
      "learning_rate": 8.322520128195106e-06,
      "loss": 0.5877,
      "step": 3550
    },
    {
      "epoch": 0.08442281761153779,
      "grad_norm": 0.576086163520813,
      "learning_rate": 8.439771750175877e-06,
      "loss": 0.6123,
      "step": 3600
    },
    {
      "epoch": 0.08559535674503137,
      "grad_norm": 0.2575463652610779,
      "learning_rate": 8.557023372156649e-06,
      "loss": 0.6426,
      "step": 3650
    },
    {
      "epoch": 0.08676789587852494,
      "grad_norm": 0.214446023106575,
      "learning_rate": 8.674274994137418e-06,
      "loss": 0.6111,
      "step": 3700
    },
    {
      "epoch": 0.08794043501201852,
      "grad_norm": 0.2203807383775711,
      "learning_rate": 8.791526616118191e-06,
      "loss": 0.658,
      "step": 3750
    },
    {
      "epoch": 0.0891129741455121,
      "grad_norm": 0.227418452501297,
      "learning_rate": 8.90877823809896e-06,
      "loss": 0.6432,
      "step": 3800
    },
    {
      "epoch": 0.09028551327900569,
      "grad_norm": 0.29445046186447144,
      "learning_rate": 9.02602986007973e-06,
      "loss": 0.5863,
      "step": 3850
    },
    {
      "epoch": 0.09145805241249927,
      "grad_norm": 0.20213930308818817,
      "learning_rate": 9.143281482060503e-06,
      "loss": 0.5788,
      "step": 3900
    },
    {
      "epoch": 0.09263059154599285,
      "grad_norm": 0.33111172914505005,
      "learning_rate": 9.260533104041273e-06,
      "loss": 0.6179,
      "step": 3950
    },
    {
      "epoch": 0.09380313067948642,
      "grad_norm": 0.25127971172332764,
      "learning_rate": 9.377784726022042e-06,
      "loss": 0.6447,
      "step": 4000
    },
    {
      "epoch": 0.09380313067948642,
      "eval_loss": 0.9603905081748962,
      "eval_runtime": 155.9034,
      "eval_samples_per_second": 64.142,
      "eval_steps_per_second": 16.036,
      "step": 4000
    },
    {
      "epoch": 0.09497566981298,
      "grad_norm": 0.4824722707271576,
      "learning_rate": 9.495036348002815e-06,
      "loss": 0.6145,
      "step": 4050
    },
    {
      "epoch": 0.09614820894647359,
      "grad_norm": 0.32859355211257935,
      "learning_rate": 9.612287969983585e-06,
      "loss": 0.6013,
      "step": 4100
    },
    {
      "epoch": 0.09732074807996717,
      "grad_norm": 0.3448038697242737,
      "learning_rate": 9.729539591964356e-06,
      "loss": 0.5578,
      "step": 4150
    },
    {
      "epoch": 0.09849328721346075,
      "grad_norm": 0.3608987331390381,
      "learning_rate": 9.846791213945126e-06,
      "loss": 0.5998,
      "step": 4200
    },
    {
      "epoch": 0.09966582634695433,
      "grad_norm": 0.3389868438243866,
      "learning_rate": 9.964042835925897e-06,
      "loss": 0.6188,
      "step": 4250
    },
    {
      "epoch": 0.10083836548044792,
      "grad_norm": 0.28720155358314514,
      "learning_rate": 1.0081294457906668e-05,
      "loss": 0.6204,
      "step": 4300
    },
    {
      "epoch": 0.1020109046139415,
      "grad_norm": 0.24513296782970428,
      "learning_rate": 1.0198546079887438e-05,
      "loss": 0.5553,
      "step": 4350
    },
    {
      "epoch": 0.10318344374743507,
      "grad_norm": 0.3972143232822418,
      "learning_rate": 1.031579770186821e-05,
      "loss": 0.5846,
      "step": 4400
    },
    {
      "epoch": 0.10435598288092865,
      "grad_norm": 0.2576247453689575,
      "learning_rate": 1.043304932384898e-05,
      "loss": 0.5727,
      "step": 4450
    },
    {
      "epoch": 0.10552852201442223,
      "grad_norm": 0.20778681337833405,
      "learning_rate": 1.055030094582975e-05,
      "loss": 0.5489,
      "step": 4500
    },
    {
      "epoch": 0.10670106114791582,
      "grad_norm": 0.3150484263896942,
      "learning_rate": 1.0667552567810523e-05,
      "loss": 0.562,
      "step": 4550
    },
    {
      "epoch": 0.1078736002814094,
      "grad_norm": 0.19164255261421204,
      "learning_rate": 1.0784804189791293e-05,
      "loss": 0.5653,
      "step": 4600
    },
    {
      "epoch": 0.10904613941490297,
      "grad_norm": 0.1929282248020172,
      "learning_rate": 1.0902055811772062e-05,
      "loss": 0.6039,
      "step": 4650
    },
    {
      "epoch": 0.11021867854839655,
      "grad_norm": 0.36197611689567566,
      "learning_rate": 1.1019307433752835e-05,
      "loss": 0.5719,
      "step": 4700
    },
    {
      "epoch": 0.11139121768189013,
      "grad_norm": 0.2777063548564911,
      "learning_rate": 1.1136559055733605e-05,
      "loss": 0.5778,
      "step": 4750
    },
    {
      "epoch": 0.11256375681538372,
      "grad_norm": 0.19840480387210846,
      "learning_rate": 1.1253810677714374e-05,
      "loss": 0.5228,
      "step": 4800
    },
    {
      "epoch": 0.1137362959488773,
      "grad_norm": 0.4027370512485504,
      "learning_rate": 1.1371062299695146e-05,
      "loss": 0.5962,
      "step": 4850
    },
    {
      "epoch": 0.11490883508237087,
      "grad_norm": 0.30363401770591736,
      "learning_rate": 1.1488313921675917e-05,
      "loss": 0.5877,
      "step": 4900
    },
    {
      "epoch": 0.11608137421586445,
      "grad_norm": 0.2790375351905823,
      "learning_rate": 1.1605565543656688e-05,
      "loss": 0.5915,
      "step": 4950
    },
    {
      "epoch": 0.11725391334935803,
      "grad_norm": 0.2771964967250824,
      "learning_rate": 1.1722817165637458e-05,
      "loss": 0.5815,
      "step": 5000
    },
    {
      "epoch": 0.11842645248285162,
      "grad_norm": 0.18234960734844208,
      "learning_rate": 1.1840068787618229e-05,
      "loss": 0.5789,
      "step": 5050
    },
    {
      "epoch": 0.1195989916163452,
      "grad_norm": 0.37983423471450806,
      "learning_rate": 1.1957320409599e-05,
      "loss": 0.541,
      "step": 5100
    },
    {
      "epoch": 0.12077153074983878,
      "grad_norm": 0.30168473720550537,
      "learning_rate": 1.207457203157977e-05,
      "loss": 0.5601,
      "step": 5150
    },
    {
      "epoch": 0.12194406988333235,
      "grad_norm": 0.2976642847061157,
      "learning_rate": 1.2191823653560541e-05,
      "loss": 0.5985,
      "step": 5200
    },
    {
      "epoch": 0.12311660901682593,
      "grad_norm": 0.22949445247650146,
      "learning_rate": 1.2309075275541312e-05,
      "loss": 0.5822,
      "step": 5250
    },
    {
      "epoch": 0.12428914815031952,
      "grad_norm": 0.2382982224225998,
      "learning_rate": 1.2426326897522082e-05,
      "loss": 0.5612,
      "step": 5300
    },
    {
      "epoch": 0.1254616872838131,
      "grad_norm": 0.19932527840137482,
      "learning_rate": 1.2543578519502855e-05,
      "loss": 0.5979,
      "step": 5350
    },
    {
      "epoch": 0.12663422641730668,
      "grad_norm": 0.17901413142681122,
      "learning_rate": 1.2660830141483624e-05,
      "loss": 0.5858,
      "step": 5400
    },
    {
      "epoch": 0.12780676555080026,
      "grad_norm": 0.27626025676727295,
      "learning_rate": 1.2778081763464394e-05,
      "loss": 0.5589,
      "step": 5450
    },
    {
      "epoch": 0.12897930468429383,
      "grad_norm": 0.20928025245666504,
      "learning_rate": 1.2895333385445165e-05,
      "loss": 0.5106,
      "step": 5500
    },
    {
      "epoch": 0.1301518438177874,
      "grad_norm": 0.211399108171463,
      "learning_rate": 1.3012585007425937e-05,
      "loss": 0.5295,
      "step": 5550
    },
    {
      "epoch": 0.131324382951281,
      "grad_norm": 0.3038526177406311,
      "learning_rate": 1.3129836629406708e-05,
      "loss": 0.5752,
      "step": 5600
    },
    {
      "epoch": 0.1324969220847746,
      "grad_norm": 0.24097374081611633,
      "learning_rate": 1.3247088251387477e-05,
      "loss": 0.533,
      "step": 5650
    },
    {
      "epoch": 0.13366946121826817,
      "grad_norm": 0.1922808587551117,
      "learning_rate": 1.3364339873368249e-05,
      "loss": 0.5596,
      "step": 5700
    },
    {
      "epoch": 0.13484200035176175,
      "grad_norm": 0.5740708112716675,
      "learning_rate": 1.348159149534902e-05,
      "loss": 0.5813,
      "step": 5750
    },
    {
      "epoch": 0.13601453948525533,
      "grad_norm": 0.21891175210475922,
      "learning_rate": 1.359884311732979e-05,
      "loss": 0.5338,
      "step": 5800
    },
    {
      "epoch": 0.1371870786187489,
      "grad_norm": 0.3413246273994446,
      "learning_rate": 1.371609473931056e-05,
      "loss": 0.5476,
      "step": 5850
    },
    {
      "epoch": 0.13835961775224248,
      "grad_norm": 0.3134210705757141,
      "learning_rate": 1.3833346361291332e-05,
      "loss": 0.5519,
      "step": 5900
    },
    {
      "epoch": 0.13953215688573606,
      "grad_norm": 0.2318161129951477,
      "learning_rate": 1.3950597983272102e-05,
      "loss": 0.5525,
      "step": 5950
    },
    {
      "epoch": 0.14070469601922964,
      "grad_norm": 0.25069287419319153,
      "learning_rate": 1.4067849605252875e-05,
      "loss": 0.5596,
      "step": 6000
    },
    {
      "epoch": 0.14070469601922964,
      "eval_loss": 0.8635527491569519,
      "eval_runtime": 154.7358,
      "eval_samples_per_second": 64.626,
      "eval_steps_per_second": 16.157,
      "step": 6000
    },
    {
      "epoch": 0.1418772351527232,
      "grad_norm": 0.2588099539279938,
      "learning_rate": 1.4185101227233644e-05,
      "loss": 0.5396,
      "step": 6050
    },
    {
      "epoch": 0.1430497742862168,
      "grad_norm": 0.2171570509672165,
      "learning_rate": 1.4302352849214414e-05,
      "loss": 0.5049,
      "step": 6100
    },
    {
      "epoch": 0.1442223134197104,
      "grad_norm": 0.17710746824741364,
      "learning_rate": 1.4419604471195185e-05,
      "loss": 0.5016,
      "step": 6150
    },
    {
      "epoch": 0.14539485255320397,
      "grad_norm": 0.19197633862495422,
      "learning_rate": 1.4536856093175956e-05,
      "loss": 0.5379,
      "step": 6200
    },
    {
      "epoch": 0.14656739168669755,
      "grad_norm": 0.172803595662117,
      "learning_rate": 1.4654107715156726e-05,
      "loss": 0.5206,
      "step": 6250
    },
    {
      "epoch": 0.14773993082019113,
      "grad_norm": 0.18230825662612915,
      "learning_rate": 1.4771359337137497e-05,
      "loss": 0.5112,
      "step": 6300
    },
    {
      "epoch": 0.1489124699536847,
      "grad_norm": 0.20131412148475647,
      "learning_rate": 1.4888610959118268e-05,
      "loss": 0.5871,
      "step": 6350
    },
    {
      "epoch": 0.15008500908717828,
      "grad_norm": 0.2705182731151581,
      "learning_rate": 1.5005862581099038e-05,
      "loss": 0.5357,
      "step": 6400
    },
    {
      "epoch": 0.15125754822067186,
      "grad_norm": 0.26931828260421753,
      "learning_rate": 1.512311420307981e-05,
      "loss": 0.5403,
      "step": 6450
    },
    {
      "epoch": 0.15243008735416544,
      "grad_norm": 0.20003433525562286,
      "learning_rate": 1.5240365825060582e-05,
      "loss": 0.567,
      "step": 6500
    },
    {
      "epoch": 0.15360262648765902,
      "grad_norm": 0.3331314027309418,
      "learning_rate": 1.535761744704135e-05,
      "loss": 0.5703,
      "step": 6550
    },
    {
      "epoch": 0.1547751656211526,
      "grad_norm": 0.34815338253974915,
      "learning_rate": 1.547486906902212e-05,
      "loss": 0.5843,
      "step": 6600
    },
    {
      "epoch": 0.1559477047546462,
      "grad_norm": 0.24933968484401703,
      "learning_rate": 1.5592120691002893e-05,
      "loss": 0.5299,
      "step": 6650
    },
    {
      "epoch": 0.15712024388813978,
      "grad_norm": 0.19074952602386475,
      "learning_rate": 1.570937231298366e-05,
      "loss": 0.5653,
      "step": 6700
    },
    {
      "epoch": 0.15829278302163335,
      "grad_norm": 0.2951699197292328,
      "learning_rate": 1.5826623934964435e-05,
      "loss": 0.6045,
      "step": 6750
    },
    {
      "epoch": 0.15946532215512693,
      "grad_norm": 0.23942196369171143,
      "learning_rate": 1.5943875556945206e-05,
      "loss": 0.5421,
      "step": 6800
    },
    {
      "epoch": 0.1606378612886205,
      "grad_norm": 0.19591015577316284,
      "learning_rate": 1.6061127178925974e-05,
      "loss": 0.5443,
      "step": 6850
    },
    {
      "epoch": 0.1618104004221141,
      "grad_norm": 0.361915647983551,
      "learning_rate": 1.6178378800906746e-05,
      "loss": 0.5331,
      "step": 6900
    },
    {
      "epoch": 0.16298293955560766,
      "grad_norm": 0.3369421660900116,
      "learning_rate": 1.6295630422887517e-05,
      "loss": 0.533,
      "step": 6950
    },
    {
      "epoch": 0.16415547868910124,
      "grad_norm": 0.22893501818180084,
      "learning_rate": 1.6412882044868288e-05,
      "loss": 0.5255,
      "step": 7000
    },
    {
      "epoch": 0.16532801782259482,
      "grad_norm": 0.21172888576984406,
      "learning_rate": 1.653013366684906e-05,
      "loss": 0.5226,
      "step": 7050
    },
    {
      "epoch": 0.1665005569560884,
      "grad_norm": 0.1910674273967743,
      "learning_rate": 1.664738528882983e-05,
      "loss": 0.5249,
      "step": 7100
    },
    {
      "epoch": 0.167673096089582,
      "grad_norm": 0.3118355870246887,
      "learning_rate": 1.6764636910810602e-05,
      "loss": 0.5221,
      "step": 7150
    },
    {
      "epoch": 0.16884563522307558,
      "grad_norm": 0.2773694694042206,
      "learning_rate": 1.688188853279137e-05,
      "loss": 0.4971,
      "step": 7200
    },
    {
      "epoch": 0.17001817435656916,
      "grad_norm": 0.33815616369247437,
      "learning_rate": 1.699914015477214e-05,
      "loss": 0.5096,
      "step": 7250
    },
    {
      "epoch": 0.17119071349006273,
      "grad_norm": 0.2549287974834442,
      "learning_rate": 1.7116391776752912e-05,
      "loss": 0.555,
      "step": 7300
    },
    {
      "epoch": 0.1723632526235563,
      "grad_norm": 0.22836199402809143,
      "learning_rate": 1.723364339873368e-05,
      "loss": 0.4923,
      "step": 7350
    },
    {
      "epoch": 0.1735357917570499,
      "grad_norm": 0.28301483392715454,
      "learning_rate": 1.7350895020714455e-05,
      "loss": 0.5304,
      "step": 7400
    },
    {
      "epoch": 0.17470833089054347,
      "grad_norm": 0.23902328312397003,
      "learning_rate": 1.7468146642695226e-05,
      "loss": 0.5617,
      "step": 7450
    },
    {
      "epoch": 0.17588087002403704,
      "grad_norm": 0.2559727430343628,
      "learning_rate": 1.7585398264675994e-05,
      "loss": 0.4983,
      "step": 7500
    },
    {
      "epoch": 0.17705340915753062,
      "grad_norm": 0.33440515398979187,
      "learning_rate": 1.7702649886656765e-05,
      "loss": 0.5125,
      "step": 7550
    },
    {
      "epoch": 0.1782259482910242,
      "grad_norm": 0.3155806064605713,
      "learning_rate": 1.7819901508637537e-05,
      "loss": 0.5026,
      "step": 7600
    },
    {
      "epoch": 0.1793984874245178,
      "grad_norm": 0.1948738396167755,
      "learning_rate": 1.7937153130618308e-05,
      "loss": 0.5278,
      "step": 7650
    },
    {
      "epoch": 0.18057102655801138,
      "grad_norm": 0.24683889746665955,
      "learning_rate": 1.805440475259908e-05,
      "loss": 0.5237,
      "step": 7700
    },
    {
      "epoch": 0.18174356569150496,
      "grad_norm": 0.33944007754325867,
      "learning_rate": 1.817165637457985e-05,
      "loss": 0.5278,
      "step": 7750
    },
    {
      "epoch": 0.18291610482499854,
      "grad_norm": 0.25240933895111084,
      "learning_rate": 1.828890799656062e-05,
      "loss": 0.5219,
      "step": 7800
    },
    {
      "epoch": 0.18408864395849212,
      "grad_norm": 0.3473970293998718,
      "learning_rate": 1.840615961854139e-05,
      "loss": 0.5055,
      "step": 7850
    },
    {
      "epoch": 0.1852611830919857,
      "grad_norm": 0.14314934611320496,
      "learning_rate": 1.852341124052216e-05,
      "loss": 0.4837,
      "step": 7900
    },
    {
      "epoch": 0.18643372222547927,
      "grad_norm": 0.3982987701892853,
      "learning_rate": 1.8640662862502932e-05,
      "loss": 0.5396,
      "step": 7950
    },
    {
      "epoch": 0.18760626135897285,
      "grad_norm": 0.3352479040622711,
      "learning_rate": 1.87579144844837e-05,
      "loss": 0.5182,
      "step": 8000
    },
    {
      "epoch": 0.18760626135897285,
      "eval_loss": 0.8372457027435303,
      "eval_runtime": 163.0972,
      "eval_samples_per_second": 61.313,
      "eval_steps_per_second": 15.328,
      "step": 8000
    },
    {
      "epoch": 0.18877880049246643,
      "grad_norm": 0.1873064637184143,
      "learning_rate": 1.8875166106464475e-05,
      "loss": 0.5168,
      "step": 8050
    },
    {
      "epoch": 0.18995133962596,
      "grad_norm": 0.2015993744134903,
      "learning_rate": 1.8992417728445246e-05,
      "loss": 0.5067,
      "step": 8100
    },
    {
      "epoch": 0.1911238787594536,
      "grad_norm": 0.3019341826438904,
      "learning_rate": 1.9109669350426014e-05,
      "loss": 0.5205,
      "step": 8150
    },
    {
      "epoch": 0.19229641789294719,
      "grad_norm": 0.2225499302148819,
      "learning_rate": 1.9226920972406785e-05,
      "loss": 0.4925,
      "step": 8200
    },
    {
      "epoch": 0.19346895702644076,
      "grad_norm": 0.1863342970609665,
      "learning_rate": 1.9344172594387556e-05,
      "loss": 0.5458,
      "step": 8250
    },
    {
      "epoch": 0.19464149615993434,
      "grad_norm": 0.24605576694011688,
      "learning_rate": 1.9461424216368324e-05,
      "loss": 0.4817,
      "step": 8300
    },
    {
      "epoch": 0.19581403529342792,
      "grad_norm": 0.25869083404541016,
      "learning_rate": 1.95786758383491e-05,
      "loss": 0.5196,
      "step": 8350
    },
    {
      "epoch": 0.1969865744269215,
      "grad_norm": 0.29356521368026733,
      "learning_rate": 1.969592746032987e-05,
      "loss": 0.4975,
      "step": 8400
    },
    {
      "epoch": 0.19815911356041507,
      "grad_norm": 0.30465054512023926,
      "learning_rate": 1.981317908231064e-05,
      "loss": 0.5372,
      "step": 8450
    },
    {
      "epoch": 0.19933165269390865,
      "grad_norm": 0.32444262504577637,
      "learning_rate": 1.993043070429141e-05,
      "loss": 0.5276,
      "step": 8500
    },
    {
      "epoch": 0.20050419182740223,
      "grad_norm": 0.24702976644039154,
      "learning_rate": 2.004768232627218e-05,
      "loss": 0.5065,
      "step": 8550
    },
    {
      "epoch": 0.20167673096089583,
      "grad_norm": 0.2657378315925598,
      "learning_rate": 2.0164933948252952e-05,
      "loss": 0.4788,
      "step": 8600
    },
    {
      "epoch": 0.2028492700943894,
      "grad_norm": 0.2930091917514801,
      "learning_rate": 2.028218557023372e-05,
      "loss": 0.4776,
      "step": 8650
    },
    {
      "epoch": 0.204021809227883,
      "grad_norm": 0.38414064049720764,
      "learning_rate": 2.0399437192214494e-05,
      "loss": 0.5266,
      "step": 8700
    },
    {
      "epoch": 0.20519434836137657,
      "grad_norm": 0.3385902941226959,
      "learning_rate": 2.0516688814195266e-05,
      "loss": 0.54,
      "step": 8750
    },
    {
      "epoch": 0.20636688749487014,
      "grad_norm": 0.2862034738063812,
      "learning_rate": 2.0633940436176033e-05,
      "loss": 0.4954,
      "step": 8800
    },
    {
      "epoch": 0.20753942662836372,
      "grad_norm": 0.43031197786331177,
      "learning_rate": 2.0751192058156805e-05,
      "loss": 0.4858,
      "step": 8850
    },
    {
      "epoch": 0.2087119657618573,
      "grad_norm": 0.3214908540248871,
      "learning_rate": 2.0868443680137576e-05,
      "loss": 0.5332,
      "step": 8900
    },
    {
      "epoch": 0.20988450489535088,
      "grad_norm": 0.180878683924675,
      "learning_rate": 2.0985695302118344e-05,
      "loss": 0.5337,
      "step": 8950
    },
    {
      "epoch": 0.21105704402884445,
      "grad_norm": 0.3676891624927521,
      "learning_rate": 2.110294692409912e-05,
      "loss": 0.4829,
      "step": 9000
    },
    {
      "epoch": 0.21222958316233803,
      "grad_norm": 0.3328915238380432,
      "learning_rate": 2.122019854607989e-05,
      "loss": 0.5084,
      "step": 9050
    },
    {
      "epoch": 0.21340212229583164,
      "grad_norm": 0.23505114018917084,
      "learning_rate": 2.1337450168060658e-05,
      "loss": 0.4848,
      "step": 9100
    },
    {
      "epoch": 0.21457466142932521,
      "grad_norm": 0.21535193920135498,
      "learning_rate": 2.145470179004143e-05,
      "loss": 0.5093,
      "step": 9150
    },
    {
      "epoch": 0.2157472005628188,
      "grad_norm": 0.3275696635246277,
      "learning_rate": 2.15719534120222e-05,
      "loss": 0.5524,
      "step": 9200
    },
    {
      "epoch": 0.21691973969631237,
      "grad_norm": 0.41012534499168396,
      "learning_rate": 2.168920503400297e-05,
      "loss": 0.5379,
      "step": 9250
    },
    {
      "epoch": 0.21809227882980595,
      "grad_norm": 0.15728150308132172,
      "learning_rate": 2.180645665598374e-05,
      "loss": 0.5093,
      "step": 9300
    },
    {
      "epoch": 0.21926481796329952,
      "grad_norm": 0.3992556929588318,
      "learning_rate": 2.1923708277964514e-05,
      "loss": 0.5198,
      "step": 9350
    },
    {
      "epoch": 0.2204373570967931,
      "grad_norm": 0.17216506600379944,
      "learning_rate": 2.2040959899945285e-05,
      "loss": 0.538,
      "step": 9400
    },
    {
      "epoch": 0.22160989623028668,
      "grad_norm": 0.22683699429035187,
      "learning_rate": 2.2158211521926053e-05,
      "loss": 0.5138,
      "step": 9450
    },
    {
      "epoch": 0.22278243536378026,
      "grad_norm": 0.24126137793064117,
      "learning_rate": 2.2275463143906824e-05,
      "loss": 0.4963,
      "step": 9500
    },
    {
      "epoch": 0.22395497449727383,
      "grad_norm": 0.33731862902641296,
      "learning_rate": 2.2392714765887596e-05,
      "loss": 0.5213,
      "step": 9550
    },
    {
      "epoch": 0.22512751363076744,
      "grad_norm": 0.22735978662967682,
      "learning_rate": 2.2509966387868364e-05,
      "loss": 0.4951,
      "step": 9600
    },
    {
      "epoch": 0.22630005276426102,
      "grad_norm": 0.32688090205192566,
      "learning_rate": 2.2627218009849138e-05,
      "loss": 0.5233,
      "step": 9650
    },
    {
      "epoch": 0.2274725918977546,
      "grad_norm": 0.5615419149398804,
      "learning_rate": 2.274446963182991e-05,
      "loss": 0.5201,
      "step": 9700
    },
    {
      "epoch": 0.22864513103124817,
      "grad_norm": 0.20382681488990784,
      "learning_rate": 2.2861721253810677e-05,
      "loss": 0.5222,
      "step": 9750
    },
    {
      "epoch": 0.22981767016474175,
      "grad_norm": 0.3411027491092682,
      "learning_rate": 2.297897287579145e-05,
      "loss": 0.5202,
      "step": 9800
    },
    {
      "epoch": 0.23099020929823533,
      "grad_norm": 0.457991361618042,
      "learning_rate": 2.309622449777222e-05,
      "loss": 0.5163,
      "step": 9850
    },
    {
      "epoch": 0.2321627484317289,
      "grad_norm": 0.21660973131656647,
      "learning_rate": 2.321347611975299e-05,
      "loss": 0.5185,
      "step": 9900
    },
    {
      "epoch": 0.23333528756522248,
      "grad_norm": 0.2903452515602112,
      "learning_rate": 2.333072774173376e-05,
      "loss": 0.4901,
      "step": 9950
    },
    {
      "epoch": 0.23450782669871606,
      "grad_norm": 0.16048748791217804,
      "learning_rate": 2.3447979363714534e-05,
      "loss": 0.4877,
      "step": 10000
    },
    {
      "epoch": 0.23450782669871606,
      "eval_loss": 0.8261668086051941,
      "eval_runtime": 123.6456,
      "eval_samples_per_second": 80.876,
      "eval_steps_per_second": 20.219,
      "step": 10000
    },
    {
      "epoch": 0.23568036583220964,
      "grad_norm": 0.322378009557724,
      "learning_rate": 2.3565230985695305e-05,
      "loss": 0.5406,
      "step": 10050
    },
    {
      "epoch": 0.23685290496570324,
      "grad_norm": 0.25347185134887695,
      "learning_rate": 2.3682482607676073e-05,
      "loss": 0.5065,
      "step": 10100
    },
    {
      "epoch": 0.23802544409919682,
      "grad_norm": 0.40246298909187317,
      "learning_rate": 2.3799734229656844e-05,
      "loss": 0.5638,
      "step": 10150
    },
    {
      "epoch": 0.2391979832326904,
      "grad_norm": 0.27887243032455444,
      "learning_rate": 2.3916985851637615e-05,
      "loss": 0.4761,
      "step": 10200
    },
    {
      "epoch": 0.24037052236618398,
      "grad_norm": 0.2965826094150543,
      "learning_rate": 2.4034237473618383e-05,
      "loss": 0.5215,
      "step": 10250
    },
    {
      "epoch": 0.24154306149967755,
      "grad_norm": 0.29166579246520996,
      "learning_rate": 2.4151489095599158e-05,
      "loss": 0.4819,
      "step": 10300
    },
    {
      "epoch": 0.24271560063317113,
      "grad_norm": 0.2645767331123352,
      "learning_rate": 2.426874071757993e-05,
      "loss": 0.5132,
      "step": 10350
    },
    {
      "epoch": 0.2438881397666647,
      "grad_norm": 0.17213930189609528,
      "learning_rate": 2.4385992339560697e-05,
      "loss": 0.5158,
      "step": 10400
    },
    {
      "epoch": 0.24506067890015829,
      "grad_norm": 0.2704728841781616,
      "learning_rate": 2.450324396154147e-05,
      "loss": 0.5099,
      "step": 10450
    },
    {
      "epoch": 0.24623321803365186,
      "grad_norm": 0.5298373103141785,
      "learning_rate": 2.462049558352224e-05,
      "loss": 0.4897,
      "step": 10500
    },
    {
      "epoch": 0.24740575716714544,
      "grad_norm": 0.2722555100917816,
      "learning_rate": 2.4737747205503007e-05,
      "loss": 0.4966,
      "step": 10550
    },
    {
      "epoch": 0.24857829630063905,
      "grad_norm": 0.4223855435848236,
      "learning_rate": 2.485499882748378e-05,
      "loss": 0.5211,
      "step": 10600
    },
    {
      "epoch": 0.24975083543413262,
      "grad_norm": 0.41518616676330566,
      "learning_rate": 2.4972250449464553e-05,
      "loss": 0.4922,
      "step": 10650
    },
    {
      "epoch": 0.2509233745676262,
      "grad_norm": 0.2761653661727905,
      "learning_rate": 2.5089502071445325e-05,
      "loss": 0.4722,
      "step": 10700
    },
    {
      "epoch": 0.2520959137011198,
      "grad_norm": 0.3204081654548645,
      "learning_rate": 2.5206753693426093e-05,
      "loss": 0.4911,
      "step": 10750
    },
    {
      "epoch": 0.25326845283461336,
      "grad_norm": 0.2762182056903839,
      "learning_rate": 2.5324005315406864e-05,
      "loss": 0.4688,
      "step": 10800
    },
    {
      "epoch": 0.25444099196810693,
      "grad_norm": 0.2537526488304138,
      "learning_rate": 2.5441256937387635e-05,
      "loss": 0.534,
      "step": 10850
    },
    {
      "epoch": 0.2556135311016005,
      "grad_norm": 0.3213875889778137,
      "learning_rate": 2.5558508559368403e-05,
      "loss": 0.5392,
      "step": 10900
    },
    {
      "epoch": 0.2567860702350941,
      "grad_norm": 0.3063974976539612,
      "learning_rate": 2.5675760181349178e-05,
      "loss": 0.5159,
      "step": 10950
    },
    {
      "epoch": 0.25795860936858767,
      "grad_norm": 0.21824613213539124,
      "learning_rate": 2.579301180332995e-05,
      "loss": 0.4895,
      "step": 11000
    },
    {
      "epoch": 0.25913114850208124,
      "grad_norm": 0.273794949054718,
      "learning_rate": 2.5910263425310717e-05,
      "loss": 0.4877,
      "step": 11050
    },
    {
      "epoch": 0.2603036876355748,
      "grad_norm": 0.2345365434885025,
      "learning_rate": 2.6027515047291488e-05,
      "loss": 0.4939,
      "step": 11100
    },
    {
      "epoch": 0.2614762267690684,
      "grad_norm": 0.1491452306509018,
      "learning_rate": 2.614476666927226e-05,
      "loss": 0.4977,
      "step": 11150
    },
    {
      "epoch": 0.262648765902562,
      "grad_norm": 0.3017372786998749,
      "learning_rate": 2.6262018291253027e-05,
      "loss": 0.5366,
      "step": 11200
    },
    {
      "epoch": 0.26382130503605555,
      "grad_norm": 0.23020903766155243,
      "learning_rate": 2.63792699132338e-05,
      "loss": 0.5179,
      "step": 11250
    },
    {
      "epoch": 0.2649938441695492,
      "grad_norm": 0.30596038699150085,
      "learning_rate": 2.6496521535214573e-05,
      "loss": 0.5297,
      "step": 11300
    },
    {
      "epoch": 0.26616638330304276,
      "grad_norm": 0.30463674664497375,
      "learning_rate": 2.6613773157195344e-05,
      "loss": 0.4824,
      "step": 11350
    },
    {
      "epoch": 0.26733892243653634,
      "grad_norm": 0.22136160731315613,
      "learning_rate": 2.6731024779176112e-05,
      "loss": 0.4722,
      "step": 11400
    },
    {
      "epoch": 0.2685114615700299,
      "grad_norm": 0.33377256989479065,
      "learning_rate": 2.6848276401156884e-05,
      "loss": 0.5027,
      "step": 11450
    },
    {
      "epoch": 0.2696840007035235,
      "grad_norm": 0.35162556171417236,
      "learning_rate": 2.6965528023137655e-05,
      "loss": 0.4889,
      "step": 11500
    },
    {
      "epoch": 0.2708565398370171,
      "grad_norm": 0.3150629699230194,
      "learning_rate": 2.7082779645118423e-05,
      "loss": 0.4975,
      "step": 11550
    },
    {
      "epoch": 0.27202907897051065,
      "grad_norm": 0.2789730131626129,
      "learning_rate": 2.7200031267099197e-05,
      "loss": 0.487,
      "step": 11600
    },
    {
      "epoch": 0.27320161810400423,
      "grad_norm": 0.3228243887424469,
      "learning_rate": 2.731728288907997e-05,
      "loss": 0.4896,
      "step": 11650
    },
    {
      "epoch": 0.2743741572374978,
      "grad_norm": 0.35100987553596497,
      "learning_rate": 2.7434534511060737e-05,
      "loss": 0.499,
      "step": 11700
    },
    {
      "epoch": 0.2755466963709914,
      "grad_norm": 0.28384095430374146,
      "learning_rate": 2.7551786133041508e-05,
      "loss": 0.5153,
      "step": 11750
    },
    {
      "epoch": 0.27671923550448496,
      "grad_norm": 0.29180824756622314,
      "learning_rate": 2.766903775502228e-05,
      "loss": 0.4749,
      "step": 11800
    },
    {
      "epoch": 0.27789177463797854,
      "grad_norm": 0.2886105179786682,
      "learning_rate": 2.7786289377003047e-05,
      "loss": 0.5052,
      "step": 11850
    },
    {
      "epoch": 0.2790643137714721,
      "grad_norm": 0.34464338421821594,
      "learning_rate": 2.7903540998983818e-05,
      "loss": 0.5123,
      "step": 11900
    },
    {
      "epoch": 0.2802368529049657,
      "grad_norm": 0.1594526767730713,
      "learning_rate": 2.8020792620964593e-05,
      "loss": 0.4913,
      "step": 11950
    },
    {
      "epoch": 0.28140939203845927,
      "grad_norm": 0.3580065965652466,
      "learning_rate": 2.813804424294536e-05,
      "loss": 0.4705,
      "step": 12000
    },
    {
      "epoch": 0.28140939203845927,
      "eval_loss": 0.8141924738883972,
      "eval_runtime": 122.6665,
      "eval_samples_per_second": 81.522,
      "eval_steps_per_second": 20.38,
      "step": 12000
    },
    {
      "epoch": 0.28258193117195285,
      "grad_norm": 0.2223004400730133,
      "learning_rate": 2.8255295864926132e-05,
      "loss": 0.4726,
      "step": 12050
    },
    {
      "epoch": 0.2837544703054464,
      "grad_norm": 0.24553456902503967,
      "learning_rate": 2.8372547486906903e-05,
      "loss": 0.5037,
      "step": 12100
    },
    {
      "epoch": 0.28492700943894,
      "grad_norm": 0.23470573127269745,
      "learning_rate": 2.8489799108887675e-05,
      "loss": 0.4664,
      "step": 12150
    },
    {
      "epoch": 0.2860995485724336,
      "grad_norm": 0.20953722298145294,
      "learning_rate": 2.8607050730868442e-05,
      "loss": 0.502,
      "step": 12200
    },
    {
      "epoch": 0.28727208770592716,
      "grad_norm": 0.31200623512268066,
      "learning_rate": 2.8724302352849217e-05,
      "loss": 0.5209,
      "step": 12250
    },
    {
      "epoch": 0.2884446268394208,
      "grad_norm": 0.30846986174583435,
      "learning_rate": 2.884155397482999e-05,
      "loss": 0.4918,
      "step": 12300
    },
    {
      "epoch": 0.28961716597291437,
      "grad_norm": 0.4560887813568115,
      "learning_rate": 2.8958805596810756e-05,
      "loss": 0.5137,
      "step": 12350
    },
    {
      "epoch": 0.29078970510640795,
      "grad_norm": 0.1958955079317093,
      "learning_rate": 2.9076057218791527e-05,
      "loss": 0.4944,
      "step": 12400
    },
    {
      "epoch": 0.2919622442399015,
      "grad_norm": 0.28953370451927185,
      "learning_rate": 2.91933088407723e-05,
      "loss": 0.5048,
      "step": 12450
    },
    {
      "epoch": 0.2931347833733951,
      "grad_norm": 0.3418773114681244,
      "learning_rate": 2.9310560462753067e-05,
      "loss": 0.4835,
      "step": 12500
    },
    {
      "epoch": 0.2943073225068887,
      "grad_norm": 0.2875736653804779,
      "learning_rate": 2.9427812084733838e-05,
      "loss": 0.5311,
      "step": 12550
    },
    {
      "epoch": 0.29547986164038226,
      "grad_norm": 0.3252187669277191,
      "learning_rate": 2.9545063706714613e-05,
      "loss": 0.4945,
      "step": 12600
    },
    {
      "epoch": 0.29665240077387584,
      "grad_norm": 0.19073368608951569,
      "learning_rate": 2.966231532869538e-05,
      "loss": 0.4991,
      "step": 12650
    },
    {
      "epoch": 0.2978249399073694,
      "grad_norm": 0.27044478058815,
      "learning_rate": 2.9779566950676152e-05,
      "loss": 0.5015,
      "step": 12700
    },
    {
      "epoch": 0.298997479040863,
      "grad_norm": 0.3034335970878601,
      "learning_rate": 2.9896818572656923e-05,
      "loss": 0.4753,
      "step": 12750
    },
    {
      "epoch": 0.30017001817435657,
      "grad_norm": 0.19061709940433502,
      "learning_rate": 2.9999999798979092e-05,
      "loss": 0.5225,
      "step": 12800
    },
    {
      "epoch": 0.30134255730785015,
      "grad_norm": 0.22112636268138885,
      "learning_rate": 2.999998248884857e-05,
      "loss": 0.4936,
      "step": 12850
    },
    {
      "epoch": 0.3025150964413437,
      "grad_norm": 0.22272750735282898,
      "learning_rate": 2.999993725918413e-05,
      "loss": 0.4756,
      "step": 12900
    },
    {
      "epoch": 0.3036876355748373,
      "grad_norm": 0.3724925220012665,
      "learning_rate": 2.9999864110069948e-05,
      "loss": 0.4944,
      "step": 12950
    },
    {
      "epoch": 0.3048601747083309,
      "grad_norm": 0.21436628699302673,
      "learning_rate": 2.9999763041642185e-05,
      "loss": 0.4401,
      "step": 13000
    },
    {
      "epoch": 0.30603271384182446,
      "grad_norm": 0.2200484722852707,
      "learning_rate": 2.9999634054088952e-05,
      "loss": 0.489,
      "step": 13050
    },
    {
      "epoch": 0.30720525297531803,
      "grad_norm": 0.24810078740119934,
      "learning_rate": 2.9999477147650338e-05,
      "loss": 0.5088,
      "step": 13100
    },
    {
      "epoch": 0.3083777921088116,
      "grad_norm": 0.3103354573249817,
      "learning_rate": 2.9999292322618398e-05,
      "loss": 0.4677,
      "step": 13150
    },
    {
      "epoch": 0.3095503312423052,
      "grad_norm": 0.18475277721881866,
      "learning_rate": 2.999907957933714e-05,
      "loss": 0.4329,
      "step": 13200
    },
    {
      "epoch": 0.31072287037579877,
      "grad_norm": 0.27672964334487915,
      "learning_rate": 2.9998838918202555e-05,
      "loss": 0.459,
      "step": 13250
    },
    {
      "epoch": 0.3118954095092924,
      "grad_norm": 0.31807589530944824,
      "learning_rate": 2.9998570339662572e-05,
      "loss": 0.5297,
      "step": 13300
    },
    {
      "epoch": 0.313067948642786,
      "grad_norm": 0.30360880494117737,
      "learning_rate": 2.9998273844217105e-05,
      "loss": 0.5113,
      "step": 13350
    },
    {
      "epoch": 0.31424048777627955,
      "grad_norm": 0.16725032031536102,
      "learning_rate": 2.9997949432418024e-05,
      "loss": 0.4878,
      "step": 13400
    },
    {
      "epoch": 0.31541302690977313,
      "grad_norm": 0.34447139501571655,
      "learning_rate": 2.9997597104869158e-05,
      "loss": 0.5098,
      "step": 13450
    },
    {
      "epoch": 0.3165855660432667,
      "grad_norm": 0.16530463099479675,
      "learning_rate": 2.999721686222629e-05,
      "loss": 0.5225,
      "step": 13500
    },
    {
      "epoch": 0.3177581051767603,
      "grad_norm": 0.32370519638061523,
      "learning_rate": 2.9996808705197174e-05,
      "loss": 0.4886,
      "step": 13550
    },
    {
      "epoch": 0.31893064431025386,
      "grad_norm": 0.26941388845443726,
      "learning_rate": 2.999637263454151e-05,
      "loss": 0.5001,
      "step": 13600
    },
    {
      "epoch": 0.32010318344374744,
      "grad_norm": 0.20510739088058472,
      "learning_rate": 2.999590865107096e-05,
      "loss": 0.5029,
      "step": 13650
    },
    {
      "epoch": 0.321275722577241,
      "grad_norm": 0.26344040036201477,
      "learning_rate": 2.999541675564914e-05,
      "loss": 0.5284,
      "step": 13700
    },
    {
      "epoch": 0.3224482617107346,
      "grad_norm": 0.2993699908256531,
      "learning_rate": 2.9994896949191615e-05,
      "loss": 0.4705,
      "step": 13750
    },
    {
      "epoch": 0.3236208008442282,
      "grad_norm": 0.1939370483160019,
      "learning_rate": 2.99943492326659e-05,
      "loss": 0.4987,
      "step": 13800
    },
    {
      "epoch": 0.32479333997772175,
      "grad_norm": 0.26522117853164673,
      "learning_rate": 2.999377360709147e-05,
      "loss": 0.4922,
      "step": 13850
    },
    {
      "epoch": 0.32596587911121533,
      "grad_norm": 0.39479634165763855,
      "learning_rate": 2.9993170073539737e-05,
      "loss": 0.5228,
      "step": 13900
    },
    {
      "epoch": 0.3271384182447089,
      "grad_norm": 0.344303160905838,
      "learning_rate": 2.9992538633134057e-05,
      "loss": 0.4707,
      "step": 13950
    },
    {
      "epoch": 0.3283109573782025,
      "grad_norm": 0.3392077684402466,
      "learning_rate": 2.9991879287049734e-05,
      "loss": 0.4673,
      "step": 14000
    },
    {
      "epoch": 0.3283109573782025,
      "eval_loss": 0.8058537840843201,
      "eval_runtime": 121.8997,
      "eval_samples_per_second": 82.035,
      "eval_steps_per_second": 20.509,
      "step": 14000
    },
    {
      "epoch": 0.32948349651169606,
      "grad_norm": 0.2575781047344208,
      "learning_rate": 2.999119203651401e-05,
      "loss": 0.4894,
      "step": 14050
    },
    {
      "epoch": 0.33065603564518964,
      "grad_norm": 0.34225207567214966,
      "learning_rate": 2.999047688280608e-05,
      "loss": 0.533,
      "step": 14100
    },
    {
      "epoch": 0.3318285747786832,
      "grad_norm": 0.2801699638366699,
      "learning_rate": 2.998973382725704e-05,
      "loss": 0.5028,
      "step": 14150
    },
    {
      "epoch": 0.3330011139121768,
      "grad_norm": 0.3502732515335083,
      "learning_rate": 2.9988962871249965e-05,
      "loss": 0.5459,
      "step": 14200
    },
    {
      "epoch": 0.3341736530456704,
      "grad_norm": 0.347746342420578,
      "learning_rate": 2.9988164016219827e-05,
      "loss": 0.4954,
      "step": 14250
    },
    {
      "epoch": 0.335346192179164,
      "grad_norm": 0.20840997993946075,
      "learning_rate": 2.998733726365354e-05,
      "loss": 0.5061,
      "step": 14300
    },
    {
      "epoch": 0.3365187313126576,
      "grad_norm": 0.2491864114999771,
      "learning_rate": 2.998648261508994e-05,
      "loss": 0.5042,
      "step": 14350
    },
    {
      "epoch": 0.33769127044615116,
      "grad_norm": 0.23862312734127045,
      "learning_rate": 2.9985600072119797e-05,
      "loss": 0.5079,
      "step": 14400
    },
    {
      "epoch": 0.33886380957964474,
      "grad_norm": 0.27339231967926025,
      "learning_rate": 2.998468963638578e-05,
      "loss": 0.4751,
      "step": 14450
    },
    {
      "epoch": 0.3400363487131383,
      "grad_norm": 0.29509100317955017,
      "learning_rate": 2.9983751309582497e-05,
      "loss": 0.4497,
      "step": 14500
    },
    {
      "epoch": 0.3412088878466319,
      "grad_norm": 0.33595260977745056,
      "learning_rate": 2.9982785093456455e-05,
      "loss": 0.5043,
      "step": 14550
    },
    {
      "epoch": 0.34238142698012547,
      "grad_norm": 0.16355222463607788,
      "learning_rate": 2.9981790989806077e-05,
      "loss": 0.4753,
      "step": 14600
    },
    {
      "epoch": 0.34355396611361905,
      "grad_norm": 0.3352331817150116,
      "learning_rate": 2.9980769000481695e-05,
      "loss": 0.4572,
      "step": 14650
    },
    {
      "epoch": 0.3447265052471126,
      "grad_norm": 0.36153796315193176,
      "learning_rate": 2.997971912738554e-05,
      "loss": 0.4832,
      "step": 14700
    },
    {
      "epoch": 0.3458990443806062,
      "grad_norm": 0.17264434695243835,
      "learning_rate": 2.997864137247174e-05,
      "loss": 0.5106,
      "step": 14750
    },
    {
      "epoch": 0.3470715835140998,
      "grad_norm": 0.33088111877441406,
      "learning_rate": 2.9977535737746342e-05,
      "loss": 0.4535,
      "step": 14800
    },
    {
      "epoch": 0.34824412264759336,
      "grad_norm": 0.29733479022979736,
      "learning_rate": 2.997640222526725e-05,
      "loss": 0.4828,
      "step": 14850
    },
    {
      "epoch": 0.34941666178108693,
      "grad_norm": 0.22853246331214905,
      "learning_rate": 2.9975240837144285e-05,
      "loss": 0.4883,
      "step": 14900
    },
    {
      "epoch": 0.3505892009145805,
      "grad_norm": 0.2090832144021988,
      "learning_rate": 2.997405157553914e-05,
      "loss": 0.4837,
      "step": 14950
    },
    {
      "epoch": 0.3517617400480741,
      "grad_norm": 0.3866957128047943,
      "learning_rate": 2.9972834442665397e-05,
      "loss": 0.4915,
      "step": 15000
    },
    {
      "epoch": 0.35293427918156767,
      "grad_norm": 0.3718699812889099,
      "learning_rate": 2.9971589440788508e-05,
      "loss": 0.4486,
      "step": 15050
    },
    {
      "epoch": 0.35410681831506124,
      "grad_norm": 0.29235759377479553,
      "learning_rate": 2.9970316572225803e-05,
      "loss": 0.4855,
      "step": 15100
    },
    {
      "epoch": 0.3552793574485548,
      "grad_norm": 0.31858325004577637,
      "learning_rate": 2.9969015839346477e-05,
      "loss": 0.5096,
      "step": 15150
    },
    {
      "epoch": 0.3564518965820484,
      "grad_norm": 0.22856444120407104,
      "learning_rate": 2.9967687244571587e-05,
      "loss": 0.4961,
      "step": 15200
    },
    {
      "epoch": 0.35762443571554203,
      "grad_norm": 0.3120279312133789,
      "learning_rate": 2.9966330790374056e-05,
      "loss": 0.4922,
      "step": 15250
    },
    {
      "epoch": 0.3587969748490356,
      "grad_norm": 0.27397051453590393,
      "learning_rate": 2.9964946479278656e-05,
      "loss": 0.5054,
      "step": 15300
    },
    {
      "epoch": 0.3599695139825292,
      "grad_norm": 0.20500780642032623,
      "learning_rate": 2.9963534313862012e-05,
      "loss": 0.4875,
      "step": 15350
    },
    {
      "epoch": 0.36114205311602277,
      "grad_norm": 0.17303122580051422,
      "learning_rate": 2.9962094296752595e-05,
      "loss": 0.5082,
      "step": 15400
    },
    {
      "epoch": 0.36231459224951634,
      "grad_norm": 0.3254593312740326,
      "learning_rate": 2.9960626430630714e-05,
      "loss": 0.4956,
      "step": 15450
    },
    {
      "epoch": 0.3634871313830099,
      "grad_norm": 0.24950747191905975,
      "learning_rate": 2.9959130718228514e-05,
      "loss": 0.4671,
      "step": 15500
    },
    {
      "epoch": 0.3646596705165035,
      "grad_norm": 0.1810421645641327,
      "learning_rate": 2.9957607162329973e-05,
      "loss": 0.494,
      "step": 15550
    },
    {
      "epoch": 0.3658322096499971,
      "grad_norm": 0.4048076272010803,
      "learning_rate": 2.9956055765770886e-05,
      "loss": 0.4827,
      "step": 15600
    },
    {
      "epoch": 0.36700474878349065,
      "grad_norm": 0.3844906687736511,
      "learning_rate": 2.9954476531438887e-05,
      "loss": 0.4729,
      "step": 15650
    },
    {
      "epoch": 0.36817728791698423,
      "grad_norm": 0.20737223327159882,
      "learning_rate": 2.9952869462273406e-05,
      "loss": 0.4709,
      "step": 15700
    },
    {
      "epoch": 0.3693498270504778,
      "grad_norm": 0.5010440349578857,
      "learning_rate": 2.9951234561265676e-05,
      "loss": 0.5041,
      "step": 15750
    },
    {
      "epoch": 0.3705223661839714,
      "grad_norm": 0.3981720507144928,
      "learning_rate": 2.9949571831458765e-05,
      "loss": 0.516,
      "step": 15800
    },
    {
      "epoch": 0.37169490531746496,
      "grad_norm": 0.3822137415409088,
      "learning_rate": 2.9947881275947508e-05,
      "loss": 0.4708,
      "step": 15850
    },
    {
      "epoch": 0.37286744445095854,
      "grad_norm": 0.25633135437965393,
      "learning_rate": 2.9946162897878547e-05,
      "loss": 0.4931,
      "step": 15900
    },
    {
      "epoch": 0.3740399835844521,
      "grad_norm": 0.3026624023914337,
      "learning_rate": 2.9944416700450302e-05,
      "loss": 0.466,
      "step": 15950
    },
    {
      "epoch": 0.3752125227179457,
      "grad_norm": 0.4629914462566376,
      "learning_rate": 2.994264268691298e-05,
      "loss": 0.511,
      "step": 16000
    },
    {
      "epoch": 0.3752125227179457,
      "eval_loss": 0.8005845546722412,
      "eval_runtime": 121.5401,
      "eval_samples_per_second": 82.277,
      "eval_steps_per_second": 20.569,
      "step": 16000
    },
    {
      "epoch": 0.3763850618514393,
      "grad_norm": 0.21592098474502563,
      "learning_rate": 2.9940840860568567e-05,
      "loss": 0.4444,
      "step": 16050
    },
    {
      "epoch": 0.37755760098493285,
      "grad_norm": 0.21082580089569092,
      "learning_rate": 2.9939011224770807e-05,
      "loss": 0.5502,
      "step": 16100
    },
    {
      "epoch": 0.37873014011842643,
      "grad_norm": 0.29114484786987305,
      "learning_rate": 2.99371537829252e-05,
      "loss": 0.5469,
      "step": 16150
    },
    {
      "epoch": 0.37990267925192,
      "grad_norm": 0.244945228099823,
      "learning_rate": 2.9935268538489023e-05,
      "loss": 0.472,
      "step": 16200
    },
    {
      "epoch": 0.38107521838541364,
      "grad_norm": 0.4284784495830536,
      "learning_rate": 2.9933355494971286e-05,
      "loss": 0.4772,
      "step": 16250
    },
    {
      "epoch": 0.3822477575189072,
      "grad_norm": 0.3536072373390198,
      "learning_rate": 2.9931414655932737e-05,
      "loss": 0.4743,
      "step": 16300
    },
    {
      "epoch": 0.3834202966524008,
      "grad_norm": 0.2532188594341278,
      "learning_rate": 2.9929446024985887e-05,
      "loss": 0.5004,
      "step": 16350
    },
    {
      "epoch": 0.38459283578589437,
      "grad_norm": 0.3445676565170288,
      "learning_rate": 2.9927449605794936e-05,
      "loss": 0.482,
      "step": 16400
    },
    {
      "epoch": 0.38576537491938795,
      "grad_norm": 0.21449539065361023,
      "learning_rate": 2.9925425402075845e-05,
      "loss": 0.4661,
      "step": 16450
    },
    {
      "epoch": 0.3869379140528815,
      "grad_norm": 0.2393672913312912,
      "learning_rate": 2.992337341759626e-05,
      "loss": 0.4984,
      "step": 16500
    },
    {
      "epoch": 0.3881104531863751,
      "grad_norm": 0.2563740015029907,
      "learning_rate": 2.992129365617556e-05,
      "loss": 0.473,
      "step": 16550
    },
    {
      "epoch": 0.3892829923198687,
      "grad_norm": 0.2511592209339142,
      "learning_rate": 2.991918612168481e-05,
      "loss": 0.498,
      "step": 16600
    },
    {
      "epoch": 0.39045553145336226,
      "grad_norm": 0.4582075774669647,
      "learning_rate": 2.991705081804677e-05,
      "loss": 0.4648,
      "step": 16650
    },
    {
      "epoch": 0.39162807058685584,
      "grad_norm": 0.2982787489891052,
      "learning_rate": 2.9914887749235894e-05,
      "loss": 0.4899,
      "step": 16700
    },
    {
      "epoch": 0.3928006097203494,
      "grad_norm": 0.3166966140270233,
      "learning_rate": 2.991269691927831e-05,
      "loss": 0.4986,
      "step": 16750
    },
    {
      "epoch": 0.393973148853843,
      "grad_norm": 0.14394865930080414,
      "learning_rate": 2.991047833225182e-05,
      "loss": 0.4767,
      "step": 16800
    },
    {
      "epoch": 0.39514568798733657,
      "grad_norm": 0.3170274794101715,
      "learning_rate": 2.9908231992285888e-05,
      "loss": 0.4861,
      "step": 16850
    },
    {
      "epoch": 0.39631822712083015,
      "grad_norm": 0.5157034993171692,
      "learning_rate": 2.9905957903561643e-05,
      "loss": 0.4769,
      "step": 16900
    },
    {
      "epoch": 0.3974907662543237,
      "grad_norm": 0.28623470664024353,
      "learning_rate": 2.990365607031185e-05,
      "loss": 0.5033,
      "step": 16950
    },
    {
      "epoch": 0.3986633053878173,
      "grad_norm": 3.3057358264923096,
      "learning_rate": 2.9901326496820916e-05,
      "loss": 0.4968,
      "step": 17000
    },
    {
      "epoch": 0.3998358445213109,
      "grad_norm": 0.40848150849342346,
      "learning_rate": 2.9898969187424903e-05,
      "loss": 0.4541,
      "step": 17050
    },
    {
      "epoch": 0.40100838365480446,
      "grad_norm": 0.3876121938228607,
      "learning_rate": 2.989658414651147e-05,
      "loss": 0.5292,
      "step": 17100
    },
    {
      "epoch": 0.40218092278829803,
      "grad_norm": 0.35061877965927124,
      "learning_rate": 2.9894171378519905e-05,
      "loss": 0.5188,
      "step": 17150
    },
    {
      "epoch": 0.40335346192179167,
      "grad_norm": 0.28072527050971985,
      "learning_rate": 2.98917308879411e-05,
      "loss": 0.4793,
      "step": 17200
    },
    {
      "epoch": 0.40452600105528524,
      "grad_norm": 0.4160352945327759,
      "learning_rate": 2.9889262679317565e-05,
      "loss": 0.4799,
      "step": 17250
    },
    {
      "epoch": 0.4056985401887788,
      "grad_norm": 0.21187201142311096,
      "learning_rate": 2.9886766757243376e-05,
      "loss": 0.4562,
      "step": 17300
    },
    {
      "epoch": 0.4068710793222724,
      "grad_norm": 0.29092639684677124,
      "learning_rate": 2.9884243126364202e-05,
      "loss": 0.475,
      "step": 17350
    },
    {
      "epoch": 0.408043618455766,
      "grad_norm": 0.23138418793678284,
      "learning_rate": 2.9881691791377297e-05,
      "loss": 0.4747,
      "step": 17400
    },
    {
      "epoch": 0.40921615758925955,
      "grad_norm": 0.3205435872077942,
      "learning_rate": 2.9879112757031464e-05,
      "loss": 0.447,
      "step": 17450
    },
    {
      "epoch": 0.41038869672275313,
      "grad_norm": 0.36186251044273376,
      "learning_rate": 2.987650602812708e-05,
      "loss": 0.4936,
      "step": 17500
    },
    {
      "epoch": 0.4115612358562467,
      "grad_norm": 0.2020445317029953,
      "learning_rate": 2.987387160951606e-05,
      "loss": 0.4724,
      "step": 17550
    },
    {
      "epoch": 0.4127337749897403,
      "grad_norm": 0.33478260040283203,
      "learning_rate": 2.9871209506101845e-05,
      "loss": 0.4608,
      "step": 17600
    },
    {
      "epoch": 0.41390631412323386,
      "grad_norm": 0.2579348385334015,
      "learning_rate": 2.9868519722839434e-05,
      "loss": 0.4824,
      "step": 17650
    },
    {
      "epoch": 0.41507885325672744,
      "grad_norm": 0.3205919861793518,
      "learning_rate": 2.9865802264735326e-05,
      "loss": 0.4983,
      "step": 17700
    },
    {
      "epoch": 0.416251392390221,
      "grad_norm": 0.22923895716667175,
      "learning_rate": 2.9863057136847545e-05,
      "loss": 0.4392,
      "step": 17750
    },
    {
      "epoch": 0.4174239315237146,
      "grad_norm": 0.303694486618042,
      "learning_rate": 2.9860284344285604e-05,
      "loss": 0.4798,
      "step": 17800
    },
    {
      "epoch": 0.4185964706572082,
      "grad_norm": 0.4377165138721466,
      "learning_rate": 2.9857483892210514e-05,
      "loss": 0.4556,
      "step": 17850
    },
    {
      "epoch": 0.41976900979070175,
      "grad_norm": 0.29674357175827026,
      "learning_rate": 2.985465578583477e-05,
      "loss": 0.4844,
      "step": 17900
    },
    {
      "epoch": 0.42094154892419533,
      "grad_norm": 0.15862292051315308,
      "learning_rate": 2.985180003042234e-05,
      "loss": 0.46,
      "step": 17950
    },
    {
      "epoch": 0.4221140880576889,
      "grad_norm": 0.2965388000011444,
      "learning_rate": 2.984891663128865e-05,
      "loss": 0.4757,
      "step": 18000
    },
    {
      "epoch": 0.4221140880576889,
      "eval_loss": 0.7969109416007996,
      "eval_runtime": 120.6437,
      "eval_samples_per_second": 82.889,
      "eval_steps_per_second": 20.722,
      "step": 18000
    },
    {
      "epoch": 0.4232866271911825,
      "grad_norm": 0.3626846969127655,
      "learning_rate": 2.984600559380059e-05,
      "loss": 0.4533,
      "step": 18050
    },
    {
      "epoch": 0.42445916632467606,
      "grad_norm": 0.5336629748344421,
      "learning_rate": 2.9843066923376487e-05,
      "loss": 0.4616,
      "step": 18100
    },
    {
      "epoch": 0.42563170545816964,
      "grad_norm": 0.3884088099002838,
      "learning_rate": 2.984010062548609e-05,
      "loss": 0.5217,
      "step": 18150
    },
    {
      "epoch": 0.4268042445916633,
      "grad_norm": 0.2271450161933899,
      "learning_rate": 2.9837106705650594e-05,
      "loss": 0.455,
      "step": 18200
    },
    {
      "epoch": 0.42797678372515685,
      "grad_norm": 0.23317013680934906,
      "learning_rate": 2.983408516944259e-05,
      "loss": 0.4989,
      "step": 18250
    },
    {
      "epoch": 0.42914932285865043,
      "grad_norm": 0.14878128468990326,
      "learning_rate": 2.9831036022486083e-05,
      "loss": 0.482,
      "step": 18300
    },
    {
      "epoch": 0.430321861992144,
      "grad_norm": 0.39159926772117615,
      "learning_rate": 2.9827959270456455e-05,
      "loss": 0.4943,
      "step": 18350
    },
    {
      "epoch": 0.4314944011256376,
      "grad_norm": 0.23881377279758453,
      "learning_rate": 2.982485491908049e-05,
      "loss": 0.4688,
      "step": 18400
    },
    {
      "epoch": 0.43266694025913116,
      "grad_norm": 0.5011515617370605,
      "learning_rate": 2.9821722974136316e-05,
      "loss": 0.4884,
      "step": 18450
    },
    {
      "epoch": 0.43383947939262474,
      "grad_norm": 0.4071427583694458,
      "learning_rate": 2.981856344145344e-05,
      "loss": 0.5046,
      "step": 18500
    },
    {
      "epoch": 0.4350120185261183,
      "grad_norm": 0.17783913016319275,
      "learning_rate": 2.9815376326912725e-05,
      "loss": 0.4729,
      "step": 18550
    },
    {
      "epoch": 0.4361845576596119,
      "grad_norm": 0.3046351671218872,
      "learning_rate": 2.9812161636446352e-05,
      "loss": 0.4998,
      "step": 18600
    },
    {
      "epoch": 0.43735709679310547,
      "grad_norm": 0.2847333550453186,
      "learning_rate": 2.980891937603784e-05,
      "loss": 0.4955,
      "step": 18650
    },
    {
      "epoch": 0.43852963592659905,
      "grad_norm": 0.41737592220306396,
      "learning_rate": 2.9805649551722024e-05,
      "loss": 0.4924,
      "step": 18700
    },
    {
      "epoch": 0.4397021750600926,
      "grad_norm": 0.27503839135169983,
      "learning_rate": 2.9802352169585042e-05,
      "loss": 0.5262,
      "step": 18750
    },
    {
      "epoch": 0.4408747141935862,
      "grad_norm": 0.37596672773361206,
      "learning_rate": 2.9799027235764327e-05,
      "loss": 0.4745,
      "step": 18800
    },
    {
      "epoch": 0.4420472533270798,
      "grad_norm": 0.45076730847358704,
      "learning_rate": 2.979567475644859e-05,
      "loss": 0.4802,
      "step": 18850
    },
    {
      "epoch": 0.44321979246057336,
      "grad_norm": 0.29721492528915405,
      "learning_rate": 2.9792294737877813e-05,
      "loss": 0.4602,
      "step": 18900
    },
    {
      "epoch": 0.44439233159406694,
      "grad_norm": 0.3136778771877289,
      "learning_rate": 2.9788887186343254e-05,
      "loss": 0.5019,
      "step": 18950
    },
    {
      "epoch": 0.4455648707275605,
      "grad_norm": 0.2558610439300537,
      "learning_rate": 2.9785452108187385e-05,
      "loss": 0.4903,
      "step": 19000
    },
    {
      "epoch": 0.4467374098610541,
      "grad_norm": 0.38380295038223267,
      "learning_rate": 2.9781989509803947e-05,
      "loss": 0.4996,
      "step": 19050
    },
    {
      "epoch": 0.44790994899454767,
      "grad_norm": 0.16458436846733093,
      "learning_rate": 2.9778499397637883e-05,
      "loss": 0.4479,
      "step": 19100
    },
    {
      "epoch": 0.44908248812804125,
      "grad_norm": 0.3447117209434509,
      "learning_rate": 2.977498177818535e-05,
      "loss": 0.462,
      "step": 19150
    },
    {
      "epoch": 0.4502550272615349,
      "grad_norm": 0.2601374089717865,
      "learning_rate": 2.977143665799372e-05,
      "loss": 0.4413,
      "step": 19200
    },
    {
      "epoch": 0.45142756639502846,
      "grad_norm": 0.32913491129875183,
      "learning_rate": 2.976786404366153e-05,
      "loss": 0.454,
      "step": 19250
    },
    {
      "epoch": 0.45260010552852203,
      "grad_norm": 0.32290220260620117,
      "learning_rate": 2.976426394183851e-05,
      "loss": 0.4784,
      "step": 19300
    },
    {
      "epoch": 0.4537726446620156,
      "grad_norm": 0.33864736557006836,
      "learning_rate": 2.976063635922554e-05,
      "loss": 0.4795,
      "step": 19350
    },
    {
      "epoch": 0.4549451837955092,
      "grad_norm": 0.4241345524787903,
      "learning_rate": 2.9756981302574664e-05,
      "loss": 0.4901,
      "step": 19400
    },
    {
      "epoch": 0.45611772292900277,
      "grad_norm": 0.25264352560043335,
      "learning_rate": 2.9753298778689043e-05,
      "loss": 0.5024,
      "step": 19450
    },
    {
      "epoch": 0.45729026206249634,
      "grad_norm": 0.27421945333480835,
      "learning_rate": 2.9749588794422987e-05,
      "loss": 0.4986,
      "step": 19500
    },
    {
      "epoch": 0.4584628011959899,
      "grad_norm": 0.34056082367897034,
      "learning_rate": 2.97458513566819e-05,
      "loss": 0.4692,
      "step": 19550
    },
    {
      "epoch": 0.4596353403294835,
      "grad_norm": 0.2298508882522583,
      "learning_rate": 2.9742086472422292e-05,
      "loss": 0.4733,
      "step": 19600
    },
    {
      "epoch": 0.4608078794629771,
      "grad_norm": 0.27686336636543274,
      "learning_rate": 2.9738294148651758e-05,
      "loss": 0.4778,
      "step": 19650
    },
    {
      "epoch": 0.46198041859647065,
      "grad_norm": 0.5162282586097717,
      "learning_rate": 2.973447439242897e-05,
      "loss": 0.5201,
      "step": 19700
    },
    {
      "epoch": 0.46315295772996423,
      "grad_norm": 0.44300875067710876,
      "learning_rate": 2.973062721086366e-05,
      "loss": 0.4871,
      "step": 19750
    },
    {
      "epoch": 0.4643254968634578,
      "grad_norm": 0.2708766460418701,
      "learning_rate": 2.9726752611116592e-05,
      "loss": 0.4811,
      "step": 19800
    },
    {
      "epoch": 0.4654980359969514,
      "grad_norm": 0.5985651016235352,
      "learning_rate": 2.972285060039959e-05,
      "loss": 0.4507,
      "step": 19850
    },
    {
      "epoch": 0.46667057513044496,
      "grad_norm": 0.32588356733322144,
      "learning_rate": 2.9718921185975464e-05,
      "loss": 0.4963,
      "step": 19900
    },
    {
      "epoch": 0.46784311426393854,
      "grad_norm": 0.3642570376396179,
      "learning_rate": 2.9714964375158076e-05,
      "loss": 0.4739,
      "step": 19950
    },
    {
      "epoch": 0.4690156533974321,
      "grad_norm": 0.3567453920841217,
      "learning_rate": 2.971098017531224e-05,
      "loss": 0.4785,
      "step": 20000
    },
    {
      "epoch": 0.4690156533974321,
      "eval_loss": 0.794750988483429,
      "eval_runtime": 120.9909,
      "eval_samples_per_second": 82.651,
      "eval_steps_per_second": 20.663,
      "step": 20000
    },
    {
      "epoch": 0.4701881925309257,
      "grad_norm": 0.2540343701839447,
      "learning_rate": 2.970696859385377e-05,
      "loss": 0.4867,
      "step": 20050
    },
    {
      "epoch": 0.4713607316644193,
      "grad_norm": 0.2123883068561554,
      "learning_rate": 2.9702929638249435e-05,
      "loss": 0.477,
      "step": 20100
    },
    {
      "epoch": 0.4725332707979129,
      "grad_norm": 0.42212167382240295,
      "learning_rate": 2.969886331601696e-05,
      "loss": 0.4894,
      "step": 20150
    },
    {
      "epoch": 0.4737058099314065,
      "grad_norm": 0.425202339887619,
      "learning_rate": 2.9694769634725017e-05,
      "loss": 0.5165,
      "step": 20200
    },
    {
      "epoch": 0.47487834906490006,
      "grad_norm": 0.42110902070999146,
      "learning_rate": 2.9690648601993195e-05,
      "loss": 0.4586,
      "step": 20250
    },
    {
      "epoch": 0.47605088819839364,
      "grad_norm": 0.36126288771629333,
      "learning_rate": 2.9686500225491976e-05,
      "loss": 0.4892,
      "step": 20300
    },
    {
      "epoch": 0.4772234273318872,
      "grad_norm": 0.2303113043308258,
      "learning_rate": 2.9682324512942768e-05,
      "loss": 0.4951,
      "step": 20350
    },
    {
      "epoch": 0.4783959664653808,
      "grad_norm": 0.1956986039876938,
      "learning_rate": 2.967812147211783e-05,
      "loss": 0.4553,
      "step": 20400
    },
    {
      "epoch": 0.4795685055988744,
      "grad_norm": 0.2649708688259125,
      "learning_rate": 2.9673891110840308e-05,
      "loss": 0.4574,
      "step": 20450
    },
    {
      "epoch": 0.48074104473236795,
      "grad_norm": 0.43180856108665466,
      "learning_rate": 2.966963343698419e-05,
      "loss": 0.466,
      "step": 20500
    },
    {
      "epoch": 0.48191358386586153,
      "grad_norm": 0.3139708340167999,
      "learning_rate": 2.96653484584743e-05,
      "loss": 0.4803,
      "step": 20550
    },
    {
      "epoch": 0.4830861229993551,
      "grad_norm": 0.2514185607433319,
      "learning_rate": 2.9661036183286293e-05,
      "loss": 0.4725,
      "step": 20600
    },
    {
      "epoch": 0.4842586621328487,
      "grad_norm": 0.42923393845558167,
      "learning_rate": 2.9656696619446625e-05,
      "loss": 0.4853,
      "step": 20650
    },
    {
      "epoch": 0.48543120126634226,
      "grad_norm": 0.25491100549697876,
      "learning_rate": 2.965232977503254e-05,
      "loss": 0.4741,
      "step": 20700
    },
    {
      "epoch": 0.48660374039983584,
      "grad_norm": 0.20702658593654633,
      "learning_rate": 2.9647935658172075e-05,
      "loss": 0.4647,
      "step": 20750
    },
    {
      "epoch": 0.4877762795333294,
      "grad_norm": 0.37718555331230164,
      "learning_rate": 2.964351427704401e-05,
      "loss": 0.5246,
      "step": 20800
    },
    {
      "epoch": 0.488948818666823,
      "grad_norm": 0.3388359844684601,
      "learning_rate": 2.9639065639877887e-05,
      "loss": 0.4703,
      "step": 20850
    },
    {
      "epoch": 0.49012135780031657,
      "grad_norm": 0.44041186571121216,
      "learning_rate": 2.9634589754953967e-05,
      "loss": 0.4767,
      "step": 20900
    },
    {
      "epoch": 0.49129389693381015,
      "grad_norm": 0.2304011434316635,
      "learning_rate": 2.9630086630603244e-05,
      "loss": 0.4627,
      "step": 20950
    },
    {
      "epoch": 0.4924664360673037,
      "grad_norm": 0.2790350914001465,
      "learning_rate": 2.9625556275207397e-05,
      "loss": 0.4817,
      "step": 21000
    },
    {
      "epoch": 0.4936389752007973,
      "grad_norm": 0.3041628301143646,
      "learning_rate": 2.9620998697198797e-05,
      "loss": 0.4651,
      "step": 21050
    },
    {
      "epoch": 0.4948115143342909,
      "grad_norm": 0.2655928432941437,
      "learning_rate": 2.9616413905060483e-05,
      "loss": 0.5033,
      "step": 21100
    },
    {
      "epoch": 0.4959840534677845,
      "grad_norm": 0.46364375948905945,
      "learning_rate": 2.961180190732615e-05,
      "loss": 0.4724,
      "step": 21150
    },
    {
      "epoch": 0.4971565926012781,
      "grad_norm": 0.23813514411449432,
      "learning_rate": 2.960716271258014e-05,
      "loss": 0.4845,
      "step": 21200
    },
    {
      "epoch": 0.49832913173477167,
      "grad_norm": 0.34603774547576904,
      "learning_rate": 2.960249632945739e-05,
      "loss": 0.4604,
      "step": 21250
    },
    {
      "epoch": 0.49950167086826525,
      "grad_norm": 0.32372498512268066,
      "learning_rate": 2.959780276664347e-05,
      "loss": 0.4776,
      "step": 21300
    },
    {
      "epoch": 0.5006742100017588,
      "grad_norm": 0.3480619192123413,
      "learning_rate": 2.9593082032874528e-05,
      "loss": 0.5015,
      "step": 21350
    },
    {
      "epoch": 0.5018467491352524,
      "grad_norm": 0.1856486052274704,
      "learning_rate": 2.9588334136937285e-05,
      "loss": 0.492,
      "step": 21400
    },
    {
      "epoch": 0.503019288268746,
      "grad_norm": 0.24231408536434174,
      "learning_rate": 2.9583559087669018e-05,
      "loss": 0.4674,
      "step": 21450
    },
    {
      "epoch": 0.5041918274022396,
      "grad_norm": 0.3315410614013672,
      "learning_rate": 2.957875689395755e-05,
      "loss": 0.4695,
      "step": 21500
    },
    {
      "epoch": 0.5053643665357331,
      "grad_norm": 0.29595479369163513,
      "learning_rate": 2.957392756474123e-05,
      "loss": 0.4968,
      "step": 21550
    },
    {
      "epoch": 0.5065369056692267,
      "grad_norm": 0.378288596868515,
      "learning_rate": 2.9569071109008904e-05,
      "loss": 0.4738,
      "step": 21600
    },
    {
      "epoch": 0.5077094448027203,
      "grad_norm": 0.3112930953502655,
      "learning_rate": 2.956418753579992e-05,
      "loss": 0.4715,
      "step": 21650
    },
    {
      "epoch": 0.5088819839362139,
      "grad_norm": 0.292670875787735,
      "learning_rate": 2.9559276854204085e-05,
      "loss": 0.4677,
      "step": 21700
    },
    {
      "epoch": 0.5100545230697074,
      "grad_norm": 0.29573991894721985,
      "learning_rate": 2.955433907336168e-05,
      "loss": 0.4657,
      "step": 21750
    },
    {
      "epoch": 0.511227062203201,
      "grad_norm": 0.2219734638929367,
      "learning_rate": 2.9549374202463413e-05,
      "loss": 0.4801,
      "step": 21800
    },
    {
      "epoch": 0.5123996013366946,
      "grad_norm": 0.34638747572898865,
      "learning_rate": 2.954438225075043e-05,
      "loss": 0.4583,
      "step": 21850
    },
    {
      "epoch": 0.5135721404701882,
      "grad_norm": 0.3343934118747711,
      "learning_rate": 2.9539363227514265e-05,
      "loss": 0.476,
      "step": 21900
    },
    {
      "epoch": 0.5147446796036818,
      "grad_norm": 0.3343493938446045,
      "learning_rate": 2.953431714209685e-05,
      "loss": 0.4568,
      "step": 21950
    },
    {
      "epoch": 0.5159172187371753,
      "grad_norm": 0.22986240684986115,
      "learning_rate": 2.9529244003890485e-05,
      "loss": 0.4532,
      "step": 22000
    },
    {
      "epoch": 0.5159172187371753,
      "eval_loss": 0.7913773059844971,
      "eval_runtime": 148.8923,
      "eval_samples_per_second": 67.163,
      "eval_steps_per_second": 16.791,
      "step": 22000
    },
    {
      "epoch": 0.5170897578706689,
      "grad_norm": 0.23129621148109436,
      "learning_rate": 2.9524143822337825e-05,
      "loss": 0.4436,
      "step": 22050
    },
    {
      "epoch": 0.5182622970041625,
      "grad_norm": 0.3499422073364258,
      "learning_rate": 2.9519016606931863e-05,
      "loss": 0.5247,
      "step": 22100
    },
    {
      "epoch": 0.5194348361376561,
      "grad_norm": 0.3560042083263397,
      "learning_rate": 2.9513862367215906e-05,
      "loss": 0.4881,
      "step": 22150
    },
    {
      "epoch": 0.5206073752711496,
      "grad_norm": 0.20761001110076904,
      "learning_rate": 2.950868111278356e-05,
      "loss": 0.4767,
      "step": 22200
    },
    {
      "epoch": 0.5217799144046432,
      "grad_norm": 0.24046359956264496,
      "learning_rate": 2.9503472853278724e-05,
      "loss": 0.4397,
      "step": 22250
    },
    {
      "epoch": 0.5229524535381368,
      "grad_norm": 0.3570183515548706,
      "learning_rate": 2.949823759839555e-05,
      "loss": 0.4634,
      "step": 22300
    },
    {
      "epoch": 0.5241249926716304,
      "grad_norm": 0.2595347762107849,
      "learning_rate": 2.949297535787844e-05,
      "loss": 0.4365,
      "step": 22350
    },
    {
      "epoch": 0.525297531805124,
      "grad_norm": 0.2884335517883301,
      "learning_rate": 2.948768614152203e-05,
      "loss": 0.4632,
      "step": 22400
    },
    {
      "epoch": 0.5264700709386175,
      "grad_norm": 0.2214239537715912,
      "learning_rate": 2.9482369959171157e-05,
      "loss": 0.473,
      "step": 22450
    },
    {
      "epoch": 0.5276426100721111,
      "grad_norm": 0.26075106859207153,
      "learning_rate": 2.9477026820720855e-05,
      "loss": 0.4591,
      "step": 22500
    },
    {
      "epoch": 0.5288151492056047,
      "grad_norm": 0.1932675689458847,
      "learning_rate": 2.947165673611634e-05,
      "loss": 0.463,
      "step": 22550
    },
    {
      "epoch": 0.5299876883390984,
      "grad_norm": 0.16102346777915955,
      "learning_rate": 2.9466259715352962e-05,
      "loss": 0.4727,
      "step": 22600
    },
    {
      "epoch": 0.531160227472592,
      "grad_norm": 0.36579954624176025,
      "learning_rate": 2.9460835768476225e-05,
      "loss": 0.4546,
      "step": 22650
    },
    {
      "epoch": 0.5323327666060855,
      "grad_norm": 0.2570153772830963,
      "learning_rate": 2.945538490558175e-05,
      "loss": 0.4573,
      "step": 22700
    },
    {
      "epoch": 0.5335053057395791,
      "grad_norm": 0.22080808877944946,
      "learning_rate": 2.9449907136815245e-05,
      "loss": 0.4625,
      "step": 22750
    },
    {
      "epoch": 0.5346778448730727,
      "grad_norm": 0.3933480381965637,
      "learning_rate": 2.9444402472372512e-05,
      "loss": 0.4517,
      "step": 22800
    },
    {
      "epoch": 0.5358503840065663,
      "grad_norm": 0.5154486894607544,
      "learning_rate": 2.94388709224994e-05,
      "loss": 0.4811,
      "step": 22850
    },
    {
      "epoch": 0.5370229231400598,
      "grad_norm": 0.3245607316493988,
      "learning_rate": 2.943331249749181e-05,
      "loss": 0.4902,
      "step": 22900
    },
    {
      "epoch": 0.5381954622735534,
      "grad_norm": 0.3587605953216553,
      "learning_rate": 2.9427727207695664e-05,
      "loss": 0.4829,
      "step": 22950
    },
    {
      "epoch": 0.539368001407047,
      "grad_norm": 0.3341209590435028,
      "learning_rate": 2.942211506350689e-05,
      "loss": 0.4525,
      "step": 23000
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 0.3502277135848999,
      "learning_rate": 2.941647607537139e-05,
      "loss": 0.4774,
      "step": 23050
    },
    {
      "epoch": 0.5417130796740341,
      "grad_norm": 0.3815761208534241,
      "learning_rate": 2.9410810253785044e-05,
      "loss": 0.4716,
      "step": 23100
    },
    {
      "epoch": 0.5428856188075277,
      "grad_norm": 0.2682870626449585,
      "learning_rate": 2.9405117609293666e-05,
      "loss": 0.504,
      "step": 23150
    },
    {
      "epoch": 0.5440581579410213,
      "grad_norm": 0.4583728015422821,
      "learning_rate": 2.9399398152493e-05,
      "loss": 0.4731,
      "step": 23200
    },
    {
      "epoch": 0.5452306970745149,
      "grad_norm": 0.25008195638656616,
      "learning_rate": 2.9393651894028706e-05,
      "loss": 0.5026,
      "step": 23250
    },
    {
      "epoch": 0.5464032362080085,
      "grad_norm": 0.17283391952514648,
      "learning_rate": 2.9387878844596308e-05,
      "loss": 0.4678,
      "step": 23300
    },
    {
      "epoch": 0.547575775341502,
      "grad_norm": 0.2905202805995941,
      "learning_rate": 2.9382079014941214e-05,
      "loss": 0.495,
      "step": 23350
    },
    {
      "epoch": 0.5487483144749956,
      "grad_norm": 0.30392327904701233,
      "learning_rate": 2.9376252415858673e-05,
      "loss": 0.4844,
      "step": 23400
    },
    {
      "epoch": 0.5499208536084892,
      "grad_norm": 0.2852601110935211,
      "learning_rate": 2.937039905819376e-05,
      "loss": 0.4588,
      "step": 23450
    },
    {
      "epoch": 0.5510933927419828,
      "grad_norm": 0.18487690389156342,
      "learning_rate": 2.9364518952841354e-05,
      "loss": 0.4951,
      "step": 23500
    },
    {
      "epoch": 0.5522659318754763,
      "grad_norm": 0.30517297983169556,
      "learning_rate": 2.9358612110746122e-05,
      "loss": 0.4955,
      "step": 23550
    },
    {
      "epoch": 0.5534384710089699,
      "grad_norm": 0.4784517288208008,
      "learning_rate": 2.9352678542902492e-05,
      "loss": 0.5337,
      "step": 23600
    },
    {
      "epoch": 0.5546110101424635,
      "grad_norm": 0.3292381465435028,
      "learning_rate": 2.9346718260354645e-05,
      "loss": 0.4796,
      "step": 23650
    },
    {
      "epoch": 0.5557835492759571,
      "grad_norm": 0.18010678887367249,
      "learning_rate": 2.934073127419648e-05,
      "loss": 0.4915,
      "step": 23700
    },
    {
      "epoch": 0.5569560884094507,
      "grad_norm": 0.2205924689769745,
      "learning_rate": 2.93347175955716e-05,
      "loss": 0.4927,
      "step": 23750
    },
    {
      "epoch": 0.5581286275429442,
      "grad_norm": 0.15574871003627777,
      "learning_rate": 2.9328677235673297e-05,
      "loss": 0.5095,
      "step": 23800
    },
    {
      "epoch": 0.5593011666764378,
      "grad_norm": 0.2064826488494873,
      "learning_rate": 2.932261020574451e-05,
      "loss": 0.4584,
      "step": 23850
    },
    {
      "epoch": 0.5604737058099314,
      "grad_norm": 0.4287954270839691,
      "learning_rate": 2.9316516517077832e-05,
      "loss": 0.4804,
      "step": 23900
    },
    {
      "epoch": 0.561646244943425,
      "grad_norm": 0.20645229518413544,
      "learning_rate": 2.9310396181015476e-05,
      "loss": 0.4276,
      "step": 23950
    },
    {
      "epoch": 0.5628187840769185,
      "grad_norm": 0.32802411913871765,
      "learning_rate": 2.930424920894925e-05,
      "loss": 0.46,
      "step": 24000
    },
    {
      "epoch": 0.5628187840769185,
      "eval_loss": 0.7869982123374939,
      "eval_runtime": 147.4917,
      "eval_samples_per_second": 67.8,
      "eval_steps_per_second": 16.95,
      "step": 24000
    },
    {
      "epoch": 0.5639913232104121,
      "grad_norm": 0.23834805190563202,
      "learning_rate": 2.9298075612320536e-05,
      "loss": 0.4595,
      "step": 24050
    },
    {
      "epoch": 0.5651638623439057,
      "grad_norm": 0.40606361627578735,
      "learning_rate": 2.9291875402620284e-05,
      "loss": 0.4648,
      "step": 24100
    },
    {
      "epoch": 0.5663364014773993,
      "grad_norm": 0.48986324667930603,
      "learning_rate": 2.9285648591388965e-05,
      "loss": 0.4882,
      "step": 24150
    },
    {
      "epoch": 0.5675089406108929,
      "grad_norm": 0.41784605383872986,
      "learning_rate": 2.9279395190216575e-05,
      "loss": 0.4619,
      "step": 24200
    },
    {
      "epoch": 0.5686814797443864,
      "grad_norm": 0.31305256485939026,
      "learning_rate": 2.9273115210742594e-05,
      "loss": 0.4758,
      "step": 24250
    },
    {
      "epoch": 0.56985401887788,
      "grad_norm": 0.36537307500839233,
      "learning_rate": 2.926680866465598e-05,
      "loss": 0.4556,
      "step": 24300
    },
    {
      "epoch": 0.5710265580113736,
      "grad_norm": 0.35839882493019104,
      "learning_rate": 2.9260475563695128e-05,
      "loss": 0.4435,
      "step": 24350
    },
    {
      "epoch": 0.5721990971448672,
      "grad_norm": 0.4512439966201782,
      "learning_rate": 2.925411591964788e-05,
      "loss": 0.4958,
      "step": 24400
    },
    {
      "epoch": 0.5733716362783607,
      "grad_norm": 0.38862869143486023,
      "learning_rate": 2.924772974435145e-05,
      "loss": 0.4532,
      "step": 24450
    },
    {
      "epoch": 0.5745441754118543,
      "grad_norm": 0.32166779041290283,
      "learning_rate": 2.9241317049692473e-05,
      "loss": 0.4715,
      "step": 24500
    },
    {
      "epoch": 0.5757167145453479,
      "grad_norm": 0.21513137221336365,
      "learning_rate": 2.9234877847606914e-05,
      "loss": 0.4688,
      "step": 24550
    },
    {
      "epoch": 0.5768892536788416,
      "grad_norm": 0.389305979013443,
      "learning_rate": 2.92284121500801e-05,
      "loss": 0.4809,
      "step": 24600
    },
    {
      "epoch": 0.5780617928123352,
      "grad_norm": 0.5238484144210815,
      "learning_rate": 2.9221919969146646e-05,
      "loss": 0.4243,
      "step": 24650
    },
    {
      "epoch": 0.5792343319458287,
      "grad_norm": 0.25039276480674744,
      "learning_rate": 2.921540131689049e-05,
      "loss": 0.4923,
      "step": 24700
    },
    {
      "epoch": 0.5804068710793223,
      "grad_norm": 0.3715086877346039,
      "learning_rate": 2.9208856205444826e-05,
      "loss": 0.4664,
      "step": 24750
    },
    {
      "epoch": 0.5815794102128159,
      "grad_norm": 0.30982518196105957,
      "learning_rate": 2.9202284646992096e-05,
      "loss": 0.5236,
      "step": 24800
    },
    {
      "epoch": 0.5827519493463095,
      "grad_norm": 0.4458017945289612,
      "learning_rate": 2.919568665376397e-05,
      "loss": 0.4565,
      "step": 24850
    },
    {
      "epoch": 0.583924488479803,
      "grad_norm": 0.17168541252613068,
      "learning_rate": 2.918906223804133e-05,
      "loss": 0.4739,
      "step": 24900
    },
    {
      "epoch": 0.5850970276132966,
      "grad_norm": 0.2695430815219879,
      "learning_rate": 2.918241141215422e-05,
      "loss": 0.4649,
      "step": 24950
    },
    {
      "epoch": 0.5862695667467902,
      "grad_norm": 0.19576826691627502,
      "learning_rate": 2.9175734188481862e-05,
      "loss": 0.4955,
      "step": 25000
    },
    {
      "epoch": 0.5874421058802838,
      "grad_norm": 0.3804488778114319,
      "learning_rate": 2.9169030579452597e-05,
      "loss": 0.4665,
      "step": 25050
    },
    {
      "epoch": 0.5886146450137774,
      "grad_norm": 0.6718562245368958,
      "learning_rate": 2.916230059754388e-05,
      "loss": 0.4561,
      "step": 25100
    },
    {
      "epoch": 0.5897871841472709,
      "grad_norm": 0.20674078166484833,
      "learning_rate": 2.9155544255282255e-05,
      "loss": 0.4825,
      "step": 25150
    },
    {
      "epoch": 0.5909597232807645,
      "grad_norm": 0.33065974712371826,
      "learning_rate": 2.914876156524334e-05,
      "loss": 0.4915,
      "step": 25200
    },
    {
      "epoch": 0.5921322624142581,
      "grad_norm": 0.45569396018981934,
      "learning_rate": 2.9141952540051777e-05,
      "loss": 0.4606,
      "step": 25250
    },
    {
      "epoch": 0.5933048015477517,
      "grad_norm": 0.4835242033004761,
      "learning_rate": 2.913511719238124e-05,
      "loss": 0.4924,
      "step": 25300
    },
    {
      "epoch": 0.5944773406812452,
      "grad_norm": 0.2561967968940735,
      "learning_rate": 2.9128255534954392e-05,
      "loss": 0.4737,
      "step": 25350
    },
    {
      "epoch": 0.5956498798147388,
      "grad_norm": 0.4169861376285553,
      "learning_rate": 2.912136758054287e-05,
      "loss": 0.4609,
      "step": 25400
    },
    {
      "epoch": 0.5968224189482324,
      "grad_norm": 0.2589930593967438,
      "learning_rate": 2.9114453341967237e-05,
      "loss": 0.4147,
      "step": 25450
    },
    {
      "epoch": 0.597994958081726,
      "grad_norm": 0.22249221801757812,
      "learning_rate": 2.9107512832097017e-05,
      "loss": 0.4747,
      "step": 25500
    },
    {
      "epoch": 0.5991674972152196,
      "grad_norm": 0.4232428967952728,
      "learning_rate": 2.9100546063850603e-05,
      "loss": 0.488,
      "step": 25550
    },
    {
      "epoch": 0.6003400363487131,
      "grad_norm": 0.5200372338294983,
      "learning_rate": 2.9093553050195268e-05,
      "loss": 0.4523,
      "step": 25600
    },
    {
      "epoch": 0.6015125754822067,
      "grad_norm": 0.2879292070865631,
      "learning_rate": 2.9086533804147148e-05,
      "loss": 0.4895,
      "step": 25650
    },
    {
      "epoch": 0.6026851146157003,
      "grad_norm": 0.21929213404655457,
      "learning_rate": 2.9079488338771182e-05,
      "loss": 0.456,
      "step": 25700
    },
    {
      "epoch": 0.6038576537491939,
      "grad_norm": 0.3176044225692749,
      "learning_rate": 2.9072416667181144e-05,
      "loss": 0.4523,
      "step": 25750
    },
    {
      "epoch": 0.6050301928826874,
      "grad_norm": 0.29462718963623047,
      "learning_rate": 2.9065318802539557e-05,
      "loss": 0.4968,
      "step": 25800
    },
    {
      "epoch": 0.606202732016181,
      "grad_norm": 0.4527049660682678,
      "learning_rate": 2.9058194758057712e-05,
      "loss": 0.4724,
      "step": 25850
    },
    {
      "epoch": 0.6073752711496746,
      "grad_norm": 0.2155551314353943,
      "learning_rate": 2.905104454699562e-05,
      "loss": 0.5197,
      "step": 25900
    },
    {
      "epoch": 0.6085478102831682,
      "grad_norm": 0.28500357270240784,
      "learning_rate": 2.9043868182662005e-05,
      "loss": 0.4563,
      "step": 25950
    },
    {
      "epoch": 0.6097203494166618,
      "grad_norm": 0.3048538863658905,
      "learning_rate": 2.9036665678414265e-05,
      "loss": 0.4826,
      "step": 26000
    },
    {
      "epoch": 0.6097203494166618,
      "eval_loss": 0.7862207889556885,
      "eval_runtime": 186.2249,
      "eval_samples_per_second": 53.698,
      "eval_steps_per_second": 13.425,
      "step": 26000
    },
    {
      "epoch": 0.6108928885501553,
      "grad_norm": 0.1817711442708969,
      "learning_rate": 2.9029437047658453e-05,
      "loss": 0.4107,
      "step": 26050
    },
    {
      "epoch": 0.6120654276836489,
      "grad_norm": 0.6372254490852356,
      "learning_rate": 2.9022182303849247e-05,
      "loss": 0.5006,
      "step": 26100
    },
    {
      "epoch": 0.6132379668171425,
      "grad_norm": 0.38642698526382446,
      "learning_rate": 2.9014901460489938e-05,
      "loss": 0.4561,
      "step": 26150
    },
    {
      "epoch": 0.6144105059506361,
      "grad_norm": 0.2841675877571106,
      "learning_rate": 2.9007594531132393e-05,
      "loss": 0.4413,
      "step": 26200
    },
    {
      "epoch": 0.6155830450841296,
      "grad_norm": 0.40371859073638916,
      "learning_rate": 2.9000261529377033e-05,
      "loss": 0.4766,
      "step": 26250
    },
    {
      "epoch": 0.6167555842176232,
      "grad_norm": 0.1948595643043518,
      "learning_rate": 2.8992902468872798e-05,
      "loss": 0.427,
      "step": 26300
    },
    {
      "epoch": 0.6179281233511168,
      "grad_norm": 0.2221611738204956,
      "learning_rate": 2.8985517363317153e-05,
      "loss": 0.4707,
      "step": 26350
    },
    {
      "epoch": 0.6191006624846104,
      "grad_norm": 0.25959983468055725,
      "learning_rate": 2.897810622645602e-05,
      "loss": 0.4968,
      "step": 26400
    },
    {
      "epoch": 0.620273201618104,
      "grad_norm": 0.17057865858078003,
      "learning_rate": 2.8970669072083777e-05,
      "loss": 0.4596,
      "step": 26450
    },
    {
      "epoch": 0.6214457407515975,
      "grad_norm": 0.25990211963653564,
      "learning_rate": 2.896320591404324e-05,
      "loss": 0.4775,
      "step": 26500
    },
    {
      "epoch": 0.6226182798850912,
      "grad_norm": 0.3938291668891907,
      "learning_rate": 2.8955716766225616e-05,
      "loss": 0.486,
      "step": 26550
    },
    {
      "epoch": 0.6237908190185848,
      "grad_norm": 0.2800213396549225,
      "learning_rate": 2.8948201642570488e-05,
      "loss": 0.5069,
      "step": 26600
    },
    {
      "epoch": 0.6249633581520784,
      "grad_norm": 0.29254716634750366,
      "learning_rate": 2.894066055706579e-05,
      "loss": 0.4783,
      "step": 26650
    },
    {
      "epoch": 0.626135897285572,
      "grad_norm": 0.4651052951812744,
      "learning_rate": 2.8933093523747776e-05,
      "loss": 0.5003,
      "step": 26700
    },
    {
      "epoch": 0.6273084364190655,
      "grad_norm": 0.332061767578125,
      "learning_rate": 2.8925500556701e-05,
      "loss": 0.463,
      "step": 26750
    },
    {
      "epoch": 0.6284809755525591,
      "grad_norm": 0.3343360126018524,
      "learning_rate": 2.891788167005829e-05,
      "loss": 0.4765,
      "step": 26800
    },
    {
      "epoch": 0.6296535146860527,
      "grad_norm": 0.311943382024765,
      "learning_rate": 2.89102368780007e-05,
      "loss": 0.4807,
      "step": 26850
    },
    {
      "epoch": 0.6308260538195463,
      "grad_norm": 0.28599268198013306,
      "learning_rate": 2.8902566194757534e-05,
      "loss": 0.4787,
      "step": 26900
    },
    {
      "epoch": 0.6319985929530398,
      "grad_norm": 0.2699959874153137,
      "learning_rate": 2.889486963460625e-05,
      "loss": 0.5116,
      "step": 26950
    },
    {
      "epoch": 0.6331711320865334,
      "grad_norm": 0.34120652079582214,
      "learning_rate": 2.8887147211872507e-05,
      "loss": 0.4846,
      "step": 27000
    },
    {
      "epoch": 0.634343671220027,
      "grad_norm": 0.2485114336013794,
      "learning_rate": 2.8879398940930068e-05,
      "loss": 0.4573,
      "step": 27050
    },
    {
      "epoch": 0.6355162103535206,
      "grad_norm": 0.21572858095169067,
      "learning_rate": 2.8871624836200838e-05,
      "loss": 0.4542,
      "step": 27100
    },
    {
      "epoch": 0.6366887494870141,
      "grad_norm": 0.7590936422348022,
      "learning_rate": 2.8863824912154784e-05,
      "loss": 0.4504,
      "step": 27150
    },
    {
      "epoch": 0.6378612886205077,
      "grad_norm": 0.36944806575775146,
      "learning_rate": 2.8855999183309944e-05,
      "loss": 0.4674,
      "step": 27200
    },
    {
      "epoch": 0.6390338277540013,
      "grad_norm": 0.2863059341907501,
      "learning_rate": 2.8848147664232376e-05,
      "loss": 0.4416,
      "step": 27250
    },
    {
      "epoch": 0.6402063668874949,
      "grad_norm": 0.20842373371124268,
      "learning_rate": 2.8840270369536155e-05,
      "loss": 0.4984,
      "step": 27300
    },
    {
      "epoch": 0.6413789060209885,
      "grad_norm": 0.1919144243001938,
      "learning_rate": 2.883236731388332e-05,
      "loss": 0.4731,
      "step": 27350
    },
    {
      "epoch": 0.642551445154482,
      "grad_norm": 0.32727497816085815,
      "learning_rate": 2.8824438511983863e-05,
      "loss": 0.4836,
      "step": 27400
    },
    {
      "epoch": 0.6437239842879756,
      "grad_norm": 0.5201116800308228,
      "learning_rate": 2.8816483978595698e-05,
      "loss": 0.4911,
      "step": 27450
    },
    {
      "epoch": 0.6448965234214692,
      "grad_norm": 0.2180083990097046,
      "learning_rate": 2.8808503728524637e-05,
      "loss": 0.4369,
      "step": 27500
    },
    {
      "epoch": 0.6460690625549628,
      "grad_norm": 0.3275837302207947,
      "learning_rate": 2.8800497776624348e-05,
      "loss": 0.4806,
      "step": 27550
    },
    {
      "epoch": 0.6472416016884563,
      "grad_norm": 0.2884136140346527,
      "learning_rate": 2.8792466137796356e-05,
      "loss": 0.4647,
      "step": 27600
    },
    {
      "epoch": 0.6484141408219499,
      "grad_norm": 0.36057189106941223,
      "learning_rate": 2.8784408826989973e-05,
      "loss": 0.4722,
      "step": 27650
    },
    {
      "epoch": 0.6495866799554435,
      "grad_norm": 0.33754968643188477,
      "learning_rate": 2.8776325859202316e-05,
      "loss": 0.4306,
      "step": 27700
    },
    {
      "epoch": 0.6507592190889371,
      "grad_norm": 0.1940678209066391,
      "learning_rate": 2.8768217249478247e-05,
      "loss": 0.4475,
      "step": 27750
    },
    {
      "epoch": 0.6519317582224307,
      "grad_norm": 0.2923644483089447,
      "learning_rate": 2.8760083012910357e-05,
      "loss": 0.4387,
      "step": 27800
    },
    {
      "epoch": 0.6531042973559242,
      "grad_norm": 0.4196312129497528,
      "learning_rate": 2.875192316463893e-05,
      "loss": 0.5007,
      "step": 27850
    },
    {
      "epoch": 0.6542768364894178,
      "grad_norm": 0.3977247178554535,
      "learning_rate": 2.874373771985194e-05,
      "loss": 0.4775,
      "step": 27900
    },
    {
      "epoch": 0.6554493756229114,
      "grad_norm": 0.4047550857067108,
      "learning_rate": 2.8735526693784986e-05,
      "loss": 0.4764,
      "step": 27950
    },
    {
      "epoch": 0.656621914756405,
      "grad_norm": 0.26516464352607727,
      "learning_rate": 2.8727290101721286e-05,
      "loss": 0.4939,
      "step": 28000
    },
    {
      "epoch": 0.656621914756405,
      "eval_loss": 0.782711386680603,
      "eval_runtime": 156.3133,
      "eval_samples_per_second": 63.974,
      "eval_steps_per_second": 15.994,
      "step": 28000
    },
    {
      "epoch": 0.6577944538898985,
      "grad_norm": 0.36287328600883484,
      "learning_rate": 2.8719027958991642e-05,
      "loss": 0.4616,
      "step": 28050
    },
    {
      "epoch": 0.6589669930233921,
      "grad_norm": 0.2718903720378876,
      "learning_rate": 2.8710740280974427e-05,
      "loss": 0.4813,
      "step": 28100
    },
    {
      "epoch": 0.6601395321568857,
      "grad_norm": 0.2881302833557129,
      "learning_rate": 2.8702427083095527e-05,
      "loss": 0.4895,
      "step": 28150
    },
    {
      "epoch": 0.6613120712903793,
      "grad_norm": 0.282747745513916,
      "learning_rate": 2.8694088380828328e-05,
      "loss": 0.4603,
      "step": 28200
    },
    {
      "epoch": 0.6624846104238729,
      "grad_norm": 0.3776010572910309,
      "learning_rate": 2.86857241896937e-05,
      "loss": 0.4926,
      "step": 28250
    },
    {
      "epoch": 0.6636571495573664,
      "grad_norm": 0.15563583374023438,
      "learning_rate": 2.8677334525259954e-05,
      "loss": 0.4604,
      "step": 28300
    },
    {
      "epoch": 0.66482968869086,
      "grad_norm": 0.15658949315547943,
      "learning_rate": 2.8668919403142798e-05,
      "loss": 0.4677,
      "step": 28350
    },
    {
      "epoch": 0.6660022278243536,
      "grad_norm": 0.4337351322174072,
      "learning_rate": 2.866047883900535e-05,
      "loss": 0.487,
      "step": 28400
    },
    {
      "epoch": 0.6671747669578472,
      "grad_norm": 0.2545074224472046,
      "learning_rate": 2.8652012848558053e-05,
      "loss": 0.4282,
      "step": 28450
    },
    {
      "epoch": 0.6683473060913409,
      "grad_norm": 0.39793160557746887,
      "learning_rate": 2.8643521447558703e-05,
      "loss": 0.4476,
      "step": 28500
    },
    {
      "epoch": 0.6695198452248344,
      "grad_norm": 0.4037146270275116,
      "learning_rate": 2.8635004651812382e-05,
      "loss": 0.4301,
      "step": 28550
    },
    {
      "epoch": 0.670692384358328,
      "grad_norm": 0.2939247488975525,
      "learning_rate": 2.862646247717144e-05,
      "loss": 0.4531,
      "step": 28600
    },
    {
      "epoch": 0.6718649234918216,
      "grad_norm": 0.2575156092643738,
      "learning_rate": 2.8617894939535463e-05,
      "loss": 0.4703,
      "step": 28650
    },
    {
      "epoch": 0.6730374626253152,
      "grad_norm": 0.4745045006275177,
      "learning_rate": 2.8609302054851244e-05,
      "loss": 0.4609,
      "step": 28700
    },
    {
      "epoch": 0.6742100017588087,
      "grad_norm": 0.5207324028015137,
      "learning_rate": 2.860068383911276e-05,
      "loss": 0.4629,
      "step": 28750
    },
    {
      "epoch": 0.6753825408923023,
      "grad_norm": 0.3928890824317932,
      "learning_rate": 2.8592040308361138e-05,
      "loss": 0.4994,
      "step": 28800
    },
    {
      "epoch": 0.6765550800257959,
      "grad_norm": 0.43536877632141113,
      "learning_rate": 2.858337147868461e-05,
      "loss": 0.5021,
      "step": 28850
    },
    {
      "epoch": 0.6777276191592895,
      "grad_norm": 0.16775290668010712,
      "learning_rate": 2.857467736621852e-05,
      "loss": 0.4847,
      "step": 28900
    },
    {
      "epoch": 0.678900158292783,
      "grad_norm": 0.35863542556762695,
      "learning_rate": 2.8565957987145247e-05,
      "loss": 0.4708,
      "step": 28950
    },
    {
      "epoch": 0.6800726974262766,
      "grad_norm": 0.273561030626297,
      "learning_rate": 2.8557213357694215e-05,
      "loss": 0.5081,
      "step": 29000
    },
    {
      "epoch": 0.6812452365597702,
      "grad_norm": 0.4566415548324585,
      "learning_rate": 2.854844349414184e-05,
      "loss": 0.4665,
      "step": 29050
    },
    {
      "epoch": 0.6824177756932638,
      "grad_norm": 0.20522651076316833,
      "learning_rate": 2.8539648412811514e-05,
      "loss": 0.4573,
      "step": 29100
    },
    {
      "epoch": 0.6835903148267574,
      "grad_norm": 0.34368470311164856,
      "learning_rate": 2.8530828130073552e-05,
      "loss": 0.4726,
      "step": 29150
    },
    {
      "epoch": 0.6847628539602509,
      "grad_norm": 0.2910609841346741,
      "learning_rate": 2.8521982662345185e-05,
      "loss": 0.455,
      "step": 29200
    },
    {
      "epoch": 0.6859353930937445,
      "grad_norm": 0.3551269471645355,
      "learning_rate": 2.8513112026090532e-05,
      "loss": 0.4525,
      "step": 29250
    },
    {
      "epoch": 0.6871079322272381,
      "grad_norm": 0.2761073708534241,
      "learning_rate": 2.8504216237820537e-05,
      "loss": 0.4668,
      "step": 29300
    },
    {
      "epoch": 0.6882804713607317,
      "grad_norm": 0.31229162216186523,
      "learning_rate": 2.849529531409298e-05,
      "loss": 0.4312,
      "step": 29350
    },
    {
      "epoch": 0.6894530104942252,
      "grad_norm": 0.42260417342185974,
      "learning_rate": 2.8486349271512402e-05,
      "loss": 0.4575,
      "step": 29400
    },
    {
      "epoch": 0.6906255496277188,
      "grad_norm": 0.2895490825176239,
      "learning_rate": 2.8477378126730125e-05,
      "loss": 0.4264,
      "step": 29450
    },
    {
      "epoch": 0.6917980887612124,
      "grad_norm": 0.41898706555366516,
      "learning_rate": 2.8468381896444175e-05,
      "loss": 0.5174,
      "step": 29500
    },
    {
      "epoch": 0.692970627894706,
      "grad_norm": 0.23906926810741425,
      "learning_rate": 2.8459360597399274e-05,
      "loss": 0.5107,
      "step": 29550
    },
    {
      "epoch": 0.6941431670281996,
      "grad_norm": 0.37401294708251953,
      "learning_rate": 2.845031424638681e-05,
      "loss": 0.481,
      "step": 29600
    },
    {
      "epoch": 0.6953157061616931,
      "grad_norm": 0.17765270173549652,
      "learning_rate": 2.844124286024479e-05,
      "loss": 0.4631,
      "step": 29650
    },
    {
      "epoch": 0.6964882452951867,
      "grad_norm": 0.2614386975765228,
      "learning_rate": 2.8432146455857836e-05,
      "loss": 0.4407,
      "step": 29700
    },
    {
      "epoch": 0.6976607844286803,
      "grad_norm": 0.20998618006706238,
      "learning_rate": 2.842302505015711e-05,
      "loss": 0.479,
      "step": 29750
    },
    {
      "epoch": 0.6988333235621739,
      "grad_norm": 0.3832109272480011,
      "learning_rate": 2.8413878660120337e-05,
      "loss": 0.4394,
      "step": 29800
    },
    {
      "epoch": 0.7000058626956674,
      "grad_norm": 0.39123818278312683,
      "learning_rate": 2.8404707302771736e-05,
      "loss": 0.4358,
      "step": 29850
    },
    {
      "epoch": 0.701178401829161,
      "grad_norm": 0.23673918843269348,
      "learning_rate": 2.839551099518198e-05,
      "loss": 0.4462,
      "step": 29900
    },
    {
      "epoch": 0.7023509409626546,
      "grad_norm": 0.26897215843200684,
      "learning_rate": 2.8386289754468214e-05,
      "loss": 0.5159,
      "step": 29950
    },
    {
      "epoch": 0.7035234800961482,
      "grad_norm": 0.2985231578350067,
      "learning_rate": 2.837704359779396e-05,
      "loss": 0.4918,
      "step": 30000
    },
    {
      "epoch": 0.7035234800961482,
      "eval_loss": 0.7815025448799133,
      "eval_runtime": 178.989,
      "eval_samples_per_second": 55.869,
      "eval_steps_per_second": 13.967,
      "step": 30000
    },
    {
      "epoch": 0.7046960192296418,
      "grad_norm": 0.2225857973098755,
      "learning_rate": 2.8367772542369145e-05,
      "loss": 0.4583,
      "step": 30050
    },
    {
      "epoch": 0.7058685583631353,
      "grad_norm": 0.30117127299308777,
      "learning_rate": 2.8358476605450014e-05,
      "loss": 0.4608,
      "step": 30100
    },
    {
      "epoch": 0.7070410974966289,
      "grad_norm": 0.3169870972633362,
      "learning_rate": 2.8349155804339136e-05,
      "loss": 0.4871,
      "step": 30150
    },
    {
      "epoch": 0.7082136366301225,
      "grad_norm": 0.2137010544538498,
      "learning_rate": 2.833981015638537e-05,
      "loss": 0.4494,
      "step": 30200
    },
    {
      "epoch": 0.7093861757636161,
      "grad_norm": 0.3259196877479553,
      "learning_rate": 2.8330439678983795e-05,
      "loss": 0.501,
      "step": 30250
    },
    {
      "epoch": 0.7105587148971096,
      "grad_norm": 0.3413242697715759,
      "learning_rate": 2.8321044389575737e-05,
      "loss": 0.4656,
      "step": 30300
    },
    {
      "epoch": 0.7117312540306032,
      "grad_norm": 0.17413967847824097,
      "learning_rate": 2.8311624305648685e-05,
      "loss": 0.4905,
      "step": 30350
    },
    {
      "epoch": 0.7129037931640968,
      "grad_norm": 0.2023351490497589,
      "learning_rate": 2.8302179444736283e-05,
      "loss": 0.4846,
      "step": 30400
    },
    {
      "epoch": 0.7140763322975904,
      "grad_norm": 0.2918337285518646,
      "learning_rate": 2.8292709824418288e-05,
      "loss": 0.5103,
      "step": 30450
    },
    {
      "epoch": 0.7152488714310841,
      "grad_norm": 0.5015662908554077,
      "learning_rate": 2.8283215462320555e-05,
      "loss": 0.4671,
      "step": 30500
    },
    {
      "epoch": 0.7164214105645776,
      "grad_norm": 0.3969416916370392,
      "learning_rate": 2.8273696376114978e-05,
      "loss": 0.5064,
      "step": 30550
    },
    {
      "epoch": 0.7175939496980712,
      "grad_norm": 0.3811984658241272,
      "learning_rate": 2.8264152583519468e-05,
      "loss": 0.448,
      "step": 30600
    },
    {
      "epoch": 0.7187664888315648,
      "grad_norm": 0.2599171996116638,
      "learning_rate": 2.825458410229794e-05,
      "loss": 0.4557,
      "step": 30650
    },
    {
      "epoch": 0.7199390279650584,
      "grad_norm": 0.4213537275791168,
      "learning_rate": 2.8244990950260243e-05,
      "loss": 0.4847,
      "step": 30700
    },
    {
      "epoch": 0.721111567098552,
      "grad_norm": 0.35120251774787903,
      "learning_rate": 2.8235373145262156e-05,
      "loss": 0.4244,
      "step": 30750
    },
    {
      "epoch": 0.7222841062320455,
      "grad_norm": 0.3085532486438751,
      "learning_rate": 2.822573070520534e-05,
      "loss": 0.4864,
      "step": 30800
    },
    {
      "epoch": 0.7234566453655391,
      "grad_norm": 0.42872360348701477,
      "learning_rate": 2.8216063648037316e-05,
      "loss": 0.4875,
      "step": 30850
    },
    {
      "epoch": 0.7246291844990327,
      "grad_norm": 0.22139927744865417,
      "learning_rate": 2.8206371991751415e-05,
      "loss": 0.45,
      "step": 30900
    },
    {
      "epoch": 0.7258017236325263,
      "grad_norm": 0.3680330812931061,
      "learning_rate": 2.8196655754386765e-05,
      "loss": 0.4618,
      "step": 30950
    },
    {
      "epoch": 0.7269742627660198,
      "grad_norm": 0.5249466896057129,
      "learning_rate": 2.8186914954028236e-05,
      "loss": 0.4765,
      "step": 31000
    },
    {
      "epoch": 0.7281468018995134,
      "grad_norm": 0.37498241662979126,
      "learning_rate": 2.8177149608806424e-05,
      "loss": 0.4731,
      "step": 31050
    },
    {
      "epoch": 0.729319341033007,
      "grad_norm": 0.4397069811820984,
      "learning_rate": 2.8167359736897612e-05,
      "loss": 0.4888,
      "step": 31100
    },
    {
      "epoch": 0.7304918801665006,
      "grad_norm": 0.3453885018825531,
      "learning_rate": 2.8157545356523736e-05,
      "loss": 0.4526,
      "step": 31150
    },
    {
      "epoch": 0.7316644192999942,
      "grad_norm": 0.3015328049659729,
      "learning_rate": 2.8147706485952337e-05,
      "loss": 0.4787,
      "step": 31200
    },
    {
      "epoch": 0.7328369584334877,
      "grad_norm": 0.37902355194091797,
      "learning_rate": 2.8137843143496554e-05,
      "loss": 0.488,
      "step": 31250
    },
    {
      "epoch": 0.7340094975669813,
      "grad_norm": 0.2836439609527588,
      "learning_rate": 2.8127955347515066e-05,
      "loss": 0.5004,
      "step": 31300
    },
    {
      "epoch": 0.7351820367004749,
      "grad_norm": 0.21737465262413025,
      "learning_rate": 2.8118043116412077e-05,
      "loss": 0.4669,
      "step": 31350
    },
    {
      "epoch": 0.7363545758339685,
      "grad_norm": 0.31973743438720703,
      "learning_rate": 2.8108106468637264e-05,
      "loss": 0.4741,
      "step": 31400
    },
    {
      "epoch": 0.737527114967462,
      "grad_norm": 0.30443447828292847,
      "learning_rate": 2.8098145422685745e-05,
      "loss": 0.4392,
      "step": 31450
    },
    {
      "epoch": 0.7386996541009556,
      "grad_norm": 0.298431396484375,
      "learning_rate": 2.8088159997098076e-05,
      "loss": 0.4782,
      "step": 31500
    },
    {
      "epoch": 0.7398721932344492,
      "grad_norm": 0.2120034247636795,
      "learning_rate": 2.8078150210460164e-05,
      "loss": 0.3955,
      "step": 31550
    },
    {
      "epoch": 0.7410447323679428,
      "grad_norm": 0.3386734426021576,
      "learning_rate": 2.8068116081403268e-05,
      "loss": 0.4952,
      "step": 31600
    },
    {
      "epoch": 0.7422172715014363,
      "grad_norm": 0.273416668176651,
      "learning_rate": 2.8058057628603957e-05,
      "loss": 0.4872,
      "step": 31650
    },
    {
      "epoch": 0.7433898106349299,
      "grad_norm": 0.30188819766044617,
      "learning_rate": 2.8047974870784082e-05,
      "loss": 0.5073,
      "step": 31700
    },
    {
      "epoch": 0.7445623497684235,
      "grad_norm": 0.33658844232559204,
      "learning_rate": 2.8037867826710714e-05,
      "loss": 0.495,
      "step": 31750
    },
    {
      "epoch": 0.7457348889019171,
      "grad_norm": 0.29392826557159424,
      "learning_rate": 2.8027736515196143e-05,
      "loss": 0.4743,
      "step": 31800
    },
    {
      "epoch": 0.7469074280354107,
      "grad_norm": 0.2212991565465927,
      "learning_rate": 2.8017580955097827e-05,
      "loss": 0.4722,
      "step": 31850
    },
    {
      "epoch": 0.7480799671689042,
      "grad_norm": 0.25481274724006653,
      "learning_rate": 2.8007401165318356e-05,
      "loss": 0.4648,
      "step": 31900
    },
    {
      "epoch": 0.7492525063023978,
      "grad_norm": 0.28334298729896545,
      "learning_rate": 2.799719716480541e-05,
      "loss": 0.4693,
      "step": 31950
    },
    {
      "epoch": 0.7504250454358914,
      "grad_norm": 0.49566414952278137,
      "learning_rate": 2.7986968972551745e-05,
      "loss": 0.4683,
      "step": 32000
    },
    {
      "epoch": 0.7504250454358914,
      "eval_loss": 0.7802421450614929,
      "eval_runtime": 165.4432,
      "eval_samples_per_second": 60.444,
      "eval_steps_per_second": 15.111,
      "step": 32000
    },
    {
      "epoch": 0.751597584569385,
      "grad_norm": 0.3417053818702698,
      "learning_rate": 2.7976716607595147e-05,
      "loss": 0.4699,
      "step": 32050
    },
    {
      "epoch": 0.7527701237028785,
      "grad_norm": 0.28650161623954773,
      "learning_rate": 2.7966440089018375e-05,
      "loss": 0.4295,
      "step": 32100
    },
    {
      "epoch": 0.7539426628363721,
      "grad_norm": 0.31927287578582764,
      "learning_rate": 2.795613943594917e-05,
      "loss": 0.4687,
      "step": 32150
    },
    {
      "epoch": 0.7551152019698657,
      "grad_norm": 0.41476795077323914,
      "learning_rate": 2.794581466756018e-05,
      "loss": 0.4909,
      "step": 32200
    },
    {
      "epoch": 0.7562877411033593,
      "grad_norm": 0.28356051445007324,
      "learning_rate": 2.7935465803068933e-05,
      "loss": 0.471,
      "step": 32250
    },
    {
      "epoch": 0.7574602802368529,
      "grad_norm": 0.5192307233810425,
      "learning_rate": 2.792509286173783e-05,
      "loss": 0.4914,
      "step": 32300
    },
    {
      "epoch": 0.7586328193703464,
      "grad_norm": 0.2682454288005829,
      "learning_rate": 2.791469586287407e-05,
      "loss": 0.4726,
      "step": 32350
    },
    {
      "epoch": 0.75980535850384,
      "grad_norm": 0.31811150908470154,
      "learning_rate": 2.7904274825829624e-05,
      "loss": 0.4697,
      "step": 32400
    },
    {
      "epoch": 0.7609778976373337,
      "grad_norm": 0.4352128803730011,
      "learning_rate": 2.789382977000123e-05,
      "loss": 0.4876,
      "step": 32450
    },
    {
      "epoch": 0.7621504367708273,
      "grad_norm": 0.4567372798919678,
      "learning_rate": 2.78833607148303e-05,
      "loss": 0.4775,
      "step": 32500
    },
    {
      "epoch": 0.7633229759043209,
      "grad_norm": 0.3179272413253784,
      "learning_rate": 2.7872867679802943e-05,
      "loss": 0.4948,
      "step": 32550
    },
    {
      "epoch": 0.7644955150378144,
      "grad_norm": 0.35605770349502563,
      "learning_rate": 2.786235068444989e-05,
      "loss": 0.4687,
      "step": 32600
    },
    {
      "epoch": 0.765668054171308,
      "grad_norm": 0.37588807940483093,
      "learning_rate": 2.7851809748346465e-05,
      "loss": 0.4775,
      "step": 32650
    },
    {
      "epoch": 0.7668405933048016,
      "grad_norm": 0.22312721610069275,
      "learning_rate": 2.7841244891112566e-05,
      "loss": 0.4671,
      "step": 32700
    },
    {
      "epoch": 0.7680131324382952,
      "grad_norm": 0.32240206003189087,
      "learning_rate": 2.783065613241261e-05,
      "loss": 0.4397,
      "step": 32750
    },
    {
      "epoch": 0.7691856715717887,
      "grad_norm": 0.437915176153183,
      "learning_rate": 2.7820043491955498e-05,
      "loss": 0.4779,
      "step": 32800
    },
    {
      "epoch": 0.7703582107052823,
      "grad_norm": 0.2781428098678589,
      "learning_rate": 2.780940698949458e-05,
      "loss": 0.4661,
      "step": 32850
    },
    {
      "epoch": 0.7715307498387759,
      "grad_norm": 0.17069581151008606,
      "learning_rate": 2.7798746644827627e-05,
      "loss": 0.4612,
      "step": 32900
    },
    {
      "epoch": 0.7727032889722695,
      "grad_norm": 0.3884926736354828,
      "learning_rate": 2.7788062477796793e-05,
      "loss": 0.4601,
      "step": 32950
    },
    {
      "epoch": 0.773875828105763,
      "grad_norm": 0.3764033019542694,
      "learning_rate": 2.777735450828856e-05,
      "loss": 0.4456,
      "step": 33000
    },
    {
      "epoch": 0.7750483672392566,
      "grad_norm": 0.3993033170700073,
      "learning_rate": 2.776662275623372e-05,
      "loss": 0.4613,
      "step": 33050
    },
    {
      "epoch": 0.7762209063727502,
      "grad_norm": 0.3238365948200226,
      "learning_rate": 2.7755867241607328e-05,
      "loss": 0.4844,
      "step": 33100
    },
    {
      "epoch": 0.7773934455062438,
      "grad_norm": 0.30242887139320374,
      "learning_rate": 2.7745087984428675e-05,
      "loss": 0.4164,
      "step": 33150
    },
    {
      "epoch": 0.7785659846397374,
      "grad_norm": 0.34836849570274353,
      "learning_rate": 2.773428500476124e-05,
      "loss": 0.4794,
      "step": 33200
    },
    {
      "epoch": 0.7797385237732309,
      "grad_norm": 0.33340445160865784,
      "learning_rate": 2.7723458322712653e-05,
      "loss": 0.4845,
      "step": 33250
    },
    {
      "epoch": 0.7809110629067245,
      "grad_norm": 0.322408527135849,
      "learning_rate": 2.771260795843467e-05,
      "loss": 0.4841,
      "step": 33300
    },
    {
      "epoch": 0.7820836020402181,
      "grad_norm": 0.19693705439567566,
      "learning_rate": 2.7701733932123115e-05,
      "loss": 0.4473,
      "step": 33350
    },
    {
      "epoch": 0.7832561411737117,
      "grad_norm": 0.2739073932170868,
      "learning_rate": 2.769083626401787e-05,
      "loss": 0.498,
      "step": 33400
    },
    {
      "epoch": 0.7844286803072053,
      "grad_norm": 0.21063926815986633,
      "learning_rate": 2.767991497440281e-05,
      "loss": 0.4548,
      "step": 33450
    },
    {
      "epoch": 0.7856012194406988,
      "grad_norm": 0.33078551292419434,
      "learning_rate": 2.766897008360578e-05,
      "loss": 0.4997,
      "step": 33500
    },
    {
      "epoch": 0.7867737585741924,
      "grad_norm": 0.2149365097284317,
      "learning_rate": 2.765800161199855e-05,
      "loss": 0.4612,
      "step": 33550
    },
    {
      "epoch": 0.787946297707686,
      "grad_norm": 0.3405512869358063,
      "learning_rate": 2.764700957999679e-05,
      "loss": 0.4769,
      "step": 33600
    },
    {
      "epoch": 0.7891188368411796,
      "grad_norm": 0.5393186211585999,
      "learning_rate": 2.7635994008060014e-05,
      "loss": 0.4803,
      "step": 33650
    },
    {
      "epoch": 0.7902913759746731,
      "grad_norm": 0.2458716630935669,
      "learning_rate": 2.7624954916691556e-05,
      "loss": 0.4483,
      "step": 33700
    },
    {
      "epoch": 0.7914639151081667,
      "grad_norm": 0.26474374532699585,
      "learning_rate": 2.761389232643853e-05,
      "loss": 0.4564,
      "step": 33750
    },
    {
      "epoch": 0.7926364542416603,
      "grad_norm": 0.28715166449546814,
      "learning_rate": 2.7602806257891777e-05,
      "loss": 0.5054,
      "step": 33800
    },
    {
      "epoch": 0.7938089933751539,
      "grad_norm": 0.3434623181819916,
      "learning_rate": 2.759169673168585e-05,
      "loss": 0.4785,
      "step": 33850
    },
    {
      "epoch": 0.7949815325086474,
      "grad_norm": 0.4767262935638428,
      "learning_rate": 2.758056376849896e-05,
      "loss": 0.4543,
      "step": 33900
    },
    {
      "epoch": 0.796154071642141,
      "grad_norm": 0.37453383207321167,
      "learning_rate": 2.756940738905294e-05,
      "loss": 0.4656,
      "step": 33950
    },
    {
      "epoch": 0.7973266107756346,
      "grad_norm": 0.21938860416412354,
      "learning_rate": 2.7558227614113202e-05,
      "loss": 0.4413,
      "step": 34000
    },
    {
      "epoch": 0.7973266107756346,
      "eval_loss": 0.7782976627349854,
      "eval_runtime": 180.1623,
      "eval_samples_per_second": 55.506,
      "eval_steps_per_second": 13.876,
      "step": 34000
    },
    {
      "epoch": 0.7984991499091282,
      "grad_norm": 0.2552945911884308,
      "learning_rate": 2.754702446448872e-05,
      "loss": 0.4764,
      "step": 34050
    },
    {
      "epoch": 0.7996716890426218,
      "grad_norm": 0.4308377206325531,
      "learning_rate": 2.7535797961031964e-05,
      "loss": 0.4859,
      "step": 34100
    },
    {
      "epoch": 0.8008442281761153,
      "grad_norm": 0.22496432065963745,
      "learning_rate": 2.752454812463887e-05,
      "loss": 0.4814,
      "step": 34150
    },
    {
      "epoch": 0.8020167673096089,
      "grad_norm": 0.48513656854629517,
      "learning_rate": 2.7513274976248815e-05,
      "loss": 0.4805,
      "step": 34200
    },
    {
      "epoch": 0.8031893064431025,
      "grad_norm": 0.3384731411933899,
      "learning_rate": 2.7501978536844557e-05,
      "loss": 0.4953,
      "step": 34250
    },
    {
      "epoch": 0.8043618455765961,
      "grad_norm": 0.30744799971580505,
      "learning_rate": 2.7490658827452208e-05,
      "loss": 0.49,
      "step": 34300
    },
    {
      "epoch": 0.8055343847100896,
      "grad_norm": 0.39252138137817383,
      "learning_rate": 2.7479315869141192e-05,
      "loss": 0.478,
      "step": 34350
    },
    {
      "epoch": 0.8067069238435833,
      "grad_norm": 0.3304125666618347,
      "learning_rate": 2.7467949683024206e-05,
      "loss": 0.4871,
      "step": 34400
    },
    {
      "epoch": 0.8078794629770769,
      "grad_norm": 0.35035374760627747,
      "learning_rate": 2.7456560290257193e-05,
      "loss": 0.4507,
      "step": 34450
    },
    {
      "epoch": 0.8090520021105705,
      "grad_norm": 0.25637632608413696,
      "learning_rate": 2.7445147712039268e-05,
      "loss": 0.4781,
      "step": 34500
    },
    {
      "epoch": 0.8102245412440641,
      "grad_norm": 0.24009782075881958,
      "learning_rate": 2.743371196961272e-05,
      "loss": 0.425,
      "step": 34550
    },
    {
      "epoch": 0.8113970803775576,
      "grad_norm": 0.31455808877944946,
      "learning_rate": 2.742225308426295e-05,
      "loss": 0.447,
      "step": 34600
    },
    {
      "epoch": 0.8125696195110512,
      "grad_norm": 0.43528953194618225,
      "learning_rate": 2.741077107731842e-05,
      "loss": 0.502,
      "step": 34650
    },
    {
      "epoch": 0.8137421586445448,
      "grad_norm": 0.28361111879348755,
      "learning_rate": 2.7399265970150658e-05,
      "loss": 0.4333,
      "step": 34700
    },
    {
      "epoch": 0.8149146977780384,
      "grad_norm": 0.28842613101005554,
      "learning_rate": 2.7387737784174154e-05,
      "loss": 0.5066,
      "step": 34750
    },
    {
      "epoch": 0.816087236911532,
      "grad_norm": 0.3360283672809601,
      "learning_rate": 2.737618654084638e-05,
      "loss": 0.475,
      "step": 34800
    },
    {
      "epoch": 0.8172597760450255,
      "grad_norm": 0.21855302155017853,
      "learning_rate": 2.7364612261667717e-05,
      "loss": 0.4297,
      "step": 34850
    },
    {
      "epoch": 0.8184323151785191,
      "grad_norm": 0.2756008803844452,
      "learning_rate": 2.735301496818142e-05,
      "loss": 0.4643,
      "step": 34900
    },
    {
      "epoch": 0.8196048543120127,
      "grad_norm": 0.27630722522735596,
      "learning_rate": 2.7341394681973584e-05,
      "loss": 0.4607,
      "step": 34950
    },
    {
      "epoch": 0.8207773934455063,
      "grad_norm": 0.4181700348854065,
      "learning_rate": 2.73297514246731e-05,
      "loss": 0.4464,
      "step": 35000
    },
    {
      "epoch": 0.8219499325789998,
      "grad_norm": 0.42321503162384033,
      "learning_rate": 2.7318085217951607e-05,
      "loss": 0.501,
      "step": 35050
    },
    {
      "epoch": 0.8231224717124934,
      "grad_norm": 0.31668150424957275,
      "learning_rate": 2.730639608352348e-05,
      "loss": 0.5051,
      "step": 35100
    },
    {
      "epoch": 0.824295010845987,
      "grad_norm": 0.24747616052627563,
      "learning_rate": 2.7294684043145745e-05,
      "loss": 0.4704,
      "step": 35150
    },
    {
      "epoch": 0.8254675499794806,
      "grad_norm": 0.1941259801387787,
      "learning_rate": 2.7282949118618087e-05,
      "loss": 0.4681,
      "step": 35200
    },
    {
      "epoch": 0.8266400891129742,
      "grad_norm": 0.20958645641803741,
      "learning_rate": 2.7271191331782757e-05,
      "loss": 0.4437,
      "step": 35250
    },
    {
      "epoch": 0.8278126282464677,
      "grad_norm": 0.2930702567100525,
      "learning_rate": 2.725941070452459e-05,
      "loss": 0.5091,
      "step": 35300
    },
    {
      "epoch": 0.8289851673799613,
      "grad_norm": 0.2546606659889221,
      "learning_rate": 2.7247607258770912e-05,
      "loss": 0.4417,
      "step": 35350
    },
    {
      "epoch": 0.8301577065134549,
      "grad_norm": 0.35444745421409607,
      "learning_rate": 2.723578101649153e-05,
      "loss": 0.4819,
      "step": 35400
    },
    {
      "epoch": 0.8313302456469485,
      "grad_norm": 0.39967548847198486,
      "learning_rate": 2.722393199969869e-05,
      "loss": 0.4878,
      "step": 35450
    },
    {
      "epoch": 0.832502784780442,
      "grad_norm": 0.39170441031455994,
      "learning_rate": 2.7212060230447003e-05,
      "loss": 0.4194,
      "step": 35500
    },
    {
      "epoch": 0.8336753239139356,
      "grad_norm": 0.42215827107429504,
      "learning_rate": 2.720016573083346e-05,
      "loss": 0.5106,
      "step": 35550
    },
    {
      "epoch": 0.8348478630474292,
      "grad_norm": 0.49260345101356506,
      "learning_rate": 2.718824852299734e-05,
      "loss": 0.4916,
      "step": 35600
    },
    {
      "epoch": 0.8360204021809228,
      "grad_norm": 0.2517266273498535,
      "learning_rate": 2.7176308629120194e-05,
      "loss": 0.4801,
      "step": 35650
    },
    {
      "epoch": 0.8371929413144164,
      "grad_norm": 0.32990726828575134,
      "learning_rate": 2.7164346071425802e-05,
      "loss": 0.5194,
      "step": 35700
    },
    {
      "epoch": 0.8383654804479099,
      "grad_norm": 0.241962730884552,
      "learning_rate": 2.7152360872180127e-05,
      "loss": 0.4938,
      "step": 35750
    },
    {
      "epoch": 0.8395380195814035,
      "grad_norm": 0.2505636215209961,
      "learning_rate": 2.7140353053691264e-05,
      "loss": 0.4577,
      "step": 35800
    },
    {
      "epoch": 0.8407105587148971,
      "grad_norm": 0.20969200134277344,
      "learning_rate": 2.7128322638309432e-05,
      "loss": 0.4873,
      "step": 35850
    },
    {
      "epoch": 0.8418830978483907,
      "grad_norm": 0.3507837951183319,
      "learning_rate": 2.7116269648426888e-05,
      "loss": 0.506,
      "step": 35900
    },
    {
      "epoch": 0.8430556369818842,
      "grad_norm": 0.2029971182346344,
      "learning_rate": 2.7104194106477917e-05,
      "loss": 0.5049,
      "step": 35950
    },
    {
      "epoch": 0.8442281761153778,
      "grad_norm": 0.2929271161556244,
      "learning_rate": 2.709209603493878e-05,
      "loss": 0.4538,
      "step": 36000
    },
    {
      "epoch": 0.8442281761153778,
      "eval_loss": 0.7761250138282776,
      "eval_runtime": 152.1486,
      "eval_samples_per_second": 65.725,
      "eval_steps_per_second": 16.431,
      "step": 36000
    },
    {
      "epoch": 0.8454007152488714,
      "grad_norm": 0.3804580271244049,
      "learning_rate": 2.7079975456327674e-05,
      "loss": 0.4601,
      "step": 36050
    },
    {
      "epoch": 0.846573254382365,
      "grad_norm": 0.5053352117538452,
      "learning_rate": 2.706783239320468e-05,
      "loss": 0.4475,
      "step": 36100
    },
    {
      "epoch": 0.8477457935158585,
      "grad_norm": 0.2395636886358261,
      "learning_rate": 2.705566686817174e-05,
      "loss": 0.4389,
      "step": 36150
    },
    {
      "epoch": 0.8489183326493521,
      "grad_norm": 0.19828937947750092,
      "learning_rate": 2.7043478903872598e-05,
      "loss": 0.4592,
      "step": 36200
    },
    {
      "epoch": 0.8500908717828457,
      "grad_norm": 0.5412546992301941,
      "learning_rate": 2.7031268522992764e-05,
      "loss": 0.4803,
      "step": 36250
    },
    {
      "epoch": 0.8512634109163393,
      "grad_norm": 0.6560482382774353,
      "learning_rate": 2.7019035748259472e-05,
      "loss": 0.462,
      "step": 36300
    },
    {
      "epoch": 0.8524359500498329,
      "grad_norm": 0.25632235407829285,
      "learning_rate": 2.7006780602441646e-05,
      "loss": 0.4803,
      "step": 36350
    },
    {
      "epoch": 0.8536084891833265,
      "grad_norm": 0.19536954164505005,
      "learning_rate": 2.699450310834984e-05,
      "loss": 0.4509,
      "step": 36400
    },
    {
      "epoch": 0.8547810283168201,
      "grad_norm": 0.5095498561859131,
      "learning_rate": 2.698220328883621e-05,
      "loss": 0.4844,
      "step": 36450
    },
    {
      "epoch": 0.8559535674503137,
      "grad_norm": 0.22905981540679932,
      "learning_rate": 2.6969881166794464e-05,
      "loss": 0.4512,
      "step": 36500
    },
    {
      "epoch": 0.8571261065838073,
      "grad_norm": 0.245151087641716,
      "learning_rate": 2.6957536765159823e-05,
      "loss": 0.4999,
      "step": 36550
    },
    {
      "epoch": 0.8582986457173009,
      "grad_norm": 0.24738335609436035,
      "learning_rate": 2.6945170106908974e-05,
      "loss": 0.4617,
      "step": 36600
    },
    {
      "epoch": 0.8594711848507944,
      "grad_norm": 0.29665327072143555,
      "learning_rate": 2.6932781215060034e-05,
      "loss": 0.49,
      "step": 36650
    },
    {
      "epoch": 0.860643723984288,
      "grad_norm": 0.37453824281692505,
      "learning_rate": 2.69203701126725e-05,
      "loss": 0.4479,
      "step": 36700
    },
    {
      "epoch": 0.8618162631177816,
      "grad_norm": 0.26029208302497864,
      "learning_rate": 2.6907936822847218e-05,
      "loss": 0.4678,
      "step": 36750
    },
    {
      "epoch": 0.8629888022512752,
      "grad_norm": 0.20391486585140228,
      "learning_rate": 2.689548136872632e-05,
      "loss": 0.441,
      "step": 36800
    },
    {
      "epoch": 0.8641613413847687,
      "grad_norm": 0.3383202850818634,
      "learning_rate": 2.6883003773493203e-05,
      "loss": 0.4528,
      "step": 36850
    },
    {
      "epoch": 0.8653338805182623,
      "grad_norm": 0.4867863953113556,
      "learning_rate": 2.687050406037246e-05,
      "loss": 0.4569,
      "step": 36900
    },
    {
      "epoch": 0.8665064196517559,
      "grad_norm": 0.3700520694255829,
      "learning_rate": 2.6857982252629877e-05,
      "loss": 0.4592,
      "step": 36950
    },
    {
      "epoch": 0.8676789587852495,
      "grad_norm": 0.24229173362255096,
      "learning_rate": 2.684543837357234e-05,
      "loss": 0.4536,
      "step": 37000
    },
    {
      "epoch": 0.868851497918743,
      "grad_norm": 0.30392730236053467,
      "learning_rate": 2.6832872446547833e-05,
      "loss": 0.4725,
      "step": 37050
    },
    {
      "epoch": 0.8700240370522366,
      "grad_norm": 0.37742483615875244,
      "learning_rate": 2.6820284494945367e-05,
      "loss": 0.4518,
      "step": 37100
    },
    {
      "epoch": 0.8711965761857302,
      "grad_norm": 0.3164655864238739,
      "learning_rate": 2.6807674542194954e-05,
      "loss": 0.4889,
      "step": 37150
    },
    {
      "epoch": 0.8723691153192238,
      "grad_norm": 0.2663430869579315,
      "learning_rate": 2.6795042611767552e-05,
      "loss": 0.4683,
      "step": 37200
    },
    {
      "epoch": 0.8735416544527174,
      "grad_norm": 0.3926668167114258,
      "learning_rate": 2.678238872717503e-05,
      "loss": 0.4505,
      "step": 37250
    },
    {
      "epoch": 0.8747141935862109,
      "grad_norm": 0.257194846868515,
      "learning_rate": 2.6769712911970127e-05,
      "loss": 0.4826,
      "step": 37300
    },
    {
      "epoch": 0.8758867327197045,
      "grad_norm": 0.3308146893978119,
      "learning_rate": 2.6757015189746384e-05,
      "loss": 0.434,
      "step": 37350
    },
    {
      "epoch": 0.8770592718531981,
      "grad_norm": 0.35791629552841187,
      "learning_rate": 2.6744295584138132e-05,
      "loss": 0.4182,
      "step": 37400
    },
    {
      "epoch": 0.8782318109866917,
      "grad_norm": 0.3506341278553009,
      "learning_rate": 2.6731554118820434e-05,
      "loss": 0.4421,
      "step": 37450
    },
    {
      "epoch": 0.8794043501201853,
      "grad_norm": 0.18080991506576538,
      "learning_rate": 2.6718790817509026e-05,
      "loss": 0.4645,
      "step": 37500
    },
    {
      "epoch": 0.8805768892536788,
      "grad_norm": 0.34987449645996094,
      "learning_rate": 2.6706005703960306e-05,
      "loss": 0.4826,
      "step": 37550
    },
    {
      "epoch": 0.8817494283871724,
      "grad_norm": 0.6106429696083069,
      "learning_rate": 2.6693198801971257e-05,
      "loss": 0.4609,
      "step": 37600
    },
    {
      "epoch": 0.882921967520666,
      "grad_norm": 0.26826608180999756,
      "learning_rate": 2.6680370135379423e-05,
      "loss": 0.4647,
      "step": 37650
    },
    {
      "epoch": 0.8840945066541596,
      "grad_norm": 0.19930626451969147,
      "learning_rate": 2.666751972806286e-05,
      "loss": 0.4281,
      "step": 37700
    },
    {
      "epoch": 0.8852670457876531,
      "grad_norm": 0.3148965537548065,
      "learning_rate": 2.665464760394009e-05,
      "loss": 0.4889,
      "step": 37750
    },
    {
      "epoch": 0.8864395849211467,
      "grad_norm": 0.368703156709671,
      "learning_rate": 2.664175378697005e-05,
      "loss": 0.4821,
      "step": 37800
    },
    {
      "epoch": 0.8876121240546403,
      "grad_norm": 0.3381229639053345,
      "learning_rate": 2.662883830115206e-05,
      "loss": 0.4899,
      "step": 37850
    },
    {
      "epoch": 0.8887846631881339,
      "grad_norm": 0.2585836350917816,
      "learning_rate": 2.6615901170525777e-05,
      "loss": 0.4552,
      "step": 37900
    },
    {
      "epoch": 0.8899572023216274,
      "grad_norm": 0.2221837192773819,
      "learning_rate": 2.660294241917113e-05,
      "loss": 0.4732,
      "step": 37950
    },
    {
      "epoch": 0.891129741455121,
      "grad_norm": 0.3575211763381958,
      "learning_rate": 2.658996207120831e-05,
      "loss": 0.4302,
      "step": 38000
    },
    {
      "epoch": 0.891129741455121,
      "eval_loss": 0.7798884510993958,
      "eval_runtime": 147.4151,
      "eval_samples_per_second": 67.836,
      "eval_steps_per_second": 16.959,
      "step": 38000
    },
    {
      "epoch": 0.8923022805886146,
      "grad_norm": 0.3955143392086029,
      "learning_rate": 2.6576960150797686e-05,
      "loss": 0.4822,
      "step": 38050
    },
    {
      "epoch": 0.8934748197221082,
      "grad_norm": 0.30287593603134155,
      "learning_rate": 2.6563936682139797e-05,
      "loss": 0.4731,
      "step": 38100
    },
    {
      "epoch": 0.8946473588556018,
      "grad_norm": 0.20277608931064606,
      "learning_rate": 2.6550891689475284e-05,
      "loss": 0.4575,
      "step": 38150
    },
    {
      "epoch": 0.8958198979890953,
      "grad_norm": 0.3463292121887207,
      "learning_rate": 2.6537825197084845e-05,
      "loss": 0.4631,
      "step": 38200
    },
    {
      "epoch": 0.8969924371225889,
      "grad_norm": 0.4513590931892395,
      "learning_rate": 2.6524737229289205e-05,
      "loss": 0.4439,
      "step": 38250
    },
    {
      "epoch": 0.8981649762560825,
      "grad_norm": 0.269883930683136,
      "learning_rate": 2.6511627810449045e-05,
      "loss": 0.4559,
      "step": 38300
    },
    {
      "epoch": 0.8993375153895762,
      "grad_norm": 0.4041230082511902,
      "learning_rate": 2.6498496964965e-05,
      "loss": 0.4836,
      "step": 38350
    },
    {
      "epoch": 0.9005100545230698,
      "grad_norm": 0.34230703115463257,
      "learning_rate": 2.6485344717277565e-05,
      "loss": 0.5005,
      "step": 38400
    },
    {
      "epoch": 0.9016825936565633,
      "grad_norm": 0.42894333600997925,
      "learning_rate": 2.6472171091867074e-05,
      "loss": 0.4503,
      "step": 38450
    },
    {
      "epoch": 0.9028551327900569,
      "grad_norm": 0.2946648597717285,
      "learning_rate": 2.6458976113253658e-05,
      "loss": 0.4551,
      "step": 38500
    },
    {
      "epoch": 0.9040276719235505,
      "grad_norm": 0.16855238378047943,
      "learning_rate": 2.6445759805997184e-05,
      "loss": 0.4794,
      "step": 38550
    },
    {
      "epoch": 0.9052002110570441,
      "grad_norm": 0.24397215247154236,
      "learning_rate": 2.643252219469723e-05,
      "loss": 0.4688,
      "step": 38600
    },
    {
      "epoch": 0.9063727501905376,
      "grad_norm": 0.3125292956829071,
      "learning_rate": 2.6419263303993018e-05,
      "loss": 0.4995,
      "step": 38650
    },
    {
      "epoch": 0.9075452893240312,
      "grad_norm": 0.4459710419178009,
      "learning_rate": 2.640598315856338e-05,
      "loss": 0.4969,
      "step": 38700
    },
    {
      "epoch": 0.9087178284575248,
      "grad_norm": 0.2624552845954895,
      "learning_rate": 2.6392681783126707e-05,
      "loss": 0.418,
      "step": 38750
    },
    {
      "epoch": 0.9098903675910184,
      "grad_norm": 0.1843908280134201,
      "learning_rate": 2.637935920244091e-05,
      "loss": 0.4805,
      "step": 38800
    },
    {
      "epoch": 0.911062906724512,
      "grad_norm": 0.23809851706027985,
      "learning_rate": 2.636601544130337e-05,
      "loss": 0.4452,
      "step": 38850
    },
    {
      "epoch": 0.9122354458580055,
      "grad_norm": 0.3241436779499054,
      "learning_rate": 2.6352650524550885e-05,
      "loss": 0.4554,
      "step": 38900
    },
    {
      "epoch": 0.9134079849914991,
      "grad_norm": 0.28402554988861084,
      "learning_rate": 2.6339264477059637e-05,
      "loss": 0.4296,
      "step": 38950
    },
    {
      "epoch": 0.9145805241249927,
      "grad_norm": 0.21284255385398865,
      "learning_rate": 2.6325857323745127e-05,
      "loss": 0.4636,
      "step": 39000
    },
    {
      "epoch": 0.9157530632584863,
      "grad_norm": 0.17413106560707092,
      "learning_rate": 2.6312429089562165e-05,
      "loss": 0.4387,
      "step": 39050
    },
    {
      "epoch": 0.9169256023919798,
      "grad_norm": 0.3287668228149414,
      "learning_rate": 2.6298979799504773e-05,
      "loss": 0.4198,
      "step": 39100
    },
    {
      "epoch": 0.9180981415254734,
      "grad_norm": 0.4725607931613922,
      "learning_rate": 2.628550947860617e-05,
      "loss": 0.4874,
      "step": 39150
    },
    {
      "epoch": 0.919270680658967,
      "grad_norm": 0.15682491660118103,
      "learning_rate": 2.627201815193873e-05,
      "loss": 0.4614,
      "step": 39200
    },
    {
      "epoch": 0.9204432197924606,
      "grad_norm": 0.30039966106414795,
      "learning_rate": 2.6258505844613923e-05,
      "loss": 0.4874,
      "step": 39250
    },
    {
      "epoch": 0.9216157589259542,
      "grad_norm": 0.2875814735889435,
      "learning_rate": 2.6244972581782256e-05,
      "loss": 0.4177,
      "step": 39300
    },
    {
      "epoch": 0.9227882980594477,
      "grad_norm": 0.18652430176734924,
      "learning_rate": 2.6231418388633258e-05,
      "loss": 0.4596,
      "step": 39350
    },
    {
      "epoch": 0.9239608371929413,
      "grad_norm": 0.276409775018692,
      "learning_rate": 2.621784329039541e-05,
      "loss": 0.4412,
      "step": 39400
    },
    {
      "epoch": 0.9251333763264349,
      "grad_norm": 0.2768758535385132,
      "learning_rate": 2.6204247312336095e-05,
      "loss": 0.4677,
      "step": 39450
    },
    {
      "epoch": 0.9263059154599285,
      "grad_norm": 0.16083739697933197,
      "learning_rate": 2.619063047976158e-05,
      "loss": 0.5071,
      "step": 39500
    },
    {
      "epoch": 0.927478454593422,
      "grad_norm": 0.4822770357131958,
      "learning_rate": 2.617699281801692e-05,
      "loss": 0.439,
      "step": 39550
    },
    {
      "epoch": 0.9286509937269156,
      "grad_norm": 0.23506338894367218,
      "learning_rate": 2.6163334352485968e-05,
      "loss": 0.4763,
      "step": 39600
    },
    {
      "epoch": 0.9298235328604092,
      "grad_norm": 0.31369245052337646,
      "learning_rate": 2.6149655108591282e-05,
      "loss": 0.4663,
      "step": 39650
    },
    {
      "epoch": 0.9309960719939028,
      "grad_norm": 0.28077974915504456,
      "learning_rate": 2.61359551117941e-05,
      "loss": 0.4929,
      "step": 39700
    },
    {
      "epoch": 0.9321686111273964,
      "grad_norm": 0.28528642654418945,
      "learning_rate": 2.6122234387594286e-05,
      "loss": 0.4889,
      "step": 39750
    },
    {
      "epoch": 0.9333411502608899,
      "grad_norm": 0.4123665988445282,
      "learning_rate": 2.610849296153028e-05,
      "loss": 0.4277,
      "step": 39800
    },
    {
      "epoch": 0.9345136893943835,
      "grad_norm": 0.21088719367980957,
      "learning_rate": 2.6094730859179076e-05,
      "loss": 0.436,
      "step": 39850
    },
    {
      "epoch": 0.9356862285278771,
      "grad_norm": 0.2713005542755127,
      "learning_rate": 2.608094810615611e-05,
      "loss": 0.422,
      "step": 39900
    },
    {
      "epoch": 0.9368587676613707,
      "grad_norm": 0.3910861313343048,
      "learning_rate": 2.6067144728115305e-05,
      "loss": 0.4739,
      "step": 39950
    },
    {
      "epoch": 0.9380313067948642,
      "grad_norm": 0.1874319165945053,
      "learning_rate": 2.6053320750748938e-05,
      "loss": 0.4512,
      "step": 40000
    },
    {
      "epoch": 0.9380313067948642,
      "eval_loss": 0.7742698192596436,
      "eval_runtime": 244.9982,
      "eval_samples_per_second": 40.817,
      "eval_steps_per_second": 10.204,
      "step": 40000
    },
    {
      "epoch": 0.9392038459283578,
      "grad_norm": 0.2836630046367645,
      "learning_rate": 2.603947619978764e-05,
      "loss": 0.4776,
      "step": 40050
    },
    {
      "epoch": 0.9403763850618514,
      "grad_norm": 0.5880705118179321,
      "learning_rate": 2.6025611101000338e-05,
      "loss": 0.4969,
      "step": 40100
    },
    {
      "epoch": 0.941548924195345,
      "grad_norm": 0.4537521302700043,
      "learning_rate": 2.6011725480194198e-05,
      "loss": 0.469,
      "step": 40150
    },
    {
      "epoch": 0.9427214633288385,
      "grad_norm": 0.27537086606025696,
      "learning_rate": 2.5997819363214594e-05,
      "loss": 0.4152,
      "step": 40200
    },
    {
      "epoch": 0.9438940024623321,
      "grad_norm": 0.4546855390071869,
      "learning_rate": 2.598389277594504e-05,
      "loss": 0.4799,
      "step": 40250
    },
    {
      "epoch": 0.9450665415958258,
      "grad_norm": 0.4848061501979828,
      "learning_rate": 2.5969945744307153e-05,
      "loss": 0.4545,
      "step": 40300
    },
    {
      "epoch": 0.9462390807293194,
      "grad_norm": 0.23662084341049194,
      "learning_rate": 2.5955978294260608e-05,
      "loss": 0.5114,
      "step": 40350
    },
    {
      "epoch": 0.947411619862813,
      "grad_norm": 0.2971338927745819,
      "learning_rate": 2.5941990451803074e-05,
      "loss": 0.4912,
      "step": 40400
    },
    {
      "epoch": 0.9485841589963065,
      "grad_norm": 0.4477233290672302,
      "learning_rate": 2.5927982242970195e-05,
      "loss": 0.4613,
      "step": 40450
    },
    {
      "epoch": 0.9497566981298001,
      "grad_norm": 0.21869561076164246,
      "learning_rate": 2.5913953693835503e-05,
      "loss": 0.4477,
      "step": 40500
    },
    {
      "epoch": 0.9509292372632937,
      "grad_norm": 0.29663124680519104,
      "learning_rate": 2.5899904830510408e-05,
      "loss": 0.4403,
      "step": 40550
    },
    {
      "epoch": 0.9521017763967873,
      "grad_norm": 0.5803835988044739,
      "learning_rate": 2.5885835679144113e-05,
      "loss": 0.4563,
      "step": 40600
    },
    {
      "epoch": 0.9532743155302809,
      "grad_norm": 0.19140291213989258,
      "learning_rate": 2.5871746265923592e-05,
      "loss": 0.4417,
      "step": 40650
    },
    {
      "epoch": 0.9544468546637744,
      "grad_norm": 0.32720136642456055,
      "learning_rate": 2.585763661707354e-05,
      "loss": 0.4336,
      "step": 40700
    },
    {
      "epoch": 0.955619393797268,
      "grad_norm": 0.3193209171295166,
      "learning_rate": 2.5843506758856306e-05,
      "loss": 0.4831,
      "step": 40750
    },
    {
      "epoch": 0.9567919329307616,
      "grad_norm": 0.3538282513618469,
      "learning_rate": 2.5829356717571856e-05,
      "loss": 0.4918,
      "step": 40800
    },
    {
      "epoch": 0.9579644720642552,
      "grad_norm": 0.22952662408351898,
      "learning_rate": 2.581518651955772e-05,
      "loss": 0.44,
      "step": 40850
    },
    {
      "epoch": 0.9591370111977487,
      "grad_norm": 0.3474271297454834,
      "learning_rate": 2.580099619118896e-05,
      "loss": 0.4634,
      "step": 40900
    },
    {
      "epoch": 0.9603095503312423,
      "grad_norm": 0.26532530784606934,
      "learning_rate": 2.5786785758878092e-05,
      "loss": 0.4495,
      "step": 40950
    },
    {
      "epoch": 0.9614820894647359,
      "grad_norm": 0.17456337809562683,
      "learning_rate": 2.5772555249075054e-05,
      "loss": 0.505,
      "step": 41000
    },
    {
      "epoch": 0.9626546285982295,
      "grad_norm": 0.3019118309020996,
      "learning_rate": 2.5758304688267163e-05,
      "loss": 0.4608,
      "step": 41050
    },
    {
      "epoch": 0.9638271677317231,
      "grad_norm": 0.22637015581130981,
      "learning_rate": 2.5744034102979046e-05,
      "loss": 0.4868,
      "step": 41100
    },
    {
      "epoch": 0.9649997068652166,
      "grad_norm": 0.5613027215003967,
      "learning_rate": 2.572974351977261e-05,
      "loss": 0.4956,
      "step": 41150
    },
    {
      "epoch": 0.9661722459987102,
      "grad_norm": 0.37759172916412354,
      "learning_rate": 2.571543296524698e-05,
      "loss": 0.4554,
      "step": 41200
    },
    {
      "epoch": 0.9673447851322038,
      "grad_norm": 0.25686943531036377,
      "learning_rate": 2.570110246603845e-05,
      "loss": 0.4676,
      "step": 41250
    },
    {
      "epoch": 0.9685173242656974,
      "grad_norm": 0.31427332758903503,
      "learning_rate": 2.5686752048820453e-05,
      "loss": 0.48,
      "step": 41300
    },
    {
      "epoch": 0.9696898633991909,
      "grad_norm": 0.18901672959327698,
      "learning_rate": 2.5672381740303472e-05,
      "loss": 0.4508,
      "step": 41350
    },
    {
      "epoch": 0.9708624025326845,
      "grad_norm": 0.3456437289714813,
      "learning_rate": 2.5657991567235033e-05,
      "loss": 0.4852,
      "step": 41400
    },
    {
      "epoch": 0.9720349416661781,
      "grad_norm": 0.3844403624534607,
      "learning_rate": 2.564358155639963e-05,
      "loss": 0.4625,
      "step": 41450
    },
    {
      "epoch": 0.9732074807996717,
      "grad_norm": 0.2778930366039276,
      "learning_rate": 2.5629151734618673e-05,
      "loss": 0.5191,
      "step": 41500
    },
    {
      "epoch": 0.9743800199331653,
      "grad_norm": 0.325923353433609,
      "learning_rate": 2.5614702128750457e-05,
      "loss": 0.4536,
      "step": 41550
    },
    {
      "epoch": 0.9755525590666588,
      "grad_norm": 0.5015804171562195,
      "learning_rate": 2.56002327656901e-05,
      "loss": 0.5,
      "step": 41600
    },
    {
      "epoch": 0.9767250982001524,
      "grad_norm": 0.49659815430641174,
      "learning_rate": 2.5585743672369494e-05,
      "loss": 0.4927,
      "step": 41650
    },
    {
      "epoch": 0.977897637333646,
      "grad_norm": 0.32334476709365845,
      "learning_rate": 2.557123487575724e-05,
      "loss": 0.4487,
      "step": 41700
    },
    {
      "epoch": 0.9790701764671396,
      "grad_norm": 0.314970999956131,
      "learning_rate": 2.555670640285864e-05,
      "loss": 0.4815,
      "step": 41750
    },
    {
      "epoch": 0.9802427156006331,
      "grad_norm": 0.3756744861602783,
      "learning_rate": 2.554215828071559e-05,
      "loss": 0.5044,
      "step": 41800
    },
    {
      "epoch": 0.9814152547341267,
      "grad_norm": 0.442545622587204,
      "learning_rate": 2.552759053640659e-05,
      "loss": 0.4755,
      "step": 41850
    },
    {
      "epoch": 0.9825877938676203,
      "grad_norm": 0.3464816212654114,
      "learning_rate": 2.5513003197046643e-05,
      "loss": 0.4555,
      "step": 41900
    },
    {
      "epoch": 0.9837603330011139,
      "grad_norm": 0.26406410336494446,
      "learning_rate": 2.549839628978722e-05,
      "loss": 0.4478,
      "step": 41950
    },
    {
      "epoch": 0.9849328721346075,
      "grad_norm": 0.2803037464618683,
      "learning_rate": 2.5483769841816227e-05,
      "loss": 0.4689,
      "step": 42000
    },
    {
      "epoch": 0.9849328721346075,
      "eval_loss": 0.775261402130127,
      "eval_runtime": 164.8076,
      "eval_samples_per_second": 60.677,
      "eval_steps_per_second": 15.169,
      "step": 42000
    },
    {
      "epoch": 0.986105411268101,
      "grad_norm": 0.3114904463291168,
      "learning_rate": 2.5469123880357943e-05,
      "loss": 0.4938,
      "step": 42050
    },
    {
      "epoch": 0.9872779504015946,
      "grad_norm": 0.339491605758667,
      "learning_rate": 2.545445843267295e-05,
      "loss": 0.4515,
      "step": 42100
    },
    {
      "epoch": 0.9884504895350882,
      "grad_norm": 0.6373336911201477,
      "learning_rate": 2.5439773526058116e-05,
      "loss": 0.4743,
      "step": 42150
    },
    {
      "epoch": 0.9896230286685818,
      "grad_norm": 0.22980764508247375,
      "learning_rate": 2.5425069187846527e-05,
      "loss": 0.4593,
      "step": 42200
    },
    {
      "epoch": 0.9907955678020754,
      "grad_norm": 0.30239495635032654,
      "learning_rate": 2.5410345445407426e-05,
      "loss": 0.4436,
      "step": 42250
    },
    {
      "epoch": 0.991968106935569,
      "grad_norm": 0.44114598631858826,
      "learning_rate": 2.5395602326146184e-05,
      "loss": 0.4788,
      "step": 42300
    },
    {
      "epoch": 0.9931406460690626,
      "grad_norm": 0.22971509397029877,
      "learning_rate": 2.5380839857504232e-05,
      "loss": 0.4561,
      "step": 42350
    },
    {
      "epoch": 0.9943131852025562,
      "grad_norm": 0.2149183601140976,
      "learning_rate": 2.5366058066959018e-05,
      "loss": 0.5008,
      "step": 42400
    },
    {
      "epoch": 0.9954857243360498,
      "grad_norm": 0.4608462154865265,
      "learning_rate": 2.535125698202396e-05,
      "loss": 0.4484,
      "step": 42450
    },
    {
      "epoch": 0.9966582634695433,
      "grad_norm": 0.22279641032218933,
      "learning_rate": 2.533643663024837e-05,
      "loss": 0.4868,
      "step": 42500
    },
    {
      "epoch": 0.9978308026030369,
      "grad_norm": 0.38561561703681946,
      "learning_rate": 2.5321597039217456e-05,
      "loss": 0.4321,
      "step": 42550
    },
    {
      "epoch": 0.9990033417365305,
      "grad_norm": 0.35953935980796814,
      "learning_rate": 2.5306738236552192e-05,
      "loss": 0.4659,
      "step": 42600
    },
    {
      "epoch": 1.000187606261359,
      "grad_norm": 0.26814937591552734,
      "learning_rate": 2.5291860249909348e-05,
      "loss": 0.4548,
      "step": 42650
    },
    {
      "epoch": 1.0013601453948526,
      "grad_norm": 0.3527926504611969,
      "learning_rate": 2.527696310698138e-05,
      "loss": 0.5185,
      "step": 42700
    },
    {
      "epoch": 1.0025326845283462,
      "grad_norm": 0.4384733736515045,
      "learning_rate": 2.5262046835496408e-05,
      "loss": 0.4511,
      "step": 42750
    },
    {
      "epoch": 1.0037052236618398,
      "grad_norm": 0.2234208732843399,
      "learning_rate": 2.5247111463218152e-05,
      "loss": 0.4807,
      "step": 42800
    },
    {
      "epoch": 1.0048777627953334,
      "grad_norm": 0.40675610303878784,
      "learning_rate": 2.523215701794589e-05,
      "loss": 0.482,
      "step": 42850
    },
    {
      "epoch": 1.006050301928827,
      "grad_norm": 0.31032317876815796,
      "learning_rate": 2.52171835275144e-05,
      "loss": 0.4684,
      "step": 42900
    },
    {
      "epoch": 1.0072228410623205,
      "grad_norm": 0.22920435667037964,
      "learning_rate": 2.52021910197939e-05,
      "loss": 0.495,
      "step": 42950
    },
    {
      "epoch": 1.008395380195814,
      "grad_norm": 0.38852113485336304,
      "learning_rate": 2.5187179522690013e-05,
      "loss": 0.43,
      "step": 43000
    },
    {
      "epoch": 1.0095679193293077,
      "grad_norm": 0.23480693995952606,
      "learning_rate": 2.51721490641437e-05,
      "loss": 0.4756,
      "step": 43050
    },
    {
      "epoch": 1.0107404584628013,
      "grad_norm": 0.37333112955093384,
      "learning_rate": 2.5157099672131236e-05,
      "loss": 0.4462,
      "step": 43100
    },
    {
      "epoch": 1.0119129975962948,
      "grad_norm": 0.5053502321243286,
      "learning_rate": 2.51420313746641e-05,
      "loss": 0.4832,
      "step": 43150
    },
    {
      "epoch": 1.0130855367297884,
      "grad_norm": 0.3302527666091919,
      "learning_rate": 2.5126944199789e-05,
      "loss": 0.456,
      "step": 43200
    },
    {
      "epoch": 1.014258075863282,
      "grad_norm": 0.3674245774745941,
      "learning_rate": 2.5111838175587748e-05,
      "loss": 0.4622,
      "step": 43250
    },
    {
      "epoch": 1.0154306149967756,
      "grad_norm": 0.3069384694099426,
      "learning_rate": 2.509671333017726e-05,
      "loss": 0.4245,
      "step": 43300
    },
    {
      "epoch": 1.0166031541302691,
      "grad_norm": 0.21687595546245575,
      "learning_rate": 2.5081569691709482e-05,
      "loss": 0.4765,
      "step": 43350
    },
    {
      "epoch": 1.0177756932637627,
      "grad_norm": 0.3400116562843323,
      "learning_rate": 2.5066407288371327e-05,
      "loss": 0.4527,
      "step": 43400
    },
    {
      "epoch": 1.0189482323972563,
      "grad_norm": 0.28814896941185,
      "learning_rate": 2.5051226148384644e-05,
      "loss": 0.4858,
      "step": 43450
    },
    {
      "epoch": 1.0201207715307499,
      "grad_norm": 0.4635160267353058,
      "learning_rate": 2.503602630000616e-05,
      "loss": 0.4622,
      "step": 43500
    },
    {
      "epoch": 1.0212933106642434,
      "grad_norm": 0.23068250715732574,
      "learning_rate": 2.5020807771527416e-05,
      "loss": 0.4474,
      "step": 43550
    },
    {
      "epoch": 1.022465849797737,
      "grad_norm": 0.325385719537735,
      "learning_rate": 2.5005570591274735e-05,
      "loss": 0.4647,
      "step": 43600
    },
    {
      "epoch": 1.0236383889312306,
      "grad_norm": 0.421852171421051,
      "learning_rate": 2.4990314787609143e-05,
      "loss": 0.4923,
      "step": 43650
    },
    {
      "epoch": 1.0248109280647242,
      "grad_norm": 0.32556286454200745,
      "learning_rate": 2.497504038892633e-05,
      "loss": 0.4325,
      "step": 43700
    },
    {
      "epoch": 1.0259834671982178,
      "grad_norm": 0.4686724841594696,
      "learning_rate": 2.4959747423656613e-05,
      "loss": 0.4432,
      "step": 43750
    },
    {
      "epoch": 1.0271560063317113,
      "grad_norm": 0.46982401609420776,
      "learning_rate": 2.4944435920264848e-05,
      "loss": 0.4615,
      "step": 43800
    },
    {
      "epoch": 1.028328545465205,
      "grad_norm": 0.3054119944572449,
      "learning_rate": 2.4929105907250402e-05,
      "loss": 0.4689,
      "step": 43850
    },
    {
      "epoch": 1.0295010845986985,
      "grad_norm": 0.4050036668777466,
      "learning_rate": 2.4913757413147104e-05,
      "loss": 0.4863,
      "step": 43900
    },
    {
      "epoch": 1.030673623732192,
      "grad_norm": 0.2533418834209442,
      "learning_rate": 2.4898390466523167e-05,
      "loss": 0.4899,
      "step": 43950
    },
    {
      "epoch": 1.0318461628656856,
      "grad_norm": 0.22604520618915558,
      "learning_rate": 2.488300509598116e-05,
      "loss": 0.4687,
      "step": 44000
    },
    {
      "epoch": 1.0318461628656856,
      "eval_loss": 0.7714993357658386,
      "eval_runtime": 151.4443,
      "eval_samples_per_second": 66.031,
      "eval_steps_per_second": 16.508,
      "step": 44000
    },
    {
      "epoch": 1.0330187019991792,
      "grad_norm": 0.4270537793636322,
      "learning_rate": 2.4867601330157947e-05,
      "loss": 0.4508,
      "step": 44050
    },
    {
      "epoch": 1.0341912411326728,
      "grad_norm": 0.37385401129722595,
      "learning_rate": 2.485217919772462e-05,
      "loss": 0.4444,
      "step": 44100
    },
    {
      "epoch": 1.0353637802661664,
      "grad_norm": 0.4529595673084259,
      "learning_rate": 2.483673872738646e-05,
      "loss": 0.4976,
      "step": 44150
    },
    {
      "epoch": 1.03653631939966,
      "grad_norm": 0.2407727688550949,
      "learning_rate": 2.4821279947882887e-05,
      "loss": 0.4397,
      "step": 44200
    },
    {
      "epoch": 1.0377088585331535,
      "grad_norm": 0.42945966124534607,
      "learning_rate": 2.4805802887987395e-05,
      "loss": 0.4689,
      "step": 44250
    },
    {
      "epoch": 1.038881397666647,
      "grad_norm": 0.18237251043319702,
      "learning_rate": 2.4790307576507498e-05,
      "loss": 0.4194,
      "step": 44300
    },
    {
      "epoch": 1.0400539368001407,
      "grad_norm": 0.3094758093357086,
      "learning_rate": 2.47747940422847e-05,
      "loss": 0.4995,
      "step": 44350
    },
    {
      "epoch": 1.0412264759336343,
      "grad_norm": 0.3237171173095703,
      "learning_rate": 2.4759262314194405e-05,
      "loss": 0.4553,
      "step": 44400
    },
    {
      "epoch": 1.0423990150671278,
      "grad_norm": 0.2576860785484314,
      "learning_rate": 2.4743712421145884e-05,
      "loss": 0.4709,
      "step": 44450
    },
    {
      "epoch": 1.0435715542006214,
      "grad_norm": 0.4657030999660492,
      "learning_rate": 2.4728144392082225e-05,
      "loss": 0.4596,
      "step": 44500
    },
    {
      "epoch": 1.044744093334115,
      "grad_norm": 0.1767319291830063,
      "learning_rate": 2.4712558255980275e-05,
      "loss": 0.5027,
      "step": 44550
    },
    {
      "epoch": 1.0459166324676086,
      "grad_norm": 0.16576695442199707,
      "learning_rate": 2.4696954041850565e-05,
      "loss": 0.4725,
      "step": 44600
    },
    {
      "epoch": 1.0470891716011022,
      "grad_norm": 0.20343628525733948,
      "learning_rate": 2.4681331778737303e-05,
      "loss": 0.4879,
      "step": 44650
    },
    {
      "epoch": 1.0482617107345957,
      "grad_norm": 0.26407548785209656,
      "learning_rate": 2.4665691495718265e-05,
      "loss": 0.4692,
      "step": 44700
    },
    {
      "epoch": 1.0494342498680893,
      "grad_norm": 0.4051646590232849,
      "learning_rate": 2.4650033221904783e-05,
      "loss": 0.4551,
      "step": 44750
    },
    {
      "epoch": 1.0506067890015829,
      "grad_norm": 0.4546118676662445,
      "learning_rate": 2.463435698644168e-05,
      "loss": 0.4746,
      "step": 44800
    },
    {
      "epoch": 1.0517793281350765,
      "grad_norm": 0.24629904329776764,
      "learning_rate": 2.4618662818507186e-05,
      "loss": 0.5025,
      "step": 44850
    },
    {
      "epoch": 1.05295186726857,
      "grad_norm": 0.3316728472709656,
      "learning_rate": 2.4602950747312935e-05,
      "loss": 0.455,
      "step": 44900
    },
    {
      "epoch": 1.0541244064020636,
      "grad_norm": 0.31769704818725586,
      "learning_rate": 2.458722080210388e-05,
      "loss": 0.444,
      "step": 44950
    },
    {
      "epoch": 1.0552969455355572,
      "grad_norm": 0.222926065325737,
      "learning_rate": 2.4571473012158226e-05,
      "loss": 0.4577,
      "step": 45000
    },
    {
      "epoch": 1.0564694846690508,
      "grad_norm": 0.23012319207191467,
      "learning_rate": 2.455570740678741e-05,
      "loss": 0.4341,
      "step": 45050
    },
    {
      "epoch": 1.0576420238025444,
      "grad_norm": 0.6933392882347107,
      "learning_rate": 2.453992401533602e-05,
      "loss": 0.4525,
      "step": 45100
    },
    {
      "epoch": 1.058814562936038,
      "grad_norm": 0.22951333224773407,
      "learning_rate": 2.4524122867181762e-05,
      "loss": 0.4637,
      "step": 45150
    },
    {
      "epoch": 1.0599871020695315,
      "grad_norm": 0.5820465683937073,
      "learning_rate": 2.4508303991735376e-05,
      "loss": 0.4486,
      "step": 45200
    },
    {
      "epoch": 1.061159641203025,
      "grad_norm": 0.32490694522857666,
      "learning_rate": 2.44924674184406e-05,
      "loss": 0.4253,
      "step": 45250
    },
    {
      "epoch": 1.0623321803365187,
      "grad_norm": 0.42521318793296814,
      "learning_rate": 2.447661317677413e-05,
      "loss": 0.4617,
      "step": 45300
    },
    {
      "epoch": 1.0635047194700122,
      "grad_norm": 0.17260076105594635,
      "learning_rate": 2.4460741296245528e-05,
      "loss": 0.4515,
      "step": 45350
    },
    {
      "epoch": 1.0646772586035058,
      "grad_norm": 0.18162529170513153,
      "learning_rate": 2.44448518063972e-05,
      "loss": 0.4592,
      "step": 45400
    },
    {
      "epoch": 1.0658497977369994,
      "grad_norm": 0.24359199404716492,
      "learning_rate": 2.4428944736804318e-05,
      "loss": 0.4672,
      "step": 45450
    },
    {
      "epoch": 1.067022336870493,
      "grad_norm": 0.2283577173948288,
      "learning_rate": 2.4413020117074786e-05,
      "loss": 0.432,
      "step": 45500
    },
    {
      "epoch": 1.0681948760039865,
      "grad_norm": 0.4274177849292755,
      "learning_rate": 2.439707797684917e-05,
      "loss": 0.4764,
      "step": 45550
    },
    {
      "epoch": 1.0693674151374801,
      "grad_norm": 0.37520188093185425,
      "learning_rate": 2.438111834580064e-05,
      "loss": 0.5096,
      "step": 45600
    },
    {
      "epoch": 1.0705399542709737,
      "grad_norm": 0.45455706119537354,
      "learning_rate": 2.4365141253634936e-05,
      "loss": 0.4876,
      "step": 45650
    },
    {
      "epoch": 1.0717124934044673,
      "grad_norm": 0.2826443612575531,
      "learning_rate": 2.4349146730090284e-05,
      "loss": 0.4705,
      "step": 45700
    },
    {
      "epoch": 1.0728850325379609,
      "grad_norm": 0.48099827766418457,
      "learning_rate": 2.4333134804937364e-05,
      "loss": 0.461,
      "step": 45750
    },
    {
      "epoch": 1.0740575716714544,
      "grad_norm": 0.4246094822883606,
      "learning_rate": 2.4317105507979247e-05,
      "loss": 0.4595,
      "step": 45800
    },
    {
      "epoch": 1.075230110804948,
      "grad_norm": 0.4215719699859619,
      "learning_rate": 2.4301058869051334e-05,
      "loss": 0.449,
      "step": 45850
    },
    {
      "epoch": 1.0764026499384416,
      "grad_norm": 0.4037249982357025,
      "learning_rate": 2.42849949180213e-05,
      "loss": 0.4553,
      "step": 45900
    },
    {
      "epoch": 1.0775751890719354,
      "grad_norm": 0.2620692253112793,
      "learning_rate": 2.4268913684789048e-05,
      "loss": 0.4701,
      "step": 45950
    },
    {
      "epoch": 1.0787477282054287,
      "grad_norm": 0.42267531156539917,
      "learning_rate": 2.4252815199286656e-05,
      "loss": 0.479,
      "step": 46000
    },
    {
      "epoch": 1.0787477282054287,
      "eval_loss": 0.7677583694458008,
      "eval_runtime": 118.3986,
      "eval_samples_per_second": 84.46,
      "eval_steps_per_second": 21.115,
      "step": 46000
    },
    {
      "epoch": 1.0799202673389225,
      "grad_norm": 0.3689723610877991,
      "learning_rate": 2.4236699491478296e-05,
      "loss": 0.4669,
      "step": 46050
    },
    {
      "epoch": 1.081092806472416,
      "grad_norm": 0.4241240322589874,
      "learning_rate": 2.4220566591360213e-05,
      "loss": 0.4735,
      "step": 46100
    },
    {
      "epoch": 1.0822653456059097,
      "grad_norm": 0.3144536316394806,
      "learning_rate": 2.420441652896065e-05,
      "loss": 0.4848,
      "step": 46150
    },
    {
      "epoch": 1.0834378847394033,
      "grad_norm": 0.5065315365791321,
      "learning_rate": 2.4188249334339782e-05,
      "loss": 0.508,
      "step": 46200
    },
    {
      "epoch": 1.0846104238728969,
      "grad_norm": 0.4350614547729492,
      "learning_rate": 2.4172065037589682e-05,
      "loss": 0.4521,
      "step": 46250
    },
    {
      "epoch": 1.0857829630063904,
      "grad_norm": 1.2593437433242798,
      "learning_rate": 2.415586366883426e-05,
      "loss": 0.4872,
      "step": 46300
    },
    {
      "epoch": 1.086955502139884,
      "grad_norm": 0.31871792674064636,
      "learning_rate": 2.4139645258229187e-05,
      "loss": 0.4751,
      "step": 46350
    },
    {
      "epoch": 1.0881280412733776,
      "grad_norm": 0.41270843148231506,
      "learning_rate": 2.4123409835961862e-05,
      "loss": 0.4384,
      "step": 46400
    },
    {
      "epoch": 1.0893005804068712,
      "grad_norm": 0.32838577032089233,
      "learning_rate": 2.4107157432251353e-05,
      "loss": 0.4811,
      "step": 46450
    },
    {
      "epoch": 1.0904731195403647,
      "grad_norm": 0.306800901889801,
      "learning_rate": 2.4090888077348325e-05,
      "loss": 0.4276,
      "step": 46500
    },
    {
      "epoch": 1.0916456586738583,
      "grad_norm": 0.45394155383110046,
      "learning_rate": 2.4074601801535014e-05,
      "loss": 0.4946,
      "step": 46550
    },
    {
      "epoch": 1.092818197807352,
      "grad_norm": 0.16689619421958923,
      "learning_rate": 2.4058298635125125e-05,
      "loss": 0.4592,
      "step": 46600
    },
    {
      "epoch": 1.0939907369408455,
      "grad_norm": 0.2207547426223755,
      "learning_rate": 2.4041978608463815e-05,
      "loss": 0.4449,
      "step": 46650
    },
    {
      "epoch": 1.095163276074339,
      "grad_norm": 0.270749032497406,
      "learning_rate": 2.4025641751927625e-05,
      "loss": 0.4719,
      "step": 46700
    },
    {
      "epoch": 1.0963358152078326,
      "grad_norm": 0.44429251551628113,
      "learning_rate": 2.400928809592442e-05,
      "loss": 0.4558,
      "step": 46750
    },
    {
      "epoch": 1.0975083543413262,
      "grad_norm": 0.43236932158470154,
      "learning_rate": 2.3992917670893332e-05,
      "loss": 0.4574,
      "step": 46800
    },
    {
      "epoch": 1.0986808934748198,
      "grad_norm": 0.4466077387332916,
      "learning_rate": 2.3976530507304697e-05,
      "loss": 0.4582,
      "step": 46850
    },
    {
      "epoch": 1.0998534326083134,
      "grad_norm": 0.41300567984580994,
      "learning_rate": 2.396012663566003e-05,
      "loss": 0.5115,
      "step": 46900
    },
    {
      "epoch": 1.101025971741807,
      "grad_norm": 0.2823593318462372,
      "learning_rate": 2.394370608649192e-05,
      "loss": 0.4554,
      "step": 46950
    },
    {
      "epoch": 1.1021985108753005,
      "grad_norm": 0.29720601439476013,
      "learning_rate": 2.392726889036401e-05,
      "loss": 0.4766,
      "step": 47000
    },
    {
      "epoch": 1.103371050008794,
      "grad_norm": 0.40964752435684204,
      "learning_rate": 2.391081507787093e-05,
      "loss": 0.4301,
      "step": 47050
    },
    {
      "epoch": 1.1045435891422877,
      "grad_norm": 0.48124656081199646,
      "learning_rate": 2.3894344679638223e-05,
      "loss": 0.4374,
      "step": 47100
    },
    {
      "epoch": 1.1057161282757813,
      "grad_norm": 0.3865106701850891,
      "learning_rate": 2.3877857726322326e-05,
      "loss": 0.452,
      "step": 47150
    },
    {
      "epoch": 1.1068886674092748,
      "grad_norm": 0.41712522506713867,
      "learning_rate": 2.3861354248610478e-05,
      "loss": 0.4698,
      "step": 47200
    },
    {
      "epoch": 1.1080612065427684,
      "grad_norm": 0.28996798396110535,
      "learning_rate": 2.3844834277220665e-05,
      "loss": 0.4542,
      "step": 47250
    },
    {
      "epoch": 1.109233745676262,
      "grad_norm": 0.2938878536224365,
      "learning_rate": 2.382829784290159e-05,
      "loss": 0.4518,
      "step": 47300
    },
    {
      "epoch": 1.1104062848097556,
      "grad_norm": 0.2479720264673233,
      "learning_rate": 2.3811744976432593e-05,
      "loss": 0.4346,
      "step": 47350
    },
    {
      "epoch": 1.1115788239432491,
      "grad_norm": 0.20343294739723206,
      "learning_rate": 2.3795175708623597e-05,
      "loss": 0.422,
      "step": 47400
    },
    {
      "epoch": 1.1127513630767427,
      "grad_norm": 0.36933010816574097,
      "learning_rate": 2.377859007031505e-05,
      "loss": 0.4142,
      "step": 47450
    },
    {
      "epoch": 1.1139239022102363,
      "grad_norm": 0.4450768232345581,
      "learning_rate": 2.376198809237788e-05,
      "loss": 0.4575,
      "step": 47500
    },
    {
      "epoch": 1.1150964413437299,
      "grad_norm": 0.3683162033557892,
      "learning_rate": 2.3745369805713416e-05,
      "loss": 0.4409,
      "step": 47550
    },
    {
      "epoch": 1.1162689804772234,
      "grad_norm": 0.4539066255092621,
      "learning_rate": 2.372873524125335e-05,
      "loss": 0.442,
      "step": 47600
    },
    {
      "epoch": 1.117441519610717,
      "grad_norm": 0.32944998145103455,
      "learning_rate": 2.3712084429959676e-05,
      "loss": 0.4622,
      "step": 47650
    },
    {
      "epoch": 1.1186140587442106,
      "grad_norm": 0.20273783802986145,
      "learning_rate": 2.3695417402824617e-05,
      "loss": 0.4469,
      "step": 47700
    },
    {
      "epoch": 1.1197865978777042,
      "grad_norm": 0.23472408950328827,
      "learning_rate": 2.3678734190870584e-05,
      "loss": 0.459,
      "step": 47750
    },
    {
      "epoch": 1.1209591370111978,
      "grad_norm": 0.23077310621738434,
      "learning_rate": 2.366203482515012e-05,
      "loss": 0.424,
      "step": 47800
    },
    {
      "epoch": 1.1221316761446913,
      "grad_norm": 0.3677551746368408,
      "learning_rate": 2.364531933674582e-05,
      "loss": 0.4681,
      "step": 47850
    },
    {
      "epoch": 1.123304215278185,
      "grad_norm": 0.23500820994377136,
      "learning_rate": 2.3628587756770305e-05,
      "loss": 0.4641,
      "step": 47900
    },
    {
      "epoch": 1.1244767544116785,
      "grad_norm": 0.32117992639541626,
      "learning_rate": 2.3611840116366128e-05,
      "loss": 0.4193,
      "step": 47950
    },
    {
      "epoch": 1.125649293545172,
      "grad_norm": 0.5144046545028687,
      "learning_rate": 2.359507644670576e-05,
      "loss": 0.4447,
      "step": 48000
    },
    {
      "epoch": 1.125649293545172,
      "eval_loss": 0.7725650668144226,
      "eval_runtime": 118.1306,
      "eval_samples_per_second": 84.652,
      "eval_steps_per_second": 21.163,
      "step": 48000
    },
    {
      "epoch": 1.1268218326786656,
      "grad_norm": 0.37502166628837585,
      "learning_rate": 2.3578296778991485e-05,
      "loss": 0.4705,
      "step": 48050
    },
    {
      "epoch": 1.1279943718121592,
      "grad_norm": 0.3634868264198303,
      "learning_rate": 2.3561501144455375e-05,
      "loss": 0.4671,
      "step": 48100
    },
    {
      "epoch": 1.1291669109456528,
      "grad_norm": 0.37873968482017517,
      "learning_rate": 2.354468957435922e-05,
      "loss": 0.4941,
      "step": 48150
    },
    {
      "epoch": 1.1303394500791464,
      "grad_norm": 0.3586740791797638,
      "learning_rate": 2.3527862099994467e-05,
      "loss": 0.4769,
      "step": 48200
    },
    {
      "epoch": 1.13151198921264,
      "grad_norm": 0.2798785865306854,
      "learning_rate": 2.351101875268217e-05,
      "loss": 0.4827,
      "step": 48250
    },
    {
      "epoch": 1.1326845283461335,
      "grad_norm": 0.298039048910141,
      "learning_rate": 2.3494159563772926e-05,
      "loss": 0.4597,
      "step": 48300
    },
    {
      "epoch": 1.1338570674796271,
      "grad_norm": 0.2873990535736084,
      "learning_rate": 2.347728456464682e-05,
      "loss": 0.4488,
      "step": 48350
    },
    {
      "epoch": 1.1350296066131207,
      "grad_norm": 0.23556014895439148,
      "learning_rate": 2.3460393786713364e-05,
      "loss": 0.471,
      "step": 48400
    },
    {
      "epoch": 1.1362021457466143,
      "grad_norm": 0.306082159280777,
      "learning_rate": 2.344348726141143e-05,
      "loss": 0.448,
      "step": 48450
    },
    {
      "epoch": 1.1373746848801078,
      "grad_norm": 0.22020885348320007,
      "learning_rate": 2.3426565020209218e-05,
      "loss": 0.4331,
      "step": 48500
    },
    {
      "epoch": 1.1385472240136014,
      "grad_norm": 0.3127880096435547,
      "learning_rate": 2.3409627094604165e-05,
      "loss": 0.4514,
      "step": 48550
    },
    {
      "epoch": 1.139719763147095,
      "grad_norm": 0.21102650463581085,
      "learning_rate": 2.3392673516122908e-05,
      "loss": 0.4281,
      "step": 48600
    },
    {
      "epoch": 1.1408923022805886,
      "grad_norm": 0.310577929019928,
      "learning_rate": 2.3375704316321223e-05,
      "loss": 0.4622,
      "step": 48650
    },
    {
      "epoch": 1.1420648414140822,
      "grad_norm": 0.49451446533203125,
      "learning_rate": 2.3358719526783948e-05,
      "loss": 0.4253,
      "step": 48700
    },
    {
      "epoch": 1.1432373805475757,
      "grad_norm": 0.28266069293022156,
      "learning_rate": 2.3341719179124947e-05,
      "loss": 0.4504,
      "step": 48750
    },
    {
      "epoch": 1.1444099196810693,
      "grad_norm": 0.45955851674079895,
      "learning_rate": 2.3324703304987052e-05,
      "loss": 0.4549,
      "step": 48800
    },
    {
      "epoch": 1.1455824588145629,
      "grad_norm": 0.2471199482679367,
      "learning_rate": 2.330767193604198e-05,
      "loss": 0.4835,
      "step": 48850
    },
    {
      "epoch": 1.1467549979480565,
      "grad_norm": 0.4007188379764557,
      "learning_rate": 2.3290625103990288e-05,
      "loss": 0.456,
      "step": 48900
    },
    {
      "epoch": 1.14792753708155,
      "grad_norm": 0.3251952528953552,
      "learning_rate": 2.3273562840561327e-05,
      "loss": 0.4529,
      "step": 48950
    },
    {
      "epoch": 1.1491000762150436,
      "grad_norm": 0.18600904941558838,
      "learning_rate": 2.325648517751316e-05,
      "loss": 0.4622,
      "step": 49000
    },
    {
      "epoch": 1.1502726153485372,
      "grad_norm": 0.2568994164466858,
      "learning_rate": 2.3239392146632524e-05,
      "loss": 0.4715,
      "step": 49050
    },
    {
      "epoch": 1.1514451544820308,
      "grad_norm": 0.2705742120742798,
      "learning_rate": 2.3222283779734746e-05,
      "loss": 0.4337,
      "step": 49100
    },
    {
      "epoch": 1.1526176936155244,
      "grad_norm": 0.37801265716552734,
      "learning_rate": 2.320516010866371e-05,
      "loss": 0.4593,
      "step": 49150
    },
    {
      "epoch": 1.153790232749018,
      "grad_norm": 0.2841789722442627,
      "learning_rate": 2.318802116529177e-05,
      "loss": 0.4602,
      "step": 49200
    },
    {
      "epoch": 1.1549627718825115,
      "grad_norm": 0.40722548961639404,
      "learning_rate": 2.3170866981519735e-05,
      "loss": 0.475,
      "step": 49250
    },
    {
      "epoch": 1.156135311016005,
      "grad_norm": 0.3378569483757019,
      "learning_rate": 2.315369758927675e-05,
      "loss": 0.4381,
      "step": 49300
    },
    {
      "epoch": 1.1573078501494987,
      "grad_norm": 0.29531747102737427,
      "learning_rate": 2.313651302052028e-05,
      "loss": 0.4539,
      "step": 49350
    },
    {
      "epoch": 1.1584803892829922,
      "grad_norm": 0.3507581949234009,
      "learning_rate": 2.3119313307236048e-05,
      "loss": 0.429,
      "step": 49400
    },
    {
      "epoch": 1.1596529284164858,
      "grad_norm": 0.4541276693344116,
      "learning_rate": 2.3102098481437953e-05,
      "loss": 0.4804,
      "step": 49450
    },
    {
      "epoch": 1.1608254675499794,
      "grad_norm": 0.19611315429210663,
      "learning_rate": 2.3084868575168018e-05,
      "loss": 0.4625,
      "step": 49500
    },
    {
      "epoch": 1.161998006683473,
      "grad_norm": 0.331237256526947,
      "learning_rate": 2.306762362049636e-05,
      "loss": 0.465,
      "step": 49550
    },
    {
      "epoch": 1.1631705458169666,
      "grad_norm": 0.27348363399505615,
      "learning_rate": 2.3050363649521084e-05,
      "loss": 0.424,
      "step": 49600
    },
    {
      "epoch": 1.1643430849504601,
      "grad_norm": 0.24563004076480865,
      "learning_rate": 2.303308869436824e-05,
      "loss": 0.4323,
      "step": 49650
    },
    {
      "epoch": 1.1655156240839537,
      "grad_norm": 0.34086552262306213,
      "learning_rate": 2.301579878719179e-05,
      "loss": 0.4579,
      "step": 49700
    },
    {
      "epoch": 1.1666881632174473,
      "grad_norm": 0.29873138666152954,
      "learning_rate": 2.299849396017351e-05,
      "loss": 0.4539,
      "step": 49750
    },
    {
      "epoch": 1.167860702350941,
      "grad_norm": 0.20518545806407928,
      "learning_rate": 2.298117424552295e-05,
      "loss": 0.4707,
      "step": 49800
    },
    {
      "epoch": 1.1690332414844344,
      "grad_norm": 0.30062803626060486,
      "learning_rate": 2.296383967547738e-05,
      "loss": 0.4379,
      "step": 49850
    },
    {
      "epoch": 1.1702057806179282,
      "grad_norm": 0.43329334259033203,
      "learning_rate": 2.2946490282301706e-05,
      "loss": 0.4558,
      "step": 49900
    },
    {
      "epoch": 1.1713783197514216,
      "grad_norm": 0.5191198587417603,
      "learning_rate": 2.2929126098288427e-05,
      "loss": 0.4495,
      "step": 49950
    },
    {
      "epoch": 1.1725508588849154,
      "grad_norm": 0.3019881546497345,
      "learning_rate": 2.2911747155757583e-05,
      "loss": 0.4207,
      "step": 50000
    },
    {
      "epoch": 1.1725508588849154,
      "eval_loss": 0.7683523297309875,
      "eval_runtime": 117.9869,
      "eval_samples_per_second": 84.755,
      "eval_steps_per_second": 21.189,
      "step": 50000
    },
    {
      "epoch": 1.1737233980184087,
      "grad_norm": 0.17777803540229797,
      "learning_rate": 2.289435348705667e-05,
      "loss": 0.4506,
      "step": 50050
    },
    {
      "epoch": 1.1748959371519025,
      "grad_norm": 0.2585689127445221,
      "learning_rate": 2.2876945124560607e-05,
      "loss": 0.4337,
      "step": 50100
    },
    {
      "epoch": 1.176068476285396,
      "grad_norm": 0.34679514169692993,
      "learning_rate": 2.2859522100671655e-05,
      "loss": 0.4344,
      "step": 50150
    },
    {
      "epoch": 1.1772410154188897,
      "grad_norm": 0.4865015745162964,
      "learning_rate": 2.284208444781936e-05,
      "loss": 0.45,
      "step": 50200
    },
    {
      "epoch": 1.1784135545523833,
      "grad_norm": 0.331938236951828,
      "learning_rate": 2.2824632198460506e-05,
      "loss": 0.4572,
      "step": 50250
    },
    {
      "epoch": 1.1795860936858769,
      "grad_norm": 0.18781299889087677,
      "learning_rate": 2.2807165385079045e-05,
      "loss": 0.4642,
      "step": 50300
    },
    {
      "epoch": 1.1807586328193704,
      "grad_norm": 0.2565305829048157,
      "learning_rate": 2.2789684040186037e-05,
      "loss": 0.4635,
      "step": 50350
    },
    {
      "epoch": 1.181931171952864,
      "grad_norm": 0.31993675231933594,
      "learning_rate": 2.2772188196319573e-05,
      "loss": 0.4381,
      "step": 50400
    },
    {
      "epoch": 1.1831037110863576,
      "grad_norm": 0.2976383864879608,
      "learning_rate": 2.275467788604476e-05,
      "loss": 0.4413,
      "step": 50450
    },
    {
      "epoch": 1.1842762502198512,
      "grad_norm": 0.3821752369403839,
      "learning_rate": 2.2737153141953606e-05,
      "loss": 0.4925,
      "step": 50500
    },
    {
      "epoch": 1.1854487893533447,
      "grad_norm": 0.3331695795059204,
      "learning_rate": 2.2719613996664998e-05,
      "loss": 0.4183,
      "step": 50550
    },
    {
      "epoch": 1.1866213284868383,
      "grad_norm": 0.35970109701156616,
      "learning_rate": 2.270206048282463e-05,
      "loss": 0.4227,
      "step": 50600
    },
    {
      "epoch": 1.187793867620332,
      "grad_norm": 0.43131938576698303,
      "learning_rate": 2.2684492633104925e-05,
      "loss": 0.482,
      "step": 50650
    },
    {
      "epoch": 1.1889664067538255,
      "grad_norm": 0.31376561522483826,
      "learning_rate": 2.266691048020501e-05,
      "loss": 0.4546,
      "step": 50700
    },
    {
      "epoch": 1.190138945887319,
      "grad_norm": 0.43175792694091797,
      "learning_rate": 2.2649314056850617e-05,
      "loss": 0.4732,
      "step": 50750
    },
    {
      "epoch": 1.1913114850208126,
      "grad_norm": 0.18149971961975098,
      "learning_rate": 2.263170339579405e-05,
      "loss": 0.4553,
      "step": 50800
    },
    {
      "epoch": 1.1924840241543062,
      "grad_norm": 0.2895059883594513,
      "learning_rate": 2.2614078529814105e-05,
      "loss": 0.4657,
      "step": 50850
    },
    {
      "epoch": 1.1936565632877998,
      "grad_norm": 0.5071765780448914,
      "learning_rate": 2.2596439491716032e-05,
      "loss": 0.4621,
      "step": 50900
    },
    {
      "epoch": 1.1948291024212934,
      "grad_norm": 0.3349756598472595,
      "learning_rate": 2.2578786314331446e-05,
      "loss": 0.5175,
      "step": 50950
    },
    {
      "epoch": 1.196001641554787,
      "grad_norm": 0.4727456867694855,
      "learning_rate": 2.256111903051829e-05,
      "loss": 0.4368,
      "step": 51000
    },
    {
      "epoch": 1.1971741806882805,
      "grad_norm": 0.3268149793148041,
      "learning_rate": 2.2543437673160742e-05,
      "loss": 0.4656,
      "step": 51050
    },
    {
      "epoch": 1.198346719821774,
      "grad_norm": 0.29385435581207275,
      "learning_rate": 2.2525742275169212e-05,
      "loss": 0.464,
      "step": 51100
    },
    {
      "epoch": 1.1995192589552677,
      "grad_norm": 0.30820924043655396,
      "learning_rate": 2.2508032869480204e-05,
      "loss": 0.4431,
      "step": 51150
    },
    {
      "epoch": 1.2006917980887613,
      "grad_norm": 0.2650797367095947,
      "learning_rate": 2.2490309489056327e-05,
      "loss": 0.4614,
      "step": 51200
    },
    {
      "epoch": 1.2018643372222548,
      "grad_norm": 0.355979859828949,
      "learning_rate": 2.247257216688618e-05,
      "loss": 0.4892,
      "step": 51250
    },
    {
      "epoch": 1.2030368763557484,
      "grad_norm": 0.4333697557449341,
      "learning_rate": 2.2454820935984324e-05,
      "loss": 0.4519,
      "step": 51300
    },
    {
      "epoch": 1.204209415489242,
      "grad_norm": 0.28736796975135803,
      "learning_rate": 2.2437055829391202e-05,
      "loss": 0.4488,
      "step": 51350
    },
    {
      "epoch": 1.2053819546227356,
      "grad_norm": 0.4015028178691864,
      "learning_rate": 2.2419276880173083e-05,
      "loss": 0.4428,
      "step": 51400
    },
    {
      "epoch": 1.2065544937562291,
      "grad_norm": 0.4286210834980011,
      "learning_rate": 2.2401484121422005e-05,
      "loss": 0.4873,
      "step": 51450
    },
    {
      "epoch": 1.2077270328897227,
      "grad_norm": 0.35969704389572144,
      "learning_rate": 2.2383677586255712e-05,
      "loss": 0.4574,
      "step": 51500
    },
    {
      "epoch": 1.2088995720232163,
      "grad_norm": 0.24177034199237823,
      "learning_rate": 2.236585730781758e-05,
      "loss": 0.4849,
      "step": 51550
    },
    {
      "epoch": 1.2100721111567099,
      "grad_norm": 0.3280395567417145,
      "learning_rate": 2.2348023319276587e-05,
      "loss": 0.4454,
      "step": 51600
    },
    {
      "epoch": 1.2112446502902035,
      "grad_norm": 0.2641109526157379,
      "learning_rate": 2.23301756538272e-05,
      "loss": 0.4823,
      "step": 51650
    },
    {
      "epoch": 1.212417189423697,
      "grad_norm": 0.40855178236961365,
      "learning_rate": 2.2312314344689363e-05,
      "loss": 0.4774,
      "step": 51700
    },
    {
      "epoch": 1.2135897285571906,
      "grad_norm": 0.43855220079421997,
      "learning_rate": 2.2294439425108406e-05,
      "loss": 0.4222,
      "step": 51750
    },
    {
      "epoch": 1.2147622676906842,
      "grad_norm": 0.23817755281925201,
      "learning_rate": 2.2276550928355004e-05,
      "loss": 0.4568,
      "step": 51800
    },
    {
      "epoch": 1.2159348068241778,
      "grad_norm": 0.3637879192829132,
      "learning_rate": 2.2258648887725093e-05,
      "loss": 0.4704,
      "step": 51850
    },
    {
      "epoch": 1.2171073459576713,
      "grad_norm": 0.614146888256073,
      "learning_rate": 2.2240733336539816e-05,
      "loss": 0.4812,
      "step": 51900
    },
    {
      "epoch": 1.218279885091165,
      "grad_norm": 0.24049104750156403,
      "learning_rate": 2.222280430814547e-05,
      "loss": 0.4416,
      "step": 51950
    },
    {
      "epoch": 1.2194524242246585,
      "grad_norm": 0.3939765691757202,
      "learning_rate": 2.220486183591344e-05,
      "loss": 0.4385,
      "step": 52000
    },
    {
      "epoch": 1.2194524242246585,
      "eval_loss": 0.7711385488510132,
      "eval_runtime": 118.4777,
      "eval_samples_per_second": 84.404,
      "eval_steps_per_second": 21.101,
      "step": 52000
    },
    {
      "epoch": 1.220624963358152,
      "grad_norm": 0.35365062952041626,
      "learning_rate": 2.2186905953240118e-05,
      "loss": 0.4785,
      "step": 52050
    },
    {
      "epoch": 1.2217975024916456,
      "grad_norm": 0.23053830862045288,
      "learning_rate": 2.216893669354688e-05,
      "loss": 0.4599,
      "step": 52100
    },
    {
      "epoch": 1.2229700416251392,
      "grad_norm": 0.29705899953842163,
      "learning_rate": 2.215095409027998e-05,
      "loss": 0.4523,
      "step": 52150
    },
    {
      "epoch": 1.2241425807586328,
      "grad_norm": 0.2413168102502823,
      "learning_rate": 2.2132958176910527e-05,
      "loss": 0.5135,
      "step": 52200
    },
    {
      "epoch": 1.2253151198921264,
      "grad_norm": 0.478544682264328,
      "learning_rate": 2.2114948986934386e-05,
      "loss": 0.4667,
      "step": 52250
    },
    {
      "epoch": 1.22648765902562,
      "grad_norm": 0.27569907903671265,
      "learning_rate": 2.209692655387214e-05,
      "loss": 0.4733,
      "step": 52300
    },
    {
      "epoch": 1.2276601981591135,
      "grad_norm": 0.2651924192905426,
      "learning_rate": 2.207889091126903e-05,
      "loss": 0.4477,
      "step": 52350
    },
    {
      "epoch": 1.2288327372926071,
      "grad_norm": 0.43510526418685913,
      "learning_rate": 2.2060842092694877e-05,
      "loss": 0.4802,
      "step": 52400
    },
    {
      "epoch": 1.2300052764261007,
      "grad_norm": 0.2335723340511322,
      "learning_rate": 2.2042780131744025e-05,
      "loss": 0.472,
      "step": 52450
    },
    {
      "epoch": 1.2311778155595943,
      "grad_norm": 0.5113958120346069,
      "learning_rate": 2.2024705062035283e-05,
      "loss": 0.4508,
      "step": 52500
    },
    {
      "epoch": 1.2323503546930878,
      "grad_norm": 0.2918396592140198,
      "learning_rate": 2.2006616917211857e-05,
      "loss": 0.4705,
      "step": 52550
    },
    {
      "epoch": 1.2335228938265814,
      "grad_norm": 0.20013535022735596,
      "learning_rate": 2.1988515730941294e-05,
      "loss": 0.4502,
      "step": 52600
    },
    {
      "epoch": 1.234695432960075,
      "grad_norm": 0.3548218607902527,
      "learning_rate": 2.197040153691541e-05,
      "loss": 0.439,
      "step": 52650
    },
    {
      "epoch": 1.2358679720935686,
      "grad_norm": 0.2888626158237457,
      "learning_rate": 2.1952274368850233e-05,
      "loss": 0.4597,
      "step": 52700
    },
    {
      "epoch": 1.2370405112270622,
      "grad_norm": 0.440524160861969,
      "learning_rate": 2.1934134260485945e-05,
      "loss": 0.4483,
      "step": 52750
    },
    {
      "epoch": 1.2382130503605557,
      "grad_norm": 0.3885774612426758,
      "learning_rate": 2.1915981245586808e-05,
      "loss": 0.4757,
      "step": 52800
    },
    {
      "epoch": 1.2393855894940493,
      "grad_norm": 0.2067619115114212,
      "learning_rate": 2.1897815357941114e-05,
      "loss": 0.4423,
      "step": 52850
    },
    {
      "epoch": 1.2405581286275429,
      "grad_norm": 0.4257228970527649,
      "learning_rate": 2.1879636631361105e-05,
      "loss": 0.5039,
      "step": 52900
    },
    {
      "epoch": 1.2417306677610365,
      "grad_norm": 0.3275536596775055,
      "learning_rate": 2.186144509968292e-05,
      "loss": 0.4364,
      "step": 52950
    },
    {
      "epoch": 1.24290320689453,
      "grad_norm": 0.40109848976135254,
      "learning_rate": 2.184324079676655e-05,
      "loss": 0.4245,
      "step": 53000
    },
    {
      "epoch": 1.2440757460280236,
      "grad_norm": 0.18557536602020264,
      "learning_rate": 2.1825023756495735e-05,
      "loss": 0.442,
      "step": 53050
    },
    {
      "epoch": 1.2452482851615172,
      "grad_norm": 0.3354750871658325,
      "learning_rate": 2.180679401277794e-05,
      "loss": 0.4421,
      "step": 53100
    },
    {
      "epoch": 1.2464208242950108,
      "grad_norm": 0.2588070034980774,
      "learning_rate": 2.178855159954426e-05,
      "loss": 0.432,
      "step": 53150
    },
    {
      "epoch": 1.2475933634285044,
      "grad_norm": 0.35677456855773926,
      "learning_rate": 2.1770296550749387e-05,
      "loss": 0.4427,
      "step": 53200
    },
    {
      "epoch": 1.248765902561998,
      "grad_norm": 0.2696671783924103,
      "learning_rate": 2.1752028900371518e-05,
      "loss": 0.4336,
      "step": 53250
    },
    {
      "epoch": 1.2499384416954915,
      "grad_norm": 0.5646373629570007,
      "learning_rate": 2.1733748682412317e-05,
      "loss": 0.461,
      "step": 53300
    },
    {
      "epoch": 1.251110980828985,
      "grad_norm": 0.35651880502700806,
      "learning_rate": 2.1715455930896835e-05,
      "loss": 0.4344,
      "step": 53350
    },
    {
      "epoch": 1.2522835199624787,
      "grad_norm": 0.2850319445133209,
      "learning_rate": 2.1697150679873444e-05,
      "loss": 0.4636,
      "step": 53400
    },
    {
      "epoch": 1.2534560590959725,
      "grad_norm": 0.22086787223815918,
      "learning_rate": 2.1678832963413798e-05,
      "loss": 0.4504,
      "step": 53450
    },
    {
      "epoch": 1.2546285982294658,
      "grad_norm": 0.4825667142868042,
      "learning_rate": 2.166050281561274e-05,
      "loss": 0.4554,
      "step": 53500
    },
    {
      "epoch": 1.2558011373629596,
      "grad_norm": 0.38590413331985474,
      "learning_rate": 2.164216027058825e-05,
      "loss": 0.4493,
      "step": 53550
    },
    {
      "epoch": 1.256973676496453,
      "grad_norm": 0.41539087891578674,
      "learning_rate": 2.1623805362481402e-05,
      "loss": 0.4974,
      "step": 53600
    },
    {
      "epoch": 1.2581462156299468,
      "grad_norm": 0.5250222086906433,
      "learning_rate": 2.1605438125456254e-05,
      "loss": 0.473,
      "step": 53650
    },
    {
      "epoch": 1.2593187547634401,
      "grad_norm": 0.5848110318183899,
      "learning_rate": 2.1587058593699833e-05,
      "loss": 0.4972,
      "step": 53700
    },
    {
      "epoch": 1.260491293896934,
      "grad_norm": 0.4877767264842987,
      "learning_rate": 2.1568666801422042e-05,
      "loss": 0.465,
      "step": 53750
    },
    {
      "epoch": 1.2616638330304273,
      "grad_norm": 0.18663673102855682,
      "learning_rate": 2.1550262782855604e-05,
      "loss": 0.4415,
      "step": 53800
    },
    {
      "epoch": 1.262836372163921,
      "grad_norm": 0.19950062036514282,
      "learning_rate": 2.1531846572256003e-05,
      "loss": 0.4636,
      "step": 53850
    },
    {
      "epoch": 1.2640089112974144,
      "grad_norm": 0.40972641110420227,
      "learning_rate": 2.1513418203901412e-05,
      "loss": 0.4715,
      "step": 53900
    },
    {
      "epoch": 1.2651814504309082,
      "grad_norm": 0.27859848737716675,
      "learning_rate": 2.149497771209263e-05,
      "loss": 0.4776,
      "step": 53950
    },
    {
      "epoch": 1.2663539895644016,
      "grad_norm": 0.4979117810726166,
      "learning_rate": 2.1476525131153035e-05,
      "loss": 0.4699,
      "step": 54000
    },
    {
      "epoch": 1.2663539895644016,
      "eval_loss": 0.7707176804542542,
      "eval_runtime": 117.8766,
      "eval_samples_per_second": 84.835,
      "eval_steps_per_second": 21.209,
      "step": 54000
    },
    {
      "epoch": 1.2675265286978954,
      "grad_norm": 0.36158838868141174,
      "learning_rate": 2.145806049542849e-05,
      "loss": 0.468,
      "step": 54050
    },
    {
      "epoch": 1.2686990678313887,
      "grad_norm": 0.4718315601348877,
      "learning_rate": 2.143958383928731e-05,
      "loss": 0.4614,
      "step": 54100
    },
    {
      "epoch": 1.2698716069648825,
      "grad_norm": 0.2717481851577759,
      "learning_rate": 2.1421095197120173e-05,
      "loss": 0.4875,
      "step": 54150
    },
    {
      "epoch": 1.271044146098376,
      "grad_norm": 0.2631092071533203,
      "learning_rate": 2.140259460334007e-05,
      "loss": 0.4919,
      "step": 54200
    },
    {
      "epoch": 1.2722166852318697,
      "grad_norm": 0.27997222542762756,
      "learning_rate": 2.138408209238224e-05,
      "loss": 0.4565,
      "step": 54250
    },
    {
      "epoch": 1.273389224365363,
      "grad_norm": 0.4643717408180237,
      "learning_rate": 2.13655576987041e-05,
      "loss": 0.471,
      "step": 54300
    },
    {
      "epoch": 1.2745617634988569,
      "grad_norm": 0.35371261835098267,
      "learning_rate": 2.1347021456785185e-05,
      "loss": 0.445,
      "step": 54350
    },
    {
      "epoch": 1.2757343026323504,
      "grad_norm": 0.40822532773017883,
      "learning_rate": 2.1328473401127093e-05,
      "loss": 0.4593,
      "step": 54400
    },
    {
      "epoch": 1.276906841765844,
      "grad_norm": 0.22974298894405365,
      "learning_rate": 2.130991356625339e-05,
      "loss": 0.4485,
      "step": 54450
    },
    {
      "epoch": 1.2780793808993376,
      "grad_norm": 0.17977838218212128,
      "learning_rate": 2.1291341986709588e-05,
      "loss": 0.4653,
      "step": 54500
    },
    {
      "epoch": 1.2792519200328312,
      "grad_norm": 0.4213763475418091,
      "learning_rate": 2.1272758697063046e-05,
      "loss": 0.4833,
      "step": 54550
    },
    {
      "epoch": 1.2804244591663247,
      "grad_norm": 0.4109313488006592,
      "learning_rate": 2.1254163731902927e-05,
      "loss": 0.4582,
      "step": 54600
    },
    {
      "epoch": 1.2815969982998183,
      "grad_norm": 0.3764592707157135,
      "learning_rate": 2.1235557125840117e-05,
      "loss": 0.4081,
      "step": 54650
    },
    {
      "epoch": 1.282769537433312,
      "grad_norm": 0.35054337978363037,
      "learning_rate": 2.121693891350718e-05,
      "loss": 0.4655,
      "step": 54700
    },
    {
      "epoch": 1.2839420765668055,
      "grad_norm": 0.29655468463897705,
      "learning_rate": 2.119830912955827e-05,
      "loss": 0.4587,
      "step": 54750
    },
    {
      "epoch": 1.285114615700299,
      "grad_norm": 0.26540055871009827,
      "learning_rate": 2.1179667808669097e-05,
      "loss": 0.4441,
      "step": 54800
    },
    {
      "epoch": 1.2862871548337926,
      "grad_norm": 0.3217494785785675,
      "learning_rate": 2.116101498553682e-05,
      "loss": 0.4573,
      "step": 54850
    },
    {
      "epoch": 1.2874596939672862,
      "grad_norm": 0.3693917989730835,
      "learning_rate": 2.1142350694880035e-05,
      "loss": 0.454,
      "step": 54900
    },
    {
      "epoch": 1.2886322331007798,
      "grad_norm": 0.4209136962890625,
      "learning_rate": 2.1123674971438663e-05,
      "loss": 0.4535,
      "step": 54950
    },
    {
      "epoch": 1.2898047722342734,
      "grad_norm": 0.19571329653263092,
      "learning_rate": 2.110498784997391e-05,
      "loss": 0.4714,
      "step": 55000
    },
    {
      "epoch": 1.290977311367767,
      "grad_norm": 0.2782137393951416,
      "learning_rate": 2.10862893652682e-05,
      "loss": 0.4268,
      "step": 55050
    },
    {
      "epoch": 1.2921498505012605,
      "grad_norm": 0.16619457304477692,
      "learning_rate": 2.1067579552125105e-05,
      "loss": 0.4839,
      "step": 55100
    },
    {
      "epoch": 1.293322389634754,
      "grad_norm": 0.3144513964653015,
      "learning_rate": 2.1048858445369283e-05,
      "loss": 0.4239,
      "step": 55150
    },
    {
      "epoch": 1.2944949287682477,
      "grad_norm": 0.4966543912887573,
      "learning_rate": 2.1030126079846413e-05,
      "loss": 0.4679,
      "step": 55200
    },
    {
      "epoch": 1.2956674679017413,
      "grad_norm": 0.2243233174085617,
      "learning_rate": 2.101138249042313e-05,
      "loss": 0.4257,
      "step": 55250
    },
    {
      "epoch": 1.2968400070352348,
      "grad_norm": 0.2908501625061035,
      "learning_rate": 2.0992627711986958e-05,
      "loss": 0.4319,
      "step": 55300
    },
    {
      "epoch": 1.2980125461687284,
      "grad_norm": 0.2961321473121643,
      "learning_rate": 2.0973861779446253e-05,
      "loss": 0.5,
      "step": 55350
    },
    {
      "epoch": 1.299185085302222,
      "grad_norm": 0.32208988070487976,
      "learning_rate": 2.095508472773013e-05,
      "loss": 0.4764,
      "step": 55400
    },
    {
      "epoch": 1.3003576244357156,
      "grad_norm": 0.30222558975219727,
      "learning_rate": 2.0936296591788392e-05,
      "loss": 0.4675,
      "step": 55450
    },
    {
      "epoch": 1.3015301635692091,
      "grad_norm": 0.18573398888111115,
      "learning_rate": 2.0917497406591484e-05,
      "loss": 0.4431,
      "step": 55500
    },
    {
      "epoch": 1.3027027027027027,
      "grad_norm": 0.42144834995269775,
      "learning_rate": 2.0898687207130413e-05,
      "loss": 0.4292,
      "step": 55550
    },
    {
      "epoch": 1.3038752418361963,
      "grad_norm": 0.43371257185935974,
      "learning_rate": 2.087986602841668e-05,
      "loss": 0.4912,
      "step": 55600
    },
    {
      "epoch": 1.3050477809696899,
      "grad_norm": 0.348127156496048,
      "learning_rate": 2.0861033905482238e-05,
      "loss": 0.4527,
      "step": 55650
    },
    {
      "epoch": 1.3062203201031835,
      "grad_norm": 0.4797091484069824,
      "learning_rate": 2.08421908733794e-05,
      "loss": 0.4866,
      "step": 55700
    },
    {
      "epoch": 1.307392859236677,
      "grad_norm": 0.3592279553413391,
      "learning_rate": 2.082333696718078e-05,
      "loss": 0.4408,
      "step": 55750
    },
    {
      "epoch": 1.3085653983701706,
      "grad_norm": 0.42859041690826416,
      "learning_rate": 2.080447222197924e-05,
      "loss": 0.4628,
      "step": 55800
    },
    {
      "epoch": 1.3097379375036642,
      "grad_norm": 0.8626359105110168,
      "learning_rate": 2.078559667288781e-05,
      "loss": 0.4479,
      "step": 55850
    },
    {
      "epoch": 1.3109104766371578,
      "grad_norm": 0.24907323718070984,
      "learning_rate": 2.0766710355039643e-05,
      "loss": 0.4502,
      "step": 55900
    },
    {
      "epoch": 1.3120830157706513,
      "grad_norm": 0.3767203688621521,
      "learning_rate": 2.0747813303587915e-05,
      "loss": 0.4425,
      "step": 55950
    },
    {
      "epoch": 1.313255554904145,
      "grad_norm": 0.28276681900024414,
      "learning_rate": 2.0728905553705798e-05,
      "loss": 0.4439,
      "step": 56000
    },
    {
      "epoch": 1.313255554904145,
      "eval_loss": 0.7703226208686829,
      "eval_runtime": 117.7463,
      "eval_samples_per_second": 84.928,
      "eval_steps_per_second": 21.232,
      "step": 56000
    },
    {
      "epoch": 1.3144280940376385,
      "grad_norm": 0.2971753180027008,
      "learning_rate": 2.0709987140586366e-05,
      "loss": 0.4557,
      "step": 56050
    },
    {
      "epoch": 1.315600633171132,
      "grad_norm": 0.37821128964424133,
      "learning_rate": 2.0691058099442553e-05,
      "loss": 0.4535,
      "step": 56100
    },
    {
      "epoch": 1.3167731723046256,
      "grad_norm": 0.5251179337501526,
      "learning_rate": 2.067211846550706e-05,
      "loss": 0.4493,
      "step": 56150
    },
    {
      "epoch": 1.3179457114381192,
      "grad_norm": 0.2834978997707367,
      "learning_rate": 2.0653168274032316e-05,
      "loss": 0.4439,
      "step": 56200
    },
    {
      "epoch": 1.3191182505716128,
      "grad_norm": 0.38054800033569336,
      "learning_rate": 2.0634207560290396e-05,
      "loss": 0.4557,
      "step": 56250
    },
    {
      "epoch": 1.3202907897051064,
      "grad_norm": 0.26266106963157654,
      "learning_rate": 2.061523635957295e-05,
      "loss": 0.4381,
      "step": 56300
    },
    {
      "epoch": 1.3214633288386,
      "grad_norm": 0.29824408888816833,
      "learning_rate": 2.0596254707191174e-05,
      "loss": 0.4853,
      "step": 56350
    },
    {
      "epoch": 1.3226358679720935,
      "grad_norm": 0.28218576312065125,
      "learning_rate": 2.0577262638475695e-05,
      "loss": 0.528,
      "step": 56400
    },
    {
      "epoch": 1.3238084071055871,
      "grad_norm": 0.42234835028648376,
      "learning_rate": 2.0558260188776535e-05,
      "loss": 0.4873,
      "step": 56450
    },
    {
      "epoch": 1.3249809462390807,
      "grad_norm": 0.35756126046180725,
      "learning_rate": 2.053924739346304e-05,
      "loss": 0.4426,
      "step": 56500
    },
    {
      "epoch": 1.3261534853725743,
      "grad_norm": 0.6088123917579651,
      "learning_rate": 2.05202242879238e-05,
      "loss": 0.4684,
      "step": 56550
    },
    {
      "epoch": 1.3273260245060678,
      "grad_norm": 0.21861088275909424,
      "learning_rate": 2.050119090756662e-05,
      "loss": 0.4619,
      "step": 56600
    },
    {
      "epoch": 1.3284985636395614,
      "grad_norm": 0.31597700715065,
      "learning_rate": 2.0482147287818404e-05,
      "loss": 0.508,
      "step": 56650
    },
    {
      "epoch": 1.329671102773055,
      "grad_norm": 0.24921172857284546,
      "learning_rate": 2.046309346412514e-05,
      "loss": 0.4494,
      "step": 56700
    },
    {
      "epoch": 1.3308436419065486,
      "grad_norm": 0.3008979856967926,
      "learning_rate": 2.044402947195177e-05,
      "loss": 0.4162,
      "step": 56750
    },
    {
      "epoch": 1.3320161810400422,
      "grad_norm": 0.27303141355514526,
      "learning_rate": 2.0424955346782216e-05,
      "loss": 0.4217,
      "step": 56800
    },
    {
      "epoch": 1.3331887201735357,
      "grad_norm": 0.6649940013885498,
      "learning_rate": 2.0405871124119212e-05,
      "loss": 0.5106,
      "step": 56850
    },
    {
      "epoch": 1.3343612593070293,
      "grad_norm": 0.23379500210285187,
      "learning_rate": 2.038677683948431e-05,
      "loss": 0.454,
      "step": 56900
    },
    {
      "epoch": 1.335533798440523,
      "grad_norm": 0.3506065905094147,
      "learning_rate": 2.0367672528417792e-05,
      "loss": 0.4575,
      "step": 56950
    },
    {
      "epoch": 1.3367063375740165,
      "grad_norm": 0.22733564674854279,
      "learning_rate": 2.0348558226478594e-05,
      "loss": 0.4934,
      "step": 57000
    },
    {
      "epoch": 1.33787887670751,
      "grad_norm": 0.3225797116756439,
      "learning_rate": 2.0329433969244253e-05,
      "loss": 0.4927,
      "step": 57050
    },
    {
      "epoch": 1.3390514158410036,
      "grad_norm": 0.3556496798992157,
      "learning_rate": 2.0310299792310833e-05,
      "loss": 0.4305,
      "step": 57100
    },
    {
      "epoch": 1.3402239549744972,
      "grad_norm": 0.5552076101303101,
      "learning_rate": 2.0291155731292858e-05,
      "loss": 0.4256,
      "step": 57150
    },
    {
      "epoch": 1.3413964941079908,
      "grad_norm": 0.16509811580181122,
      "learning_rate": 2.0272001821823256e-05,
      "loss": 0.4589,
      "step": 57200
    },
    {
      "epoch": 1.3425690332414844,
      "grad_norm": 0.21571803092956543,
      "learning_rate": 2.0252838099553284e-05,
      "loss": 0.4523,
      "step": 57250
    },
    {
      "epoch": 1.343741572374978,
      "grad_norm": 0.3357020914554596,
      "learning_rate": 2.0233664600152466e-05,
      "loss": 0.4609,
      "step": 57300
    },
    {
      "epoch": 1.3449141115084715,
      "grad_norm": 0.2910429537296295,
      "learning_rate": 2.021448135930852e-05,
      "loss": 0.477,
      "step": 57350
    },
    {
      "epoch": 1.3460866506419653,
      "grad_norm": 0.35633793473243713,
      "learning_rate": 2.0195288412727293e-05,
      "loss": 0.4549,
      "step": 57400
    },
    {
      "epoch": 1.3472591897754587,
      "grad_norm": 0.2252482771873474,
      "learning_rate": 2.0176085796132705e-05,
      "loss": 0.4359,
      "step": 57450
    },
    {
      "epoch": 1.3484317289089525,
      "grad_norm": 0.21143990755081177,
      "learning_rate": 2.0156873545266675e-05,
      "loss": 0.4842,
      "step": 57500
    },
    {
      "epoch": 1.3496042680424458,
      "grad_norm": 0.4933795928955078,
      "learning_rate": 2.013765169588904e-05,
      "loss": 0.4556,
      "step": 57550
    },
    {
      "epoch": 1.3507768071759396,
      "grad_norm": 0.2839409112930298,
      "learning_rate": 2.0118420283777527e-05,
      "loss": 0.4474,
      "step": 57600
    },
    {
      "epoch": 1.351949346309433,
      "grad_norm": 0.35473698377609253,
      "learning_rate": 2.0099179344727642e-05,
      "loss": 0.458,
      "step": 57650
    },
    {
      "epoch": 1.3531218854429268,
      "grad_norm": 0.34104445576667786,
      "learning_rate": 2.0079928914552624e-05,
      "loss": 0.4733,
      "step": 57700
    },
    {
      "epoch": 1.3542944245764201,
      "grad_norm": 0.3010089099407196,
      "learning_rate": 2.0060669029083396e-05,
      "loss": 0.466,
      "step": 57750
    },
    {
      "epoch": 1.355466963709914,
      "grad_norm": 0.42210230231285095,
      "learning_rate": 2.004139972416846e-05,
      "loss": 0.4497,
      "step": 57800
    },
    {
      "epoch": 1.3566395028434073,
      "grad_norm": 0.24049851298332214,
      "learning_rate": 2.0022121035673867e-05,
      "loss": 0.4325,
      "step": 57850
    },
    {
      "epoch": 1.357812041976901,
      "grad_norm": 0.43596673011779785,
      "learning_rate": 2.0002832999483115e-05,
      "loss": 0.4704,
      "step": 57900
    },
    {
      "epoch": 1.3589845811103944,
      "grad_norm": 0.5173012614250183,
      "learning_rate": 1.9983535651497114e-05,
      "loss": 0.4756,
      "step": 57950
    },
    {
      "epoch": 1.3601571202438882,
      "grad_norm": 0.21083834767341614,
      "learning_rate": 1.996422902763411e-05,
      "loss": 0.44,
      "step": 58000
    },
    {
      "epoch": 1.3601571202438882,
      "eval_loss": 0.7691562175750732,
      "eval_runtime": 118.1629,
      "eval_samples_per_second": 84.629,
      "eval_steps_per_second": 21.157,
      "step": 58000
    },
    {
      "epoch": 1.3613296593773816,
      "grad_norm": 0.33509260416030884,
      "learning_rate": 1.99449131638296e-05,
      "loss": 0.4719,
      "step": 58050
    },
    {
      "epoch": 1.3625021985108754,
      "grad_norm": 0.29538729786872864,
      "learning_rate": 1.9925588096036292e-05,
      "loss": 0.4452,
      "step": 58100
    },
    {
      "epoch": 1.3636747376443688,
      "grad_norm": 0.4199083149433136,
      "learning_rate": 1.9906253860224023e-05,
      "loss": 0.4221,
      "step": 58150
    },
    {
      "epoch": 1.3648472767778626,
      "grad_norm": 0.35770556330680847,
      "learning_rate": 1.9886910492379678e-05,
      "loss": 0.4399,
      "step": 58200
    },
    {
      "epoch": 1.366019815911356,
      "grad_norm": 0.3312048316001892,
      "learning_rate": 1.9867558028507166e-05,
      "loss": 0.4857,
      "step": 58250
    },
    {
      "epoch": 1.3671923550448497,
      "grad_norm": 0.25374865531921387,
      "learning_rate": 1.9848196504627312e-05,
      "loss": 0.4455,
      "step": 58300
    },
    {
      "epoch": 1.3683648941783433,
      "grad_norm": 0.24966947734355927,
      "learning_rate": 1.9828825956777803e-05,
      "loss": 0.4777,
      "step": 58350
    },
    {
      "epoch": 1.3695374333118369,
      "grad_norm": 0.3078325390815735,
      "learning_rate": 1.9809446421013132e-05,
      "loss": 0.4373,
      "step": 58400
    },
    {
      "epoch": 1.3707099724453304,
      "grad_norm": 0.4307745099067688,
      "learning_rate": 1.9790057933404505e-05,
      "loss": 0.4315,
      "step": 58450
    },
    {
      "epoch": 1.371882511578824,
      "grad_norm": 0.2642225921154022,
      "learning_rate": 1.9770660530039806e-05,
      "loss": 0.4594,
      "step": 58500
    },
    {
      "epoch": 1.3730550507123176,
      "grad_norm": 0.509814441204071,
      "learning_rate": 1.9751254247023508e-05,
      "loss": 0.4819,
      "step": 58550
    },
    {
      "epoch": 1.3742275898458112,
      "grad_norm": 0.22243225574493408,
      "learning_rate": 1.973183912047661e-05,
      "loss": 0.4469,
      "step": 58600
    },
    {
      "epoch": 1.3754001289793047,
      "grad_norm": 0.24768346548080444,
      "learning_rate": 1.9712415186536573e-05,
      "loss": 0.4595,
      "step": 58650
    },
    {
      "epoch": 1.3765726681127983,
      "grad_norm": 0.28895846009254456,
      "learning_rate": 1.9692982481357247e-05,
      "loss": 0.4566,
      "step": 58700
    },
    {
      "epoch": 1.377745207246292,
      "grad_norm": 0.3448325991630554,
      "learning_rate": 1.9673541041108822e-05,
      "loss": 0.456,
      "step": 58750
    },
    {
      "epoch": 1.3789177463797855,
      "grad_norm": 0.3739347457885742,
      "learning_rate": 1.9654090901977736e-05,
      "loss": 0.479,
      "step": 58800
    },
    {
      "epoch": 1.380090285513279,
      "grad_norm": 0.34015047550201416,
      "learning_rate": 1.963463210016661e-05,
      "loss": 0.4495,
      "step": 58850
    },
    {
      "epoch": 1.3812628246467726,
      "grad_norm": 0.31448090076446533,
      "learning_rate": 1.9615164671894208e-05,
      "loss": 0.4343,
      "step": 58900
    },
    {
      "epoch": 1.3824353637802662,
      "grad_norm": 0.45045730471611023,
      "learning_rate": 1.9595688653395337e-05,
      "loss": 0.5022,
      "step": 58950
    },
    {
      "epoch": 1.3836079029137598,
      "grad_norm": 0.20661665499210358,
      "learning_rate": 1.9576204080920793e-05,
      "loss": 0.4381,
      "step": 59000
    },
    {
      "epoch": 1.3847804420472534,
      "grad_norm": 0.33620062470436096,
      "learning_rate": 1.95567109907373e-05,
      "loss": 0.4585,
      "step": 59050
    },
    {
      "epoch": 1.385952981180747,
      "grad_norm": 0.2263491004705429,
      "learning_rate": 1.9537209419127435e-05,
      "loss": 0.4501,
      "step": 59100
    },
    {
      "epoch": 1.3871255203142405,
      "grad_norm": 0.30563437938690186,
      "learning_rate": 1.9517699402389556e-05,
      "loss": 0.4605,
      "step": 59150
    },
    {
      "epoch": 1.388298059447734,
      "grad_norm": 0.24920010566711426,
      "learning_rate": 1.9498180976837746e-05,
      "loss": 0.4394,
      "step": 59200
    },
    {
      "epoch": 1.3894705985812277,
      "grad_norm": 0.2914060354232788,
      "learning_rate": 1.9478654178801734e-05,
      "loss": 0.4487,
      "step": 59250
    },
    {
      "epoch": 1.3906431377147213,
      "grad_norm": 0.35320237278938293,
      "learning_rate": 1.9459119044626846e-05,
      "loss": 0.4786,
      "step": 59300
    },
    {
      "epoch": 1.3918156768482148,
      "grad_norm": 0.34216809272766113,
      "learning_rate": 1.9439575610673903e-05,
      "loss": 0.4357,
      "step": 59350
    },
    {
      "epoch": 1.3929882159817084,
      "grad_norm": 0.4736582934856415,
      "learning_rate": 1.9420023913319188e-05,
      "loss": 0.4581,
      "step": 59400
    },
    {
      "epoch": 1.394160755115202,
      "grad_norm": 0.34558168053627014,
      "learning_rate": 1.9400463988954365e-05,
      "loss": 0.4531,
      "step": 59450
    },
    {
      "epoch": 1.3953332942486956,
      "grad_norm": 0.37027594447135925,
      "learning_rate": 1.93808958739864e-05,
      "loss": 0.4412,
      "step": 59500
    },
    {
      "epoch": 1.3965058333821891,
      "grad_norm": 0.25465402007102966,
      "learning_rate": 1.9361319604837522e-05,
      "loss": 0.4676,
      "step": 59550
    },
    {
      "epoch": 1.3976783725156827,
      "grad_norm": 0.30603575706481934,
      "learning_rate": 1.934173521794512e-05,
      "loss": 0.4283,
      "step": 59600
    },
    {
      "epoch": 1.3988509116491763,
      "grad_norm": 0.2032083421945572,
      "learning_rate": 1.9322142749761704e-05,
      "loss": 0.4315,
      "step": 59650
    },
    {
      "epoch": 1.4000234507826699,
      "grad_norm": 0.4703346788883209,
      "learning_rate": 1.9302542236754825e-05,
      "loss": 0.4665,
      "step": 59700
    },
    {
      "epoch": 1.4011959899161635,
      "grad_norm": 0.3488200008869171,
      "learning_rate": 1.9282933715406997e-05,
      "loss": 0.4981,
      "step": 59750
    },
    {
      "epoch": 1.402368529049657,
      "grad_norm": 0.1978043168783188,
      "learning_rate": 1.9263317222215654e-05,
      "loss": 0.4188,
      "step": 59800
    },
    {
      "epoch": 1.4035410681831506,
      "grad_norm": 0.3016641438007355,
      "learning_rate": 1.9243692793693058e-05,
      "loss": 0.4449,
      "step": 59850
    },
    {
      "epoch": 1.4047136073166442,
      "grad_norm": 0.21137873828411102,
      "learning_rate": 1.922406046636625e-05,
      "loss": 0.4637,
      "step": 59900
    },
    {
      "epoch": 1.4058861464501378,
      "grad_norm": 0.34390488266944885,
      "learning_rate": 1.920442027677696e-05,
      "loss": 0.4597,
      "step": 59950
    },
    {
      "epoch": 1.4070586855836313,
      "grad_norm": 0.36886441707611084,
      "learning_rate": 1.9184772261481568e-05,
      "loss": 0.4849,
      "step": 60000
    },
    {
      "epoch": 1.4070586855836313,
      "eval_loss": 0.7661431431770325,
      "eval_runtime": 118.0397,
      "eval_samples_per_second": 84.717,
      "eval_steps_per_second": 21.179,
      "step": 60000
    },
    {
      "epoch": 1.408231224717125,
      "grad_norm": 0.33016663789749146,
      "learning_rate": 1.9165116457051004e-05,
      "loss": 0.4628,
      "step": 60050
    },
    {
      "epoch": 1.4094037638506185,
      "grad_norm": 0.3776843249797821,
      "learning_rate": 1.9145452900070715e-05,
      "loss": 0.4264,
      "step": 60100
    },
    {
      "epoch": 1.410576302984112,
      "grad_norm": 0.2861965596675873,
      "learning_rate": 1.9125781627140555e-05,
      "loss": 0.4591,
      "step": 60150
    },
    {
      "epoch": 1.4117488421176057,
      "grad_norm": 0.36392903327941895,
      "learning_rate": 1.9106102674874756e-05,
      "loss": 0.4732,
      "step": 60200
    },
    {
      "epoch": 1.4129213812510992,
      "grad_norm": 0.23833927512168884,
      "learning_rate": 1.9086416079901842e-05,
      "loss": 0.4554,
      "step": 60250
    },
    {
      "epoch": 1.4140939203845928,
      "grad_norm": 0.4982411861419678,
      "learning_rate": 1.9066721878864556e-05,
      "loss": 0.5086,
      "step": 60300
    },
    {
      "epoch": 1.4152664595180864,
      "grad_norm": 0.34697434306144714,
      "learning_rate": 1.9047020108419804e-05,
      "loss": 0.4588,
      "step": 60350
    },
    {
      "epoch": 1.41643899865158,
      "grad_norm": 0.20582756400108337,
      "learning_rate": 1.902731080523858e-05,
      "loss": 0.4293,
      "step": 60400
    },
    {
      "epoch": 1.4176115377850735,
      "grad_norm": 0.2698236405849457,
      "learning_rate": 1.900759400600589e-05,
      "loss": 0.4464,
      "step": 60450
    },
    {
      "epoch": 1.4187840769185671,
      "grad_norm": 0.29021602869033813,
      "learning_rate": 1.8987869747420707e-05,
      "loss": 0.4538,
      "step": 60500
    },
    {
      "epoch": 1.4199566160520607,
      "grad_norm": 0.37090563774108887,
      "learning_rate": 1.8968138066195882e-05,
      "loss": 0.4577,
      "step": 60550
    },
    {
      "epoch": 1.4211291551855543,
      "grad_norm": 0.32033345103263855,
      "learning_rate": 1.8948398999058076e-05,
      "loss": 0.4466,
      "step": 60600
    },
    {
      "epoch": 1.4223016943190478,
      "grad_norm": 0.35770782828330994,
      "learning_rate": 1.89286525827477e-05,
      "loss": 0.4073,
      "step": 60650
    },
    {
      "epoch": 1.4234742334525414,
      "grad_norm": 0.3535322844982147,
      "learning_rate": 1.8908898854018857e-05,
      "loss": 0.4464,
      "step": 60700
    },
    {
      "epoch": 1.424646772586035,
      "grad_norm": 0.17130470275878906,
      "learning_rate": 1.888913784963924e-05,
      "loss": 0.4216,
      "step": 60750
    },
    {
      "epoch": 1.4258193117195286,
      "grad_norm": 0.3937409222126007,
      "learning_rate": 1.8869369606390095e-05,
      "loss": 0.5009,
      "step": 60800
    },
    {
      "epoch": 1.4269918508530222,
      "grad_norm": 0.17101408541202545,
      "learning_rate": 1.8849594161066144e-05,
      "loss": 0.4596,
      "step": 60850
    },
    {
      "epoch": 1.4281643899865157,
      "grad_norm": 0.19143670797348022,
      "learning_rate": 1.882981155047551e-05,
      "loss": 0.4631,
      "step": 60900
    },
    {
      "epoch": 1.4293369291200093,
      "grad_norm": 0.24363555014133453,
      "learning_rate": 1.8810021811439656e-05,
      "loss": 0.4216,
      "step": 60950
    },
    {
      "epoch": 1.430509468253503,
      "grad_norm": 0.33451056480407715,
      "learning_rate": 1.8790224980793304e-05,
      "loss": 0.479,
      "step": 61000
    },
    {
      "epoch": 1.4316820073869965,
      "grad_norm": 0.41078701615333557,
      "learning_rate": 1.8770421095384394e-05,
      "loss": 0.4332,
      "step": 61050
    },
    {
      "epoch": 1.43285454652049,
      "grad_norm": 0.3116348385810852,
      "learning_rate": 1.8750610192073978e-05,
      "loss": 0.4666,
      "step": 61100
    },
    {
      "epoch": 1.4340270856539836,
      "grad_norm": 0.5440768599510193,
      "learning_rate": 1.8730792307736172e-05,
      "loss": 0.4432,
      "step": 61150
    },
    {
      "epoch": 1.4351996247874772,
      "grad_norm": 0.42629680037498474,
      "learning_rate": 1.8710967479258107e-05,
      "loss": 0.4652,
      "step": 61200
    },
    {
      "epoch": 1.4363721639209708,
      "grad_norm": 0.2896248400211334,
      "learning_rate": 1.8691135743539813e-05,
      "loss": 0.4587,
      "step": 61250
    },
    {
      "epoch": 1.4375447030544644,
      "grad_norm": 0.5151333212852478,
      "learning_rate": 1.8671297137494193e-05,
      "loss": 0.4594,
      "step": 61300
    },
    {
      "epoch": 1.4387172421879582,
      "grad_norm": 0.3223682641983032,
      "learning_rate": 1.865145169804693e-05,
      "loss": 0.4381,
      "step": 61350
    },
    {
      "epoch": 1.4398897813214515,
      "grad_norm": 0.3492993712425232,
      "learning_rate": 1.863159946213643e-05,
      "loss": 0.4888,
      "step": 61400
    },
    {
      "epoch": 1.4410623204549453,
      "grad_norm": 0.3360246419906616,
      "learning_rate": 1.8611740466713747e-05,
      "loss": 0.4224,
      "step": 61450
    },
    {
      "epoch": 1.4422348595884387,
      "grad_norm": 0.337872177362442,
      "learning_rate": 1.8591874748742515e-05,
      "loss": 0.4121,
      "step": 61500
    },
    {
      "epoch": 1.4434073987219325,
      "grad_norm": 0.20852094888687134,
      "learning_rate": 1.8572002345198886e-05,
      "loss": 0.4407,
      "step": 61550
    },
    {
      "epoch": 1.4445799378554258,
      "grad_norm": 0.3153616487979889,
      "learning_rate": 1.8552123293071453e-05,
      "loss": 0.4569,
      "step": 61600
    },
    {
      "epoch": 1.4457524769889196,
      "grad_norm": 0.48222190141677856,
      "learning_rate": 1.8532237629361187e-05,
      "loss": 0.4551,
      "step": 61650
    },
    {
      "epoch": 1.446925016122413,
      "grad_norm": 0.36483657360076904,
      "learning_rate": 1.8512345391081348e-05,
      "loss": 0.49,
      "step": 61700
    },
    {
      "epoch": 1.4480975552559068,
      "grad_norm": 0.35878852009773254,
      "learning_rate": 1.8492446615257464e-05,
      "loss": 0.417,
      "step": 61750
    },
    {
      "epoch": 1.4492700943894001,
      "grad_norm": 0.2683813273906708,
      "learning_rate": 1.8472541338927205e-05,
      "loss": 0.4167,
      "step": 61800
    },
    {
      "epoch": 1.450442633522894,
      "grad_norm": 0.42818090319633484,
      "learning_rate": 1.8452629599140352e-05,
      "loss": 0.4413,
      "step": 61850
    },
    {
      "epoch": 1.4516151726563873,
      "grad_norm": 0.4247044622898102,
      "learning_rate": 1.843271143295872e-05,
      "loss": 0.4614,
      "step": 61900
    },
    {
      "epoch": 1.452787711789881,
      "grad_norm": 0.2879079282283783,
      "learning_rate": 1.8412786877456073e-05,
      "loss": 0.4912,
      "step": 61950
    },
    {
      "epoch": 1.4539602509233744,
      "grad_norm": 0.46209967136383057,
      "learning_rate": 1.8392855969718075e-05,
      "loss": 0.479,
      "step": 62000
    },
    {
      "epoch": 1.4539602509233744,
      "eval_loss": 0.7641148567199707,
      "eval_runtime": 118.1159,
      "eval_samples_per_second": 84.663,
      "eval_steps_per_second": 21.166,
      "step": 62000
    },
    {
      "epoch": 1.4551327900568682,
      "grad_norm": 0.221284419298172,
      "learning_rate": 1.8372918746842215e-05,
      "loss": 0.4237,
      "step": 62050
    },
    {
      "epoch": 1.4563053291903616,
      "grad_norm": 0.313457727432251,
      "learning_rate": 1.8352975245937738e-05,
      "loss": 0.438,
      "step": 62100
    },
    {
      "epoch": 1.4574778683238554,
      "grad_norm": 0.22685156762599945,
      "learning_rate": 1.8333025504125564e-05,
      "loss": 0.4304,
      "step": 62150
    },
    {
      "epoch": 1.4586504074573488,
      "grad_norm": 0.45481592416763306,
      "learning_rate": 1.831306955853824e-05,
      "loss": 0.466,
      "step": 62200
    },
    {
      "epoch": 1.4598229465908426,
      "grad_norm": 0.3883720636367798,
      "learning_rate": 1.8293107446319853e-05,
      "loss": 0.4089,
      "step": 62250
    },
    {
      "epoch": 1.4609954857243361,
      "grad_norm": 0.31501176953315735,
      "learning_rate": 1.827313920462597e-05,
      "loss": 0.4405,
      "step": 62300
    },
    {
      "epoch": 1.4621680248578297,
      "grad_norm": 0.2237585186958313,
      "learning_rate": 1.8253164870623575e-05,
      "loss": 0.4629,
      "step": 62350
    },
    {
      "epoch": 1.4633405639913233,
      "grad_norm": 0.45273950695991516,
      "learning_rate": 1.823318448149098e-05,
      "loss": 0.4722,
      "step": 62400
    },
    {
      "epoch": 1.4645131031248169,
      "grad_norm": 0.3944071829319,
      "learning_rate": 1.8213198074417775e-05,
      "loss": 0.4524,
      "step": 62450
    },
    {
      "epoch": 1.4656856422583104,
      "grad_norm": 0.2374553084373474,
      "learning_rate": 1.8193205686604742e-05,
      "loss": 0.4685,
      "step": 62500
    },
    {
      "epoch": 1.466858181391804,
      "grad_norm": 0.350243479013443,
      "learning_rate": 1.8173207355263802e-05,
      "loss": 0.432,
      "step": 62550
    },
    {
      "epoch": 1.4680307205252976,
      "grad_norm": 0.4580775797367096,
      "learning_rate": 1.8153203117617944e-05,
      "loss": 0.4664,
      "step": 62600
    },
    {
      "epoch": 1.4692032596587912,
      "grad_norm": 0.38209110498428345,
      "learning_rate": 1.8133193010901143e-05,
      "loss": 0.4505,
      "step": 62650
    },
    {
      "epoch": 1.4703757987922847,
      "grad_norm": 0.45968830585479736,
      "learning_rate": 1.8113177072358293e-05,
      "loss": 0.4441,
      "step": 62700
    },
    {
      "epoch": 1.4715483379257783,
      "grad_norm": 0.3272923529148102,
      "learning_rate": 1.8093155339245154e-05,
      "loss": 0.4566,
      "step": 62750
    },
    {
      "epoch": 1.472720877059272,
      "grad_norm": 0.2471359670162201,
      "learning_rate": 1.8073127848828263e-05,
      "loss": 0.3895,
      "step": 62800
    },
    {
      "epoch": 1.4738934161927655,
      "grad_norm": 0.5224098563194275,
      "learning_rate": 1.805309463838488e-05,
      "loss": 0.46,
      "step": 62850
    },
    {
      "epoch": 1.475065955326259,
      "grad_norm": 0.35650986433029175,
      "learning_rate": 1.803305574520291e-05,
      "loss": 0.4286,
      "step": 62900
    },
    {
      "epoch": 1.4762384944597526,
      "grad_norm": 0.36388400197029114,
      "learning_rate": 1.8013011206580828e-05,
      "loss": 0.4502,
      "step": 62950
    },
    {
      "epoch": 1.4774110335932462,
      "grad_norm": 0.3338649272918701,
      "learning_rate": 1.7992961059827626e-05,
      "loss": 0.4393,
      "step": 63000
    },
    {
      "epoch": 1.4785835727267398,
      "grad_norm": 0.33110249042510986,
      "learning_rate": 1.7972905342262733e-05,
      "loss": 0.4786,
      "step": 63050
    },
    {
      "epoch": 1.4797561118602334,
      "grad_norm": 0.5300941467285156,
      "learning_rate": 1.7952844091215944e-05,
      "loss": 0.4691,
      "step": 63100
    },
    {
      "epoch": 1.480928650993727,
      "grad_norm": 0.3050095736980438,
      "learning_rate": 1.793277734402735e-05,
      "loss": 0.4464,
      "step": 63150
    },
    {
      "epoch": 1.4821011901272205,
      "grad_norm": 0.4270963668823242,
      "learning_rate": 1.7912705138047284e-05,
      "loss": 0.4363,
      "step": 63200
    },
    {
      "epoch": 1.483273729260714,
      "grad_norm": 0.20039905607700348,
      "learning_rate": 1.7892627510636227e-05,
      "loss": 0.4883,
      "step": 63250
    },
    {
      "epoch": 1.4844462683942077,
      "grad_norm": 0.3721906542778015,
      "learning_rate": 1.7872544499164758e-05,
      "loss": 0.448,
      "step": 63300
    },
    {
      "epoch": 1.4856188075277013,
      "grad_norm": 0.39129969477653503,
      "learning_rate": 1.7852456141013474e-05,
      "loss": 0.4522,
      "step": 63350
    },
    {
      "epoch": 1.4867913466611948,
      "grad_norm": 0.31746506690979004,
      "learning_rate": 1.783236247357292e-05,
      "loss": 0.4579,
      "step": 63400
    },
    {
      "epoch": 1.4879638857946884,
      "grad_norm": 0.4028860628604889,
      "learning_rate": 1.7812263534243542e-05,
      "loss": 0.4374,
      "step": 63450
    },
    {
      "epoch": 1.489136424928182,
      "grad_norm": 0.36776411533355713,
      "learning_rate": 1.7792159360435575e-05,
      "loss": 0.4299,
      "step": 63500
    },
    {
      "epoch": 1.4903089640616756,
      "grad_norm": 0.42534178495407104,
      "learning_rate": 1.7772049989569005e-05,
      "loss": 0.447,
      "step": 63550
    },
    {
      "epoch": 1.4914815031951691,
      "grad_norm": 0.31695976853370667,
      "learning_rate": 1.77519354590735e-05,
      "loss": 0.4868,
      "step": 63600
    },
    {
      "epoch": 1.4926540423286627,
      "grad_norm": 0.31699031591415405,
      "learning_rate": 1.7731815806388318e-05,
      "loss": 0.4838,
      "step": 63650
    },
    {
      "epoch": 1.4938265814621563,
      "grad_norm": 0.26513633131980896,
      "learning_rate": 1.7711691068962262e-05,
      "loss": 0.4859,
      "step": 63700
    },
    {
      "epoch": 1.4949991205956499,
      "grad_norm": 0.29999274015426636,
      "learning_rate": 1.76915612842536e-05,
      "loss": 0.4529,
      "step": 63750
    },
    {
      "epoch": 1.4961716597291435,
      "grad_norm": 0.3009786605834961,
      "learning_rate": 1.7671426489729977e-05,
      "loss": 0.4562,
      "step": 63800
    },
    {
      "epoch": 1.497344198862637,
      "grad_norm": 0.3843785226345062,
      "learning_rate": 1.7651286722868386e-05,
      "loss": 0.42,
      "step": 63850
    },
    {
      "epoch": 1.4985167379961306,
      "grad_norm": 0.31784379482269287,
      "learning_rate": 1.763114202115506e-05,
      "loss": 0.4296,
      "step": 63900
    },
    {
      "epoch": 1.4996892771296242,
      "grad_norm": 0.36266714334487915,
      "learning_rate": 1.7610992422085428e-05,
      "loss": 0.4072,
      "step": 63950
    },
    {
      "epoch": 1.5008618162631178,
      "grad_norm": 0.36118555068969727,
      "learning_rate": 1.7590837963164016e-05,
      "loss": 0.4038,
      "step": 64000
    },
    {
      "epoch": 1.5008618162631178,
      "eval_loss": 0.7640205025672913,
      "eval_runtime": 117.7695,
      "eval_samples_per_second": 84.912,
      "eval_steps_per_second": 21.228,
      "step": 64000
    },
    {
      "epoch": 1.5020343553966113,
      "grad_norm": 0.20273517072200775,
      "learning_rate": 1.7570678681904414e-05,
      "loss": 0.5102,
      "step": 64050
    },
    {
      "epoch": 1.503206894530105,
      "grad_norm": 0.2688237428665161,
      "learning_rate": 1.7550514615829178e-05,
      "loss": 0.476,
      "step": 64100
    },
    {
      "epoch": 1.5043794336635985,
      "grad_norm": 0.4556010663509369,
      "learning_rate": 1.7530345802469782e-05,
      "loss": 0.4607,
      "step": 64150
    },
    {
      "epoch": 1.505551972797092,
      "grad_norm": 0.3533894717693329,
      "learning_rate": 1.7510172279366523e-05,
      "loss": 0.4559,
      "step": 64200
    },
    {
      "epoch": 1.5067245119305857,
      "grad_norm": 0.4198839068412781,
      "learning_rate": 1.7489994084068468e-05,
      "loss": 0.4515,
      "step": 64250
    },
    {
      "epoch": 1.5078970510640792,
      "grad_norm": 0.4600948095321655,
      "learning_rate": 1.746981125413338e-05,
      "loss": 0.454,
      "step": 64300
    },
    {
      "epoch": 1.5090695901975728,
      "grad_norm": 0.24806876480579376,
      "learning_rate": 1.744962382712766e-05,
      "loss": 0.48,
      "step": 64350
    },
    {
      "epoch": 1.5102421293310664,
      "grad_norm": 0.18454040586948395,
      "learning_rate": 1.7429431840626242e-05,
      "loss": 0.4275,
      "step": 64400
    },
    {
      "epoch": 1.51141466846456,
      "grad_norm": 0.3716292381286621,
      "learning_rate": 1.7409235332212565e-05,
      "loss": 0.4859,
      "step": 64450
    },
    {
      "epoch": 1.5125872075980535,
      "grad_norm": 0.26490622758865356,
      "learning_rate": 1.7389034339478485e-05,
      "loss": 0.4577,
      "step": 64500
    },
    {
      "epoch": 1.5137597467315471,
      "grad_norm": 0.35822662711143494,
      "learning_rate": 1.736882890002419e-05,
      "loss": 0.4701,
      "step": 64550
    },
    {
      "epoch": 1.5149322858650407,
      "grad_norm": 0.29351806640625,
      "learning_rate": 1.7348619051458163e-05,
      "loss": 0.498,
      "step": 64600
    },
    {
      "epoch": 1.5161048249985343,
      "grad_norm": 0.27223435044288635,
      "learning_rate": 1.7328404831397074e-05,
      "loss": 0.4308,
      "step": 64650
    },
    {
      "epoch": 1.517277364132028,
      "grad_norm": 0.2592868506908417,
      "learning_rate": 1.7308186277465754e-05,
      "loss": 0.4458,
      "step": 64700
    },
    {
      "epoch": 1.5184499032655214,
      "grad_norm": 0.29234713315963745,
      "learning_rate": 1.728796342729708e-05,
      "loss": 0.4734,
      "step": 64750
    },
    {
      "epoch": 1.5196224423990152,
      "grad_norm": 0.34045737981796265,
      "learning_rate": 1.726773631853193e-05,
      "loss": 0.4693,
      "step": 64800
    },
    {
      "epoch": 1.5207949815325086,
      "grad_norm": 0.4978848099708557,
      "learning_rate": 1.7247504988819115e-05,
      "loss": 0.4664,
      "step": 64850
    },
    {
      "epoch": 1.5219675206660024,
      "grad_norm": 0.41713181138038635,
      "learning_rate": 1.72272694758153e-05,
      "loss": 0.4468,
      "step": 64900
    },
    {
      "epoch": 1.5231400597994957,
      "grad_norm": 0.3178139925003052,
      "learning_rate": 1.7207029817184928e-05,
      "loss": 0.4683,
      "step": 64950
    },
    {
      "epoch": 1.5243125989329895,
      "grad_norm": 0.2656571567058563,
      "learning_rate": 1.7186786050600174e-05,
      "loss": 0.4701,
      "step": 65000
    },
    {
      "epoch": 1.525485138066483,
      "grad_norm": 0.3926345705986023,
      "learning_rate": 1.7166538213740842e-05,
      "loss": 0.4664,
      "step": 65050
    },
    {
      "epoch": 1.5266576771999767,
      "grad_norm": 0.45550820231437683,
      "learning_rate": 1.7146286344294333e-05,
      "loss": 0.5079,
      "step": 65100
    },
    {
      "epoch": 1.52783021633347,
      "grad_norm": 0.28205251693725586,
      "learning_rate": 1.7126030479955526e-05,
      "loss": 0.4631,
      "step": 65150
    },
    {
      "epoch": 1.5290027554669638,
      "grad_norm": 0.243458092212677,
      "learning_rate": 1.7105770658426763e-05,
      "loss": 0.459,
      "step": 65200
    },
    {
      "epoch": 1.5301752946004572,
      "grad_norm": 0.442824125289917,
      "learning_rate": 1.7085506917417737e-05,
      "loss": 0.4468,
      "step": 65250
    },
    {
      "epoch": 1.531347833733951,
      "grad_norm": 0.333900511264801,
      "learning_rate": 1.7065239294645432e-05,
      "loss": 0.4299,
      "step": 65300
    },
    {
      "epoch": 1.5325203728674444,
      "grad_norm": 0.2140553742647171,
      "learning_rate": 1.7044967827834075e-05,
      "loss": 0.4193,
      "step": 65350
    },
    {
      "epoch": 1.5336929120009382,
      "grad_norm": 0.2330974042415619,
      "learning_rate": 1.7024692554715028e-05,
      "loss": 0.4463,
      "step": 65400
    },
    {
      "epoch": 1.5348654511344315,
      "grad_norm": 0.26593199372291565,
      "learning_rate": 1.700441351302675e-05,
      "loss": 0.4598,
      "step": 65450
    },
    {
      "epoch": 1.5360379902679253,
      "grad_norm": 0.44690316915512085,
      "learning_rate": 1.6984130740514713e-05,
      "loss": 0.4419,
      "step": 65500
    },
    {
      "epoch": 1.5372105294014187,
      "grad_norm": 0.21958208084106445,
      "learning_rate": 1.6963844274931332e-05,
      "loss": 0.4829,
      "step": 65550
    },
    {
      "epoch": 1.5383830685349125,
      "grad_norm": 0.36108630895614624,
      "learning_rate": 1.6943554154035888e-05,
      "loss": 0.4444,
      "step": 65600
    },
    {
      "epoch": 1.5395556076684058,
      "grad_norm": 0.28389593958854675,
      "learning_rate": 1.6923260415594484e-05,
      "loss": 0.472,
      "step": 65650
    },
    {
      "epoch": 1.5407281468018996,
      "grad_norm": 0.19911980628967285,
      "learning_rate": 1.690296309737994e-05,
      "loss": 0.4737,
      "step": 65700
    },
    {
      "epoch": 1.541900685935393,
      "grad_norm": 0.2389659583568573,
      "learning_rate": 1.688266223717174e-05,
      "loss": 0.4467,
      "step": 65750
    },
    {
      "epoch": 1.5430732250688868,
      "grad_norm": 0.3573566973209381,
      "learning_rate": 1.686235787275597e-05,
      "loss": 0.4595,
      "step": 65800
    },
    {
      "epoch": 1.5442457642023801,
      "grad_norm": 0.1897396445274353,
      "learning_rate": 1.6842050041925233e-05,
      "loss": 0.4431,
      "step": 65850
    },
    {
      "epoch": 1.545418303335874,
      "grad_norm": 0.2649535834789276,
      "learning_rate": 1.6821738782478587e-05,
      "loss": 0.4716,
      "step": 65900
    },
    {
      "epoch": 1.5465908424693673,
      "grad_norm": 0.3579157888889313,
      "learning_rate": 1.6801424132221464e-05,
      "loss": 0.4706,
      "step": 65950
    },
    {
      "epoch": 1.547763381602861,
      "grad_norm": 0.24978986382484436,
      "learning_rate": 1.6781106128965615e-05,
      "loss": 0.4383,
      "step": 66000
    },
    {
      "epoch": 1.547763381602861,
      "eval_loss": 0.7665382027626038,
      "eval_runtime": 117.7714,
      "eval_samples_per_second": 84.91,
      "eval_steps_per_second": 21.228,
      "step": 66000
    },
    {
      "epoch": 1.5489359207363544,
      "grad_norm": 0.47286754846572876,
      "learning_rate": 1.6760784810529032e-05,
      "loss": 0.4575,
      "step": 66050
    },
    {
      "epoch": 1.5501084598698482,
      "grad_norm": 0.2517044246196747,
      "learning_rate": 1.6740460214735873e-05,
      "loss": 0.4527,
      "step": 66100
    },
    {
      "epoch": 1.5512809990033416,
      "grad_norm": 0.27323147654533386,
      "learning_rate": 1.6720132379416398e-05,
      "loss": 0.4801,
      "step": 66150
    },
    {
      "epoch": 1.5524535381368354,
      "grad_norm": 0.5018685460090637,
      "learning_rate": 1.6699801342406897e-05,
      "loss": 0.4279,
      "step": 66200
    },
    {
      "epoch": 1.5536260772703288,
      "grad_norm": 0.34909266233444214,
      "learning_rate": 1.667946714154962e-05,
      "loss": 0.4839,
      "step": 66250
    },
    {
      "epoch": 1.5547986164038226,
      "grad_norm": 0.37331318855285645,
      "learning_rate": 1.66591298146927e-05,
      "loss": 0.4548,
      "step": 66300
    },
    {
      "epoch": 1.555971155537316,
      "grad_norm": 0.3977275490760803,
      "learning_rate": 1.6638789399690105e-05,
      "loss": 0.4433,
      "step": 66350
    },
    {
      "epoch": 1.5571436946708097,
      "grad_norm": 0.4019782245159149,
      "learning_rate": 1.661844593440153e-05,
      "loss": 0.4577,
      "step": 66400
    },
    {
      "epoch": 1.558316233804303,
      "grad_norm": 0.2365659773349762,
      "learning_rate": 1.6598099456692362e-05,
      "loss": 0.4557,
      "step": 66450
    },
    {
      "epoch": 1.5594887729377969,
      "grad_norm": 0.31621453166007996,
      "learning_rate": 1.657775000443359e-05,
      "loss": 0.4663,
      "step": 66500
    },
    {
      "epoch": 1.5606613120712902,
      "grad_norm": 0.24899597465991974,
      "learning_rate": 1.6557397615501736e-05,
      "loss": 0.4131,
      "step": 66550
    },
    {
      "epoch": 1.561833851204784,
      "grad_norm": 0.24478182196617126,
      "learning_rate": 1.6537042327778792e-05,
      "loss": 0.4537,
      "step": 66600
    },
    {
      "epoch": 1.5630063903382774,
      "grad_norm": 0.186197891831398,
      "learning_rate": 1.651668417915215e-05,
      "loss": 0.4476,
      "step": 66650
    },
    {
      "epoch": 1.5641789294717712,
      "grad_norm": 0.29982760548591614,
      "learning_rate": 1.6496323207514517e-05,
      "loss": 0.4267,
      "step": 66700
    },
    {
      "epoch": 1.5653514686052648,
      "grad_norm": 0.2706441879272461,
      "learning_rate": 1.6475959450763864e-05,
      "loss": 0.478,
      "step": 66750
    },
    {
      "epoch": 1.5665240077387583,
      "grad_norm": 0.326903373003006,
      "learning_rate": 1.6455592946803346e-05,
      "loss": 0.4464,
      "step": 66800
    },
    {
      "epoch": 1.567696546872252,
      "grad_norm": 0.324010968208313,
      "learning_rate": 1.6435223733541213e-05,
      "loss": 0.429,
      "step": 66850
    },
    {
      "epoch": 1.5688690860057455,
      "grad_norm": 0.38420459628105164,
      "learning_rate": 1.6414851848890782e-05,
      "loss": 0.4483,
      "step": 66900
    },
    {
      "epoch": 1.570041625139239,
      "grad_norm": 0.2580522894859314,
      "learning_rate": 1.639447733077033e-05,
      "loss": 0.4239,
      "step": 66950
    },
    {
      "epoch": 1.5712141642727326,
      "grad_norm": 0.3459864854812622,
      "learning_rate": 1.637410021710304e-05,
      "loss": 0.4655,
      "step": 67000
    },
    {
      "epoch": 1.5723867034062262,
      "grad_norm": 0.2876800000667572,
      "learning_rate": 1.6353720545816925e-05,
      "loss": 0.4466,
      "step": 67050
    },
    {
      "epoch": 1.5735592425397198,
      "grad_norm": 0.3702459931373596,
      "learning_rate": 1.6333338354844754e-05,
      "loss": 0.4481,
      "step": 67100
    },
    {
      "epoch": 1.5747317816732134,
      "grad_norm": 0.4273340702056885,
      "learning_rate": 1.6312953682123987e-05,
      "loss": 0.4498,
      "step": 67150
    },
    {
      "epoch": 1.575904320806707,
      "grad_norm": 0.3651605546474457,
      "learning_rate": 1.6292566565596714e-05,
      "loss": 0.4695,
      "step": 67200
    },
    {
      "epoch": 1.5770768599402005,
      "grad_norm": 0.34547969698905945,
      "learning_rate": 1.6272177043209556e-05,
      "loss": 0.4485,
      "step": 67250
    },
    {
      "epoch": 1.578249399073694,
      "grad_norm": 0.2778182923793793,
      "learning_rate": 1.6251785152913626e-05,
      "loss": 0.4579,
      "step": 67300
    },
    {
      "epoch": 1.5794219382071877,
      "grad_norm": 0.35374322533607483,
      "learning_rate": 1.6231390932664446e-05,
      "loss": 0.4762,
      "step": 67350
    },
    {
      "epoch": 1.5805944773406813,
      "grad_norm": 0.27618321776390076,
      "learning_rate": 1.6210994420421857e-05,
      "loss": 0.4842,
      "step": 67400
    },
    {
      "epoch": 1.5817670164741748,
      "grad_norm": 0.2889818251132965,
      "learning_rate": 1.6190595654149983e-05,
      "loss": 0.4491,
      "step": 67450
    },
    {
      "epoch": 1.5829395556076684,
      "grad_norm": 0.5329071283340454,
      "learning_rate": 1.6170194671817138e-05,
      "loss": 0.4464,
      "step": 67500
    },
    {
      "epoch": 1.584112094741162,
      "grad_norm": 0.32275208830833435,
      "learning_rate": 1.6149791511395764e-05,
      "loss": 0.4224,
      "step": 67550
    },
    {
      "epoch": 1.5852846338746556,
      "grad_norm": 0.2904199957847595,
      "learning_rate": 1.612938621086235e-05,
      "loss": 0.4381,
      "step": 67600
    },
    {
      "epoch": 1.5864571730081491,
      "grad_norm": 0.3532509505748749,
      "learning_rate": 1.6108978808197372e-05,
      "loss": 0.4666,
      "step": 67650
    },
    {
      "epoch": 1.5876297121416427,
      "grad_norm": 0.4015501141548157,
      "learning_rate": 1.6088569341385217e-05,
      "loss": 0.4465,
      "step": 67700
    },
    {
      "epoch": 1.5888022512751363,
      "grad_norm": 0.2395353466272354,
      "learning_rate": 1.6068157848414125e-05,
      "loss": 0.4872,
      "step": 67750
    },
    {
      "epoch": 1.5899747904086299,
      "grad_norm": 0.544955849647522,
      "learning_rate": 1.6047744367276094e-05,
      "loss": 0.4386,
      "step": 67800
    },
    {
      "epoch": 1.5911473295421235,
      "grad_norm": 0.23981580138206482,
      "learning_rate": 1.602732893596682e-05,
      "loss": 0.4216,
      "step": 67850
    },
    {
      "epoch": 1.592319868675617,
      "grad_norm": 0.35090410709381104,
      "learning_rate": 1.600691159248565e-05,
      "loss": 0.4549,
      "step": 67900
    },
    {
      "epoch": 1.5934924078091106,
      "grad_norm": 0.3796846270561218,
      "learning_rate": 1.5986492374835462e-05,
      "loss": 0.4626,
      "step": 67950
    },
    {
      "epoch": 1.5946649469426042,
      "grad_norm": 0.4566230773925781,
      "learning_rate": 1.5966071321022645e-05,
      "loss": 0.4677,
      "step": 68000
    },
    {
      "epoch": 1.5946649469426042,
      "eval_loss": 0.767306923866272,
      "eval_runtime": 118.0898,
      "eval_samples_per_second": 84.681,
      "eval_steps_per_second": 21.17,
      "step": 68000
    },
    {
      "epoch": 1.5958374860760978,
      "grad_norm": 0.2528766989707947,
      "learning_rate": 1.594564846905699e-05,
      "loss": 0.4599,
      "step": 68050
    },
    {
      "epoch": 1.5970100252095913,
      "grad_norm": 0.2771257162094116,
      "learning_rate": 1.592522385695165e-05,
      "loss": 0.4523,
      "step": 68100
    },
    {
      "epoch": 1.598182564343085,
      "grad_norm": 0.34211060404777527,
      "learning_rate": 1.590479752272304e-05,
      "loss": 0.4721,
      "step": 68150
    },
    {
      "epoch": 1.5993551034765785,
      "grad_norm": 0.3633289933204651,
      "learning_rate": 1.5884369504390795e-05,
      "loss": 0.4799,
      "step": 68200
    },
    {
      "epoch": 1.600527642610072,
      "grad_norm": 0.3304908871650696,
      "learning_rate": 1.5863939839977663e-05,
      "loss": 0.4653,
      "step": 68250
    },
    {
      "epoch": 1.6017001817435657,
      "grad_norm": 0.2993142306804657,
      "learning_rate": 1.584350856750948e-05,
      "loss": 0.4642,
      "step": 68300
    },
    {
      "epoch": 1.6028727208770592,
      "grad_norm": 0.3088359832763672,
      "learning_rate": 1.5823075725015055e-05,
      "loss": 0.4541,
      "step": 68350
    },
    {
      "epoch": 1.6040452600105528,
      "grad_norm": 0.4217180013656616,
      "learning_rate": 1.5802641350526136e-05,
      "loss": 0.4508,
      "step": 68400
    },
    {
      "epoch": 1.6052177991440464,
      "grad_norm": 0.43297165632247925,
      "learning_rate": 1.5782205482077313e-05,
      "loss": 0.4321,
      "step": 68450
    },
    {
      "epoch": 1.60639033827754,
      "grad_norm": 0.49976539611816406,
      "learning_rate": 1.5761768157705952e-05,
      "loss": 0.4583,
      "step": 68500
    },
    {
      "epoch": 1.6075628774110335,
      "grad_norm": 0.2681984007358551,
      "learning_rate": 1.5741329415452148e-05,
      "loss": 0.4637,
      "step": 68550
    },
    {
      "epoch": 1.6087354165445271,
      "grad_norm": 0.3212721049785614,
      "learning_rate": 1.572088929335861e-05,
      "loss": 0.4663,
      "step": 68600
    },
    {
      "epoch": 1.609907955678021,
      "grad_norm": 0.4247382581233978,
      "learning_rate": 1.5700447829470635e-05,
      "loss": 0.4242,
      "step": 68650
    },
    {
      "epoch": 1.6110804948115143,
      "grad_norm": 0.6731895804405212,
      "learning_rate": 1.5680005061836013e-05,
      "loss": 0.4859,
      "step": 68700
    },
    {
      "epoch": 1.612253033945008,
      "grad_norm": 0.3847983479499817,
      "learning_rate": 1.5659561028504947e-05,
      "loss": 0.4389,
      "step": 68750
    },
    {
      "epoch": 1.6134255730785014,
      "grad_norm": 0.28057384490966797,
      "learning_rate": 1.563911576753002e-05,
      "loss": 0.4534,
      "step": 68800
    },
    {
      "epoch": 1.6145981122119952,
      "grad_norm": 0.41976141929626465,
      "learning_rate": 1.5618669316966077e-05,
      "loss": 0.4471,
      "step": 68850
    },
    {
      "epoch": 1.6157706513454886,
      "grad_norm": 0.27889755368232727,
      "learning_rate": 1.559822171487019e-05,
      "loss": 0.4619,
      "step": 68900
    },
    {
      "epoch": 1.6169431904789824,
      "grad_norm": 0.2951051890850067,
      "learning_rate": 1.557777299930158e-05,
      "loss": 0.4305,
      "step": 68950
    },
    {
      "epoch": 1.6181157296124757,
      "grad_norm": 0.17805582284927368,
      "learning_rate": 1.5557323208321516e-05,
      "loss": 0.4493,
      "step": 69000
    },
    {
      "epoch": 1.6192882687459695,
      "grad_norm": 0.2661152780056,
      "learning_rate": 1.5536872379993295e-05,
      "loss": 0.4382,
      "step": 69050
    },
    {
      "epoch": 1.620460807879463,
      "grad_norm": 0.3988037407398224,
      "learning_rate": 1.5516420552382132e-05,
      "loss": 0.4658,
      "step": 69100
    },
    {
      "epoch": 1.6216333470129567,
      "grad_norm": 0.3350147008895874,
      "learning_rate": 1.5495967763555104e-05,
      "loss": 0.4751,
      "step": 69150
    },
    {
      "epoch": 1.62280588614645,
      "grad_norm": 0.41750651597976685,
      "learning_rate": 1.5475514051581084e-05,
      "loss": 0.4849,
      "step": 69200
    },
    {
      "epoch": 1.6239784252799438,
      "grad_norm": 0.45993101596832275,
      "learning_rate": 1.5455059454530644e-05,
      "loss": 0.4371,
      "step": 69250
    },
    {
      "epoch": 1.6251509644134372,
      "grad_norm": 0.374270498752594,
      "learning_rate": 1.5434604010476026e-05,
      "loss": 0.4473,
      "step": 69300
    },
    {
      "epoch": 1.626323503546931,
      "grad_norm": 0.451741486787796,
      "learning_rate": 1.541414775749104e-05,
      "loss": 0.4612,
      "step": 69350
    },
    {
      "epoch": 1.6274960426804244,
      "grad_norm": 0.3674762547016144,
      "learning_rate": 1.539369073365099e-05,
      "loss": 0.4728,
      "step": 69400
    },
    {
      "epoch": 1.6286685818139182,
      "grad_norm": 0.2121860384941101,
      "learning_rate": 1.5373232977032638e-05,
      "loss": 0.446,
      "step": 69450
    },
    {
      "epoch": 1.6298411209474115,
      "grad_norm": 0.396992564201355,
      "learning_rate": 1.5352774525714084e-05,
      "loss": 0.443,
      "step": 69500
    },
    {
      "epoch": 1.6310136600809053,
      "grad_norm": 0.3960913419723511,
      "learning_rate": 1.5332315417774742e-05,
      "loss": 0.4229,
      "step": 69550
    },
    {
      "epoch": 1.6321861992143987,
      "grad_norm": 0.30760008096694946,
      "learning_rate": 1.5311855691295242e-05,
      "loss": 0.4167,
      "step": 69600
    },
    {
      "epoch": 1.6333587383478925,
      "grad_norm": 0.2920495867729187,
      "learning_rate": 1.529139538435736e-05,
      "loss": 0.5064,
      "step": 69650
    },
    {
      "epoch": 1.6345312774813858,
      "grad_norm": 0.6260857582092285,
      "learning_rate": 1.5270934535043956e-05,
      "loss": 0.4724,
      "step": 69700
    },
    {
      "epoch": 1.6357038166148796,
      "grad_norm": 0.43228214979171753,
      "learning_rate": 1.5250473181438904e-05,
      "loss": 0.4597,
      "step": 69750
    },
    {
      "epoch": 1.636876355748373,
      "grad_norm": 0.3764851987361908,
      "learning_rate": 1.5230011361627008e-05,
      "loss": 0.4246,
      "step": 69800
    },
    {
      "epoch": 1.6380488948818668,
      "grad_norm": 0.38028115034103394,
      "learning_rate": 1.5209549113693942e-05,
      "loss": 0.4424,
      "step": 69850
    },
    {
      "epoch": 1.6392214340153601,
      "grad_norm": 0.2065298855304718,
      "learning_rate": 1.5189086475726184e-05,
      "loss": 0.4436,
      "step": 69900
    },
    {
      "epoch": 1.640393973148854,
      "grad_norm": 0.30225038528442383,
      "learning_rate": 1.5168623485810926e-05,
      "loss": 0.4466,
      "step": 69950
    },
    {
      "epoch": 1.6415665122823473,
      "grad_norm": 0.35378435254096985,
      "learning_rate": 1.514816018203603e-05,
      "loss": 0.4533,
      "step": 70000
    },
    {
      "epoch": 1.6415665122823473,
      "eval_loss": 0.7659292221069336,
      "eval_runtime": 119.8762,
      "eval_samples_per_second": 83.419,
      "eval_steps_per_second": 20.855,
      "step": 70000
    },
    {
      "epoch": 1.642739051415841,
      "grad_norm": 0.3479296863079071,
      "learning_rate": 1.5127696602489927e-05,
      "loss": 0.441,
      "step": 70050
    },
    {
      "epoch": 1.6439115905493344,
      "grad_norm": 0.1881496161222458,
      "learning_rate": 1.5107232785261576e-05,
      "loss": 0.4196,
      "step": 70100
    },
    {
      "epoch": 1.6450841296828282,
      "grad_norm": 0.31583303213119507,
      "learning_rate": 1.5086768768440362e-05,
      "loss": 0.4194,
      "step": 70150
    },
    {
      "epoch": 1.6462566688163216,
      "grad_norm": 0.4203726053237915,
      "learning_rate": 1.5066304590116056e-05,
      "loss": 0.4832,
      "step": 70200
    },
    {
      "epoch": 1.6474292079498154,
      "grad_norm": 0.2514176666736603,
      "learning_rate": 1.5045840288378723e-05,
      "loss": 0.487,
      "step": 70250
    },
    {
      "epoch": 1.6486017470833088,
      "grad_norm": 0.2868369221687317,
      "learning_rate": 1.5025375901318657e-05,
      "loss": 0.4438,
      "step": 70300
    },
    {
      "epoch": 1.6497742862168026,
      "grad_norm": 0.6183373928070068,
      "learning_rate": 1.5004911467026314e-05,
      "loss": 0.4606,
      "step": 70350
    },
    {
      "epoch": 1.650946825350296,
      "grad_norm": 0.36666974425315857,
      "learning_rate": 1.4984447023592236e-05,
      "loss": 0.4516,
      "step": 70400
    },
    {
      "epoch": 1.6521193644837897,
      "grad_norm": 0.4226853847503662,
      "learning_rate": 1.4963982609106985e-05,
      "loss": 0.4914,
      "step": 70450
    },
    {
      "epoch": 1.653291903617283,
      "grad_norm": 0.39731565117836,
      "learning_rate": 1.494351826166106e-05,
      "loss": 0.4347,
      "step": 70500
    },
    {
      "epoch": 1.6544644427507769,
      "grad_norm": 0.42531225085258484,
      "learning_rate": 1.4923054019344844e-05,
      "loss": 0.4644,
      "step": 70550
    },
    {
      "epoch": 1.6556369818842704,
      "grad_norm": 0.2102389633655548,
      "learning_rate": 1.4902589920248526e-05,
      "loss": 0.4529,
      "step": 70600
    },
    {
      "epoch": 1.656809521017764,
      "grad_norm": 0.30137819051742554,
      "learning_rate": 1.4882126002462017e-05,
      "loss": 0.4299,
      "step": 70650
    },
    {
      "epoch": 1.6579820601512576,
      "grad_norm": 0.3987412452697754,
      "learning_rate": 1.4861662304074906e-05,
      "loss": 0.4315,
      "step": 70700
    },
    {
      "epoch": 1.6591545992847512,
      "grad_norm": 0.3786076307296753,
      "learning_rate": 1.4841198863176359e-05,
      "loss": 0.4517,
      "step": 70750
    },
    {
      "epoch": 1.6603271384182448,
      "grad_norm": 0.29197677969932556,
      "learning_rate": 1.4820735717855076e-05,
      "loss": 0.451,
      "step": 70800
    },
    {
      "epoch": 1.6614996775517383,
      "grad_norm": 0.43381521105766296,
      "learning_rate": 1.4800272906199193e-05,
      "loss": 0.4064,
      "step": 70850
    },
    {
      "epoch": 1.662672216685232,
      "grad_norm": 0.21654991805553436,
      "learning_rate": 1.4779810466296236e-05,
      "loss": 0.4557,
      "step": 70900
    },
    {
      "epoch": 1.6638447558187255,
      "grad_norm": 0.33808428049087524,
      "learning_rate": 1.4759348436233034e-05,
      "loss": 0.4457,
      "step": 70950
    },
    {
      "epoch": 1.665017294952219,
      "grad_norm": 0.42565473914146423,
      "learning_rate": 1.4738886854095658e-05,
      "loss": 0.4342,
      "step": 71000
    },
    {
      "epoch": 1.6661898340857126,
      "grad_norm": 0.3492909073829651,
      "learning_rate": 1.4718425757969335e-05,
      "loss": 0.4754,
      "step": 71050
    },
    {
      "epoch": 1.6673623732192062,
      "grad_norm": 0.42877647280693054,
      "learning_rate": 1.4697965185938399e-05,
      "loss": 0.4396,
      "step": 71100
    },
    {
      "epoch": 1.6685349123526998,
      "grad_norm": 0.32141172885894775,
      "learning_rate": 1.46775051760862e-05,
      "loss": 0.4711,
      "step": 71150
    },
    {
      "epoch": 1.6697074514861934,
      "grad_norm": 0.41359463334083557,
      "learning_rate": 1.4657045766495049e-05,
      "loss": 0.4538,
      "step": 71200
    },
    {
      "epoch": 1.670879990619687,
      "grad_norm": 0.25356751680374146,
      "learning_rate": 1.4636586995246132e-05,
      "loss": 0.4174,
      "step": 71250
    },
    {
      "epoch": 1.6720525297531805,
      "grad_norm": 0.4140589237213135,
      "learning_rate": 1.4616128900419452e-05,
      "loss": 0.4905,
      "step": 71300
    },
    {
      "epoch": 1.673225068886674,
      "grad_norm": 0.30837249755859375,
      "learning_rate": 1.4595671520093753e-05,
      "loss": 0.4741,
      "step": 71350
    },
    {
      "epoch": 1.6743976080201677,
      "grad_norm": 0.5706541538238525,
      "learning_rate": 1.4575214892346448e-05,
      "loss": 0.4555,
      "step": 71400
    },
    {
      "epoch": 1.6755701471536613,
      "grad_norm": 0.27902430295944214,
      "learning_rate": 1.4554759055253544e-05,
      "loss": 0.4461,
      "step": 71450
    },
    {
      "epoch": 1.6767426862871548,
      "grad_norm": 0.425403892993927,
      "learning_rate": 1.4534304046889589e-05,
      "loss": 0.4775,
      "step": 71500
    },
    {
      "epoch": 1.6779152254206484,
      "grad_norm": 0.2790914475917816,
      "learning_rate": 1.4513849905327573e-05,
      "loss": 0.4385,
      "step": 71550
    },
    {
      "epoch": 1.679087764554142,
      "grad_norm": 0.2890022397041321,
      "learning_rate": 1.4493396668638888e-05,
      "loss": 0.4516,
      "step": 71600
    },
    {
      "epoch": 1.6802603036876356,
      "grad_norm": 0.31190913915634155,
      "learning_rate": 1.4472944374893225e-05,
      "loss": 0.4725,
      "step": 71650
    },
    {
      "epoch": 1.6814328428211291,
      "grad_norm": 0.2589890956878662,
      "learning_rate": 1.4452493062158537e-05,
      "loss": 0.5099,
      "step": 71700
    },
    {
      "epoch": 1.6826053819546227,
      "grad_norm": 0.45036372542381287,
      "learning_rate": 1.4432042768500945e-05,
      "loss": 0.5111,
      "step": 71750
    },
    {
      "epoch": 1.6837779210881163,
      "grad_norm": 0.35142645239830017,
      "learning_rate": 1.4411593531984662e-05,
      "loss": 0.4624,
      "step": 71800
    },
    {
      "epoch": 1.6849504602216099,
      "grad_norm": 0.25526463985443115,
      "learning_rate": 1.4391145390671948e-05,
      "loss": 0.4456,
      "step": 71850
    },
    {
      "epoch": 1.6861229993551035,
      "grad_norm": 0.38736048340797424,
      "learning_rate": 1.4370698382623018e-05,
      "loss": 0.4731,
      "step": 71900
    },
    {
      "epoch": 1.687295538488597,
      "grad_norm": 0.4301828444004059,
      "learning_rate": 1.4350252545895978e-05,
      "loss": 0.4772,
      "step": 71950
    },
    {
      "epoch": 1.6884680776220906,
      "grad_norm": 0.26033568382263184,
      "learning_rate": 1.4329807918546757e-05,
      "loss": 0.4421,
      "step": 72000
    },
    {
      "epoch": 1.6884680776220906,
      "eval_loss": 0.764549732208252,
      "eval_runtime": 126.1147,
      "eval_samples_per_second": 79.293,
      "eval_steps_per_second": 19.823,
      "step": 72000
    },
    {
      "epoch": 1.6896171659729142,
      "grad_norm": 0.17759859561920166,
      "learning_rate": 1.4309364538629027e-05,
      "loss": 0.4391,
      "step": 72050
    },
    {
      "epoch": 1.690789705106408,
      "grad_norm": 0.2801699638366699,
      "learning_rate": 1.4288922444194147e-05,
      "loss": 0.4154,
      "step": 72100
    },
    {
      "epoch": 1.6919622442399014,
      "grad_norm": 0.3569837212562561,
      "learning_rate": 1.4268481673291072e-05,
      "loss": 0.5056,
      "step": 72150
    },
    {
      "epoch": 1.6931347833733952,
      "grad_norm": 0.41751375794410706,
      "learning_rate": 1.4248042263966296e-05,
      "loss": 0.4731,
      "step": 72200
    },
    {
      "epoch": 1.6943073225068885,
      "grad_norm": 0.23638443648815155,
      "learning_rate": 1.4227604254263797e-05,
      "loss": 0.4838,
      "step": 72250
    },
    {
      "epoch": 1.6954798616403823,
      "grad_norm": 0.1857987493276596,
      "learning_rate": 1.4207167682224923e-05,
      "loss": 0.4557,
      "step": 72300
    },
    {
      "epoch": 1.6966524007738757,
      "grad_norm": 0.31365153193473816,
      "learning_rate": 1.418673258588836e-05,
      "loss": 0.4292,
      "step": 72350
    },
    {
      "epoch": 1.6978249399073695,
      "grad_norm": 0.36861440539360046,
      "learning_rate": 1.416629900329004e-05,
      "loss": 0.4524,
      "step": 72400
    },
    {
      "epoch": 1.6989974790408628,
      "grad_norm": 0.3552660346031189,
      "learning_rate": 1.4145866972463085e-05,
      "loss": 0.4221,
      "step": 72450
    },
    {
      "epoch": 1.7001700181743566,
      "grad_norm": 0.3950439691543579,
      "learning_rate": 1.412543653143772e-05,
      "loss": 0.4294,
      "step": 72500
    },
    {
      "epoch": 1.70134255730785,
      "grad_norm": 0.3051322102546692,
      "learning_rate": 1.4105007718241215e-05,
      "loss": 0.4204,
      "step": 72550
    },
    {
      "epoch": 1.7025150964413438,
      "grad_norm": 0.5742371082305908,
      "learning_rate": 1.4084580570897815e-05,
      "loss": 0.4953,
      "step": 72600
    },
    {
      "epoch": 1.7036876355748372,
      "grad_norm": 0.3409884572029114,
      "learning_rate": 1.4064155127428658e-05,
      "loss": 0.4587,
      "step": 72650
    },
    {
      "epoch": 1.704860174708331,
      "grad_norm": 0.5037392377853394,
      "learning_rate": 1.4043731425851712e-05,
      "loss": 0.4525,
      "step": 72700
    },
    {
      "epoch": 1.7060327138418243,
      "grad_norm": 0.3020317554473877,
      "learning_rate": 1.4023309504181701e-05,
      "loss": 0.4667,
      "step": 72750
    },
    {
      "epoch": 1.707205252975318,
      "grad_norm": 0.3310062289237976,
      "learning_rate": 1.4002889400430044e-05,
      "loss": 0.4481,
      "step": 72800
    },
    {
      "epoch": 1.7083777921088115,
      "grad_norm": 0.20500220358371735,
      "learning_rate": 1.3982471152604767e-05,
      "loss": 0.4298,
      "step": 72850
    },
    {
      "epoch": 1.7095503312423053,
      "grad_norm": 0.36093735694885254,
      "learning_rate": 1.3962054798710445e-05,
      "loss": 0.4827,
      "step": 72900
    },
    {
      "epoch": 1.7107228703757988,
      "grad_norm": 0.4209170639514923,
      "learning_rate": 1.3941640376748129e-05,
      "loss": 0.4605,
      "step": 72950
    },
    {
      "epoch": 1.7118954095092924,
      "grad_norm": 0.3233787715435028,
      "learning_rate": 1.3921227924715274e-05,
      "loss": 0.4671,
      "step": 73000
    },
    {
      "epoch": 1.713067948642786,
      "grad_norm": 0.41028091311454773,
      "learning_rate": 1.3900817480605667e-05,
      "loss": 0.4621,
      "step": 73050
    },
    {
      "epoch": 1.7142404877762796,
      "grad_norm": 0.18465356528759003,
      "learning_rate": 1.3880409082409356e-05,
      "loss": 0.4778,
      "step": 73100
    },
    {
      "epoch": 1.7154130269097732,
      "grad_norm": 0.40901121497154236,
      "learning_rate": 1.3860002768112585e-05,
      "loss": 0.4688,
      "step": 73150
    },
    {
      "epoch": 1.7165855660432667,
      "grad_norm": 0.20570912957191467,
      "learning_rate": 1.3839598575697718e-05,
      "loss": 0.4788,
      "step": 73200
    },
    {
      "epoch": 1.7177581051767603,
      "grad_norm": 0.35502946376800537,
      "learning_rate": 1.3819196543143168e-05,
      "loss": 0.4246,
      "step": 73250
    },
    {
      "epoch": 1.7189306443102539,
      "grad_norm": 0.31988993287086487,
      "learning_rate": 1.3798796708423327e-05,
      "loss": 0.4456,
      "step": 73300
    },
    {
      "epoch": 1.7201031834437475,
      "grad_norm": 0.36344534158706665,
      "learning_rate": 1.3778399109508501e-05,
      "loss": 0.471,
      "step": 73350
    },
    {
      "epoch": 1.721275722577241,
      "grad_norm": 0.5195659399032593,
      "learning_rate": 1.375800378436483e-05,
      "loss": 0.4043,
      "step": 73400
    },
    {
      "epoch": 1.7224482617107346,
      "grad_norm": 0.3069750964641571,
      "learning_rate": 1.3737610770954221e-05,
      "loss": 0.4611,
      "step": 73450
    },
    {
      "epoch": 1.7236208008442282,
      "grad_norm": 0.2806078791618347,
      "learning_rate": 1.371722010723428e-05,
      "loss": 0.4588,
      "step": 73500
    },
    {
      "epoch": 1.7247933399777218,
      "grad_norm": 0.29916656017303467,
      "learning_rate": 1.3696831831158246e-05,
      "loss": 0.441,
      "step": 73550
    },
    {
      "epoch": 1.7259658791112154,
      "grad_norm": 0.37132951617240906,
      "learning_rate": 1.3676445980674902e-05,
      "loss": 0.4592,
      "step": 73600
    },
    {
      "epoch": 1.727138418244709,
      "grad_norm": 0.37835338711738586,
      "learning_rate": 1.3656062593728524e-05,
      "loss": 0.4569,
      "step": 73650
    },
    {
      "epoch": 1.7283109573782025,
      "grad_norm": 0.2887437641620636,
      "learning_rate": 1.3635681708258798e-05,
      "loss": 0.4513,
      "step": 73700
    },
    {
      "epoch": 1.729483496511696,
      "grad_norm": 0.3842609226703644,
      "learning_rate": 1.3615303362200765e-05,
      "loss": 0.4738,
      "step": 73750
    },
    {
      "epoch": 1.7306560356451897,
      "grad_norm": 0.26126646995544434,
      "learning_rate": 1.3594927593484719e-05,
      "loss": 0.4442,
      "step": 73800
    },
    {
      "epoch": 1.7318285747786832,
      "grad_norm": 0.2593865394592285,
      "learning_rate": 1.3574554440036175e-05,
      "loss": 0.4494,
      "step": 73850
    },
    {
      "epoch": 1.7330011139121768,
      "grad_norm": 0.32477983832359314,
      "learning_rate": 1.3554183939775772e-05,
      "loss": 0.481,
      "step": 73900
    },
    {
      "epoch": 1.7341736530456704,
      "grad_norm": 0.24342526495456696,
      "learning_rate": 1.3533816130619214e-05,
      "loss": 0.4678,
      "step": 73950
    },
    {
      "epoch": 1.735346192179164,
      "grad_norm": 0.5205371379852295,
      "learning_rate": 1.3513451050477192e-05,
      "loss": 0.4704,
      "step": 74000
    },
    {
      "epoch": 1.735346192179164,
      "eval_loss": 0.7629601359367371,
      "eval_runtime": 129.164,
      "eval_samples_per_second": 77.421,
      "eval_steps_per_second": 19.355,
      "step": 74000
    },
    {
      "epoch": 1.7365187313126575,
      "grad_norm": 0.23910251259803772,
      "learning_rate": 1.3493088737255326e-05,
      "loss": 0.4587,
      "step": 74050
    },
    {
      "epoch": 1.7376912704461511,
      "grad_norm": 0.35058116912841797,
      "learning_rate": 1.3472729228854072e-05,
      "loss": 0.4211,
      "step": 74100
    },
    {
      "epoch": 1.7388638095796447,
      "grad_norm": 0.18799087405204773,
      "learning_rate": 1.3452372563168678e-05,
      "loss": 0.4449,
      "step": 74150
    },
    {
      "epoch": 1.7400363487131383,
      "grad_norm": 0.46474015712738037,
      "learning_rate": 1.3432018778089096e-05,
      "loss": 0.3898,
      "step": 74200
    },
    {
      "epoch": 1.7412088878466319,
      "grad_norm": 0.39669090509414673,
      "learning_rate": 1.3411667911499915e-05,
      "loss": 0.4662,
      "step": 74250
    },
    {
      "epoch": 1.7423814269801254,
      "grad_norm": 0.3076871633529663,
      "learning_rate": 1.3391320001280292e-05,
      "loss": 0.4802,
      "step": 74300
    },
    {
      "epoch": 1.743553966113619,
      "grad_norm": 0.33568188548088074,
      "learning_rate": 1.3370975085303886e-05,
      "loss": 0.4864,
      "step": 74350
    },
    {
      "epoch": 1.7447265052471126,
      "grad_norm": 0.2559061646461487,
      "learning_rate": 1.3350633201438778e-05,
      "loss": 0.4744,
      "step": 74400
    },
    {
      "epoch": 1.7458990443806062,
      "grad_norm": 0.3001956045627594,
      "learning_rate": 1.3330294387547406e-05,
      "loss": 0.4573,
      "step": 74450
    },
    {
      "epoch": 1.7470715835140997,
      "grad_norm": 0.21523256599903107,
      "learning_rate": 1.3309958681486495e-05,
      "loss": 0.4559,
      "step": 74500
    },
    {
      "epoch": 1.7482441226475933,
      "grad_norm": 0.22073988616466522,
      "learning_rate": 1.3289626121106984e-05,
      "loss": 0.4324,
      "step": 74550
    },
    {
      "epoch": 1.749416661781087,
      "grad_norm": 0.4533173739910126,
      "learning_rate": 1.3269296744253957e-05,
      "loss": 0.4553,
      "step": 74600
    },
    {
      "epoch": 1.7505892009145805,
      "grad_norm": 0.2943035960197449,
      "learning_rate": 1.3248970588766579e-05,
      "loss": 0.4736,
      "step": 74650
    },
    {
      "epoch": 1.751761740048074,
      "grad_norm": 0.36720189452171326,
      "learning_rate": 1.3228647692478004e-05,
      "loss": 0.4247,
      "step": 74700
    },
    {
      "epoch": 1.7529342791815676,
      "grad_norm": 0.5610695481300354,
      "learning_rate": 1.3208328093215337e-05,
      "loss": 0.427,
      "step": 74750
    },
    {
      "epoch": 1.7541068183150612,
      "grad_norm": 0.2670585513114929,
      "learning_rate": 1.3188011828799535e-05,
      "loss": 0.4499,
      "step": 74800
    },
    {
      "epoch": 1.755279357448555,
      "grad_norm": 0.32576265931129456,
      "learning_rate": 1.3167698937045352e-05,
      "loss": 0.4734,
      "step": 74850
    },
    {
      "epoch": 1.7564518965820484,
      "grad_norm": 0.2978537082672119,
      "learning_rate": 1.314738945576126e-05,
      "loss": 0.4516,
      "step": 74900
    },
    {
      "epoch": 1.7576244357155422,
      "grad_norm": 0.28369152545928955,
      "learning_rate": 1.3127083422749392e-05,
      "loss": 0.4832,
      "step": 74950
    },
    {
      "epoch": 1.7587969748490355,
      "grad_norm": 0.37329310178756714,
      "learning_rate": 1.3106780875805457e-05,
      "loss": 0.4428,
      "step": 75000
    },
    {
      "epoch": 1.7599695139825293,
      "grad_norm": 0.4972146451473236,
      "learning_rate": 1.308648185271867e-05,
      "loss": 0.4784,
      "step": 75050
    },
    {
      "epoch": 1.7611420531160227,
      "grad_norm": 0.30108213424682617,
      "learning_rate": 1.3066186391271695e-05,
      "loss": 0.4562,
      "step": 75100
    },
    {
      "epoch": 1.7623145922495165,
      "grad_norm": 0.16729675233364105,
      "learning_rate": 1.3045894529240568e-05,
      "loss": 0.441,
      "step": 75150
    },
    {
      "epoch": 1.7634871313830098,
      "grad_norm": 0.44633427262306213,
      "learning_rate": 1.3025606304394616e-05,
      "loss": 0.508,
      "step": 75200
    },
    {
      "epoch": 1.7646596705165036,
      "grad_norm": 0.28801044821739197,
      "learning_rate": 1.3005321754496408e-05,
      "loss": 0.4361,
      "step": 75250
    },
    {
      "epoch": 1.765832209649997,
      "grad_norm": 0.30074676871299744,
      "learning_rate": 1.2985040917301663e-05,
      "loss": 0.4531,
      "step": 75300
    },
    {
      "epoch": 1.7670047487834908,
      "grad_norm": 0.2923160493373871,
      "learning_rate": 1.2964763830559192e-05,
      "loss": 0.4639,
      "step": 75350
    },
    {
      "epoch": 1.7681772879169841,
      "grad_norm": 0.272928923368454,
      "learning_rate": 1.294449053201083e-05,
      "loss": 0.4337,
      "step": 75400
    },
    {
      "epoch": 1.769349827050478,
      "grad_norm": 0.2755488455295563,
      "learning_rate": 1.2924221059391356e-05,
      "loss": 0.4474,
      "step": 75450
    },
    {
      "epoch": 1.7705223661839713,
      "grad_norm": 0.34998130798339844,
      "learning_rate": 1.2903955450428429e-05,
      "loss": 0.4497,
      "step": 75500
    },
    {
      "epoch": 1.771694905317465,
      "grad_norm": 0.5004467368125916,
      "learning_rate": 1.2883693742842517e-05,
      "loss": 0.439,
      "step": 75550
    },
    {
      "epoch": 1.7728674444509585,
      "grad_norm": 0.2964773178100586,
      "learning_rate": 1.2863435974346826e-05,
      "loss": 0.4503,
      "step": 75600
    },
    {
      "epoch": 1.7740399835844523,
      "grad_norm": 0.5664293766021729,
      "learning_rate": 1.284318218264723e-05,
      "loss": 0.4368,
      "step": 75650
    },
    {
      "epoch": 1.7752125227179456,
      "grad_norm": 0.7109941840171814,
      "learning_rate": 1.2822932405442205e-05,
      "loss": 0.4415,
      "step": 75700
    },
    {
      "epoch": 1.7763850618514394,
      "grad_norm": 0.2940194010734558,
      "learning_rate": 1.2802686680422744e-05,
      "loss": 0.454,
      "step": 75750
    },
    {
      "epoch": 1.7775576009849328,
      "grad_norm": 0.17885173857212067,
      "learning_rate": 1.2782445045272308e-05,
      "loss": 0.4144,
      "step": 75800
    },
    {
      "epoch": 1.7787301401184266,
      "grad_norm": 0.3950788080692291,
      "learning_rate": 1.2762207537666745e-05,
      "loss": 0.4752,
      "step": 75850
    },
    {
      "epoch": 1.77990267925192,
      "grad_norm": 0.23420044779777527,
      "learning_rate": 1.2741974195274217e-05,
      "loss": 0.465,
      "step": 75900
    },
    {
      "epoch": 1.7810752183854137,
      "grad_norm": 0.22936303913593292,
      "learning_rate": 1.272174505575513e-05,
      "loss": 0.469,
      "step": 75950
    },
    {
      "epoch": 1.782247757518907,
      "grad_norm": 0.6195647120475769,
      "learning_rate": 1.2701520156762075e-05,
      "loss": 0.4379,
      "step": 76000
    },
    {
      "epoch": 1.782247757518907,
      "eval_loss": 0.762956440448761,
      "eval_runtime": 162.1063,
      "eval_samples_per_second": 61.688,
      "eval_steps_per_second": 15.422,
      "step": 76000
    },
    {
      "epoch": 1.7834202966524009,
      "grad_norm": 0.24101488292217255,
      "learning_rate": 1.2681299535939744e-05,
      "loss": 0.4812,
      "step": 76050
    },
    {
      "epoch": 1.7845928357858942,
      "grad_norm": 0.41413089632987976,
      "learning_rate": 1.266108323092487e-05,
      "loss": 0.4512,
      "step": 76100
    },
    {
      "epoch": 1.785765374919388,
      "grad_norm": 0.35756754875183105,
      "learning_rate": 1.2640871279346152e-05,
      "loss": 0.4737,
      "step": 76150
    },
    {
      "epoch": 1.7869379140528814,
      "grad_norm": 0.32363462448120117,
      "learning_rate": 1.262066371882418e-05,
      "loss": 0.4472,
      "step": 76200
    },
    {
      "epoch": 1.7881104531863752,
      "grad_norm": 0.3637132942676544,
      "learning_rate": 1.260046058697138e-05,
      "loss": 0.4466,
      "step": 76250
    },
    {
      "epoch": 1.7892829923198685,
      "grad_norm": 0.3399372100830078,
      "learning_rate": 1.2580261921391932e-05,
      "loss": 0.4729,
      "step": 76300
    },
    {
      "epoch": 1.7904555314533623,
      "grad_norm": 0.2458295077085495,
      "learning_rate": 1.2560067759681699e-05,
      "loss": 0.4267,
      "step": 76350
    },
    {
      "epoch": 1.7916280705868557,
      "grad_norm": 0.4583691358566284,
      "learning_rate": 1.2539878139428165e-05,
      "loss": 0.4697,
      "step": 76400
    },
    {
      "epoch": 1.7928006097203495,
      "grad_norm": 0.39060088992118835,
      "learning_rate": 1.2519693098210359e-05,
      "loss": 0.4947,
      "step": 76450
    },
    {
      "epoch": 1.7939731488538428,
      "grad_norm": 0.27615970373153687,
      "learning_rate": 1.2499512673598786e-05,
      "loss": 0.4588,
      "step": 76500
    },
    {
      "epoch": 1.7951456879873366,
      "grad_norm": 0.4519774913787842,
      "learning_rate": 1.2479336903155364e-05,
      "loss": 0.4351,
      "step": 76550
    },
    {
      "epoch": 1.79631822712083,
      "grad_norm": 0.2576690912246704,
      "learning_rate": 1.2459165824433343e-05,
      "loss": 0.4473,
      "step": 76600
    },
    {
      "epoch": 1.7974907662543238,
      "grad_norm": 0.21590787172317505,
      "learning_rate": 1.243899947497724e-05,
      "loss": 0.4166,
      "step": 76650
    },
    {
      "epoch": 1.7986633053878172,
      "grad_norm": 0.3121616244316101,
      "learning_rate": 1.241883789232277e-05,
      "loss": 0.4737,
      "step": 76700
    },
    {
      "epoch": 1.799835844521311,
      "grad_norm": 0.47257253527641296,
      "learning_rate": 1.2398681113996779e-05,
      "loss": 0.4788,
      "step": 76750
    },
    {
      "epoch": 1.8010083836548045,
      "grad_norm": 0.35446637868881226,
      "learning_rate": 1.2378529177517165e-05,
      "loss": 0.4471,
      "step": 76800
    },
    {
      "epoch": 1.8021809227882981,
      "grad_norm": 0.30395787954330444,
      "learning_rate": 1.2358382120392815e-05,
      "loss": 0.477,
      "step": 76850
    },
    {
      "epoch": 1.8033534619217917,
      "grad_norm": 0.38862118124961853,
      "learning_rate": 1.2338239980123543e-05,
      "loss": 0.4663,
      "step": 76900
    },
    {
      "epoch": 1.8045260010552853,
      "grad_norm": 0.5013944506645203,
      "learning_rate": 1.2318102794199998e-05,
      "loss": 0.4872,
      "step": 76950
    },
    {
      "epoch": 1.8056985401887788,
      "grad_norm": 0.3943241834640503,
      "learning_rate": 1.2297970600103617e-05,
      "loss": 0.4574,
      "step": 77000
    },
    {
      "epoch": 1.8068710793222724,
      "grad_norm": 0.2742699086666107,
      "learning_rate": 1.227784343530654e-05,
      "loss": 0.4582,
      "step": 77050
    },
    {
      "epoch": 1.808043618455766,
      "grad_norm": 0.24402005970478058,
      "learning_rate": 1.2257721337271545e-05,
      "loss": 0.4449,
      "step": 77100
    },
    {
      "epoch": 1.8092161575892596,
      "grad_norm": 0.2479785978794098,
      "learning_rate": 1.2237604343451988e-05,
      "loss": 0.4547,
      "step": 77150
    },
    {
      "epoch": 1.8103886967227532,
      "grad_norm": 0.38036638498306274,
      "learning_rate": 1.2217492491291714e-05,
      "loss": 0.4245,
      "step": 77200
    },
    {
      "epoch": 1.8115612358562467,
      "grad_norm": 0.24283212423324585,
      "learning_rate": 1.2197385818225004e-05,
      "loss": 0.4157,
      "step": 77250
    },
    {
      "epoch": 1.8127337749897403,
      "grad_norm": 0.27956628799438477,
      "learning_rate": 1.2177284361676503e-05,
      "loss": 0.4975,
      "step": 77300
    },
    {
      "epoch": 1.8139063141232339,
      "grad_norm": 0.3663233816623688,
      "learning_rate": 1.2157188159061134e-05,
      "loss": 0.4366,
      "step": 77350
    },
    {
      "epoch": 1.8150788532567275,
      "grad_norm": 0.3487664461135864,
      "learning_rate": 1.2137097247784052e-05,
      "loss": 0.4705,
      "step": 77400
    },
    {
      "epoch": 1.816251392390221,
      "grad_norm": 0.2402295619249344,
      "learning_rate": 1.2117011665240557e-05,
      "loss": 0.4517,
      "step": 77450
    },
    {
      "epoch": 1.8174239315237146,
      "grad_norm": 0.3407626152038574,
      "learning_rate": 1.2096931448816034e-05,
      "loss": 0.4316,
      "step": 77500
    },
    {
      "epoch": 1.8185964706572082,
      "grad_norm": 0.39692607522010803,
      "learning_rate": 1.2076856635885881e-05,
      "loss": 0.4489,
      "step": 77550
    },
    {
      "epoch": 1.8197690097907018,
      "grad_norm": 0.8963048458099365,
      "learning_rate": 1.2056787263815432e-05,
      "loss": 0.4599,
      "step": 77600
    },
    {
      "epoch": 1.8209415489241954,
      "grad_norm": 0.5691484808921814,
      "learning_rate": 1.2036723369959902e-05,
      "loss": 0.4483,
      "step": 77650
    },
    {
      "epoch": 1.822114088057689,
      "grad_norm": 0.3848046660423279,
      "learning_rate": 1.2016664991664304e-05,
      "loss": 0.4669,
      "step": 77700
    },
    {
      "epoch": 1.8232866271911825,
      "grad_norm": 0.3754461407661438,
      "learning_rate": 1.1996612166263385e-05,
      "loss": 0.4987,
      "step": 77750
    },
    {
      "epoch": 1.824459166324676,
      "grad_norm": 0.34867623448371887,
      "learning_rate": 1.1976564931081561e-05,
      "loss": 0.4579,
      "step": 77800
    },
    {
      "epoch": 1.8256317054581697,
      "grad_norm": 0.24717669188976288,
      "learning_rate": 1.1956523323432839e-05,
      "loss": 0.4479,
      "step": 77850
    },
    {
      "epoch": 1.8268042445916632,
      "grad_norm": 0.3570568859577179,
      "learning_rate": 1.193648738062075e-05,
      "loss": 0.4289,
      "step": 77900
    },
    {
      "epoch": 1.8279767837251568,
      "grad_norm": 0.3173350393772125,
      "learning_rate": 1.1916457139938285e-05,
      "loss": 0.498,
      "step": 77950
    },
    {
      "epoch": 1.8291493228586504,
      "grad_norm": 0.4886721670627594,
      "learning_rate": 1.189643263866782e-05,
      "loss": 0.4388,
      "step": 78000
    },
    {
      "epoch": 1.8291493228586504,
      "eval_loss": 0.7625876069068909,
      "eval_runtime": 157.9898,
      "eval_samples_per_second": 63.295,
      "eval_steps_per_second": 15.824,
      "step": 78000
    },
    {
      "epoch": 1.830321861992144,
      "grad_norm": 0.2979600727558136,
      "learning_rate": 1.1876413914081046e-05,
      "loss": 0.4516,
      "step": 78050
    },
    {
      "epoch": 1.8314944011256376,
      "grad_norm": 0.2424180954694748,
      "learning_rate": 1.1856401003438906e-05,
      "loss": 0.4696,
      "step": 78100
    },
    {
      "epoch": 1.8326669402591311,
      "grad_norm": 0.41536515951156616,
      "learning_rate": 1.183639394399152e-05,
      "loss": 0.418,
      "step": 78150
    },
    {
      "epoch": 1.8338394793926247,
      "grad_norm": 0.3855496048927307,
      "learning_rate": 1.1816392772978113e-05,
      "loss": 0.497,
      "step": 78200
    },
    {
      "epoch": 1.8350120185261183,
      "grad_norm": 0.3025680184364319,
      "learning_rate": 1.1796397527626958e-05,
      "loss": 0.4668,
      "step": 78250
    },
    {
      "epoch": 1.8361845576596119,
      "grad_norm": 0.19999335706233978,
      "learning_rate": 1.177640824515529e-05,
      "loss": 0.4703,
      "step": 78300
    },
    {
      "epoch": 1.8373570967931054,
      "grad_norm": 0.23542316257953644,
      "learning_rate": 1.175642496276925e-05,
      "loss": 0.4975,
      "step": 78350
    },
    {
      "epoch": 1.838529635926599,
      "grad_norm": 0.3740081489086151,
      "learning_rate": 1.173644771766381e-05,
      "loss": 0.4721,
      "step": 78400
    },
    {
      "epoch": 1.8397021750600926,
      "grad_norm": 0.3636854588985443,
      "learning_rate": 1.1716476547022703e-05,
      "loss": 0.4596,
      "step": 78450
    },
    {
      "epoch": 1.8408747141935862,
      "grad_norm": 0.19594858586788177,
      "learning_rate": 1.1696511488018362e-05,
      "loss": 0.4633,
      "step": 78500
    },
    {
      "epoch": 1.8420472533270797,
      "grad_norm": 0.5151434540748596,
      "learning_rate": 1.1676552577811837e-05,
      "loss": 0.4953,
      "step": 78550
    },
    {
      "epoch": 1.8432197924605733,
      "grad_norm": 0.24194785952568054,
      "learning_rate": 1.1656599853552738e-05,
      "loss": 0.4723,
      "step": 78600
    },
    {
      "epoch": 1.844392331594067,
      "grad_norm": 0.28566309809684753,
      "learning_rate": 1.1636653352379158e-05,
      "loss": 0.4297,
      "step": 78650
    },
    {
      "epoch": 1.8455648707275605,
      "grad_norm": 0.29550105333328247,
      "learning_rate": 1.1616713111417607e-05,
      "loss": 0.4498,
      "step": 78700
    },
    {
      "epoch": 1.846737409861054,
      "grad_norm": 0.5405353307723999,
      "learning_rate": 1.1596779167782944e-05,
      "loss": 0.4279,
      "step": 78750
    },
    {
      "epoch": 1.8479099489945479,
      "grad_norm": 0.38185426592826843,
      "learning_rate": 1.157685155857831e-05,
      "loss": 0.435,
      "step": 78800
    },
    {
      "epoch": 1.8490824881280412,
      "grad_norm": 0.35062262415885925,
      "learning_rate": 1.1556930320895047e-05,
      "loss": 0.4454,
      "step": 78850
    },
    {
      "epoch": 1.850255027261535,
      "grad_norm": 0.3508025109767914,
      "learning_rate": 1.1537015491812648e-05,
      "loss": 0.4688,
      "step": 78900
    },
    {
      "epoch": 1.8514275663950284,
      "grad_norm": 0.26412564516067505,
      "learning_rate": 1.151710710839867e-05,
      "loss": 0.4401,
      "step": 78950
    },
    {
      "epoch": 1.8526001055285222,
      "grad_norm": 0.5229805111885071,
      "learning_rate": 1.1497205207708675e-05,
      "loss": 0.477,
      "step": 79000
    },
    {
      "epoch": 1.8537726446620155,
      "grad_norm": 0.2971090078353882,
      "learning_rate": 1.1477309826786162e-05,
      "loss": 0.433,
      "step": 79050
    },
    {
      "epoch": 1.8549451837955093,
      "grad_norm": 0.18577267229557037,
      "learning_rate": 1.145742100266249e-05,
      "loss": 0.4554,
      "step": 79100
    },
    {
      "epoch": 1.8561177229290027,
      "grad_norm": 0.27104413509368896,
      "learning_rate": 1.1437538772356812e-05,
      "loss": 0.4218,
      "step": 79150
    },
    {
      "epoch": 1.8572902620624965,
      "grad_norm": 0.49735331535339355,
      "learning_rate": 1.1417663172876017e-05,
      "loss": 0.5046,
      "step": 79200
    },
    {
      "epoch": 1.8584628011959898,
      "grad_norm": 0.5241568684577942,
      "learning_rate": 1.1397794241214644e-05,
      "loss": 0.4464,
      "step": 79250
    },
    {
      "epoch": 1.8596353403294836,
      "grad_norm": 0.42546015977859497,
      "learning_rate": 1.1377932014354824e-05,
      "loss": 0.4796,
      "step": 79300
    },
    {
      "epoch": 1.860807879462977,
      "grad_norm": 0.23763683438301086,
      "learning_rate": 1.135807652926621e-05,
      "loss": 0.4143,
      "step": 79350
    },
    {
      "epoch": 1.8619804185964708,
      "grad_norm": 0.23652201890945435,
      "learning_rate": 1.1338227822905895e-05,
      "loss": 0.4642,
      "step": 79400
    },
    {
      "epoch": 1.8631529577299641,
      "grad_norm": 0.44916239380836487,
      "learning_rate": 1.1318385932218375e-05,
      "loss": 0.4318,
      "step": 79450
    },
    {
      "epoch": 1.864325496863458,
      "grad_norm": 0.4212465286254883,
      "learning_rate": 1.1298550894135447e-05,
      "loss": 0.4293,
      "step": 79500
    },
    {
      "epoch": 1.8654980359969513,
      "grad_norm": 0.24460718035697937,
      "learning_rate": 1.1278722745576155e-05,
      "loss": 0.4418,
      "step": 79550
    },
    {
      "epoch": 1.866670575130445,
      "grad_norm": 0.2070329636335373,
      "learning_rate": 1.1258901523446716e-05,
      "loss": 0.444,
      "step": 79600
    },
    {
      "epoch": 1.8678431142639385,
      "grad_norm": 0.42569494247436523,
      "learning_rate": 1.1239087264640462e-05,
      "loss": 0.4467,
      "step": 79650
    },
    {
      "epoch": 1.8690156533974323,
      "grad_norm": 0.4002840518951416,
      "learning_rate": 1.1219280006037759e-05,
      "loss": 0.4526,
      "step": 79700
    },
    {
      "epoch": 1.8701881925309256,
      "grad_norm": 0.29950737953186035,
      "learning_rate": 1.1199479784505948e-05,
      "loss": 0.4511,
      "step": 79750
    },
    {
      "epoch": 1.8713607316644194,
      "grad_norm": 0.4199772775173187,
      "learning_rate": 1.1179686636899266e-05,
      "loss": 0.4763,
      "step": 79800
    },
    {
      "epoch": 1.8725332707979128,
      "grad_norm": 0.2091180980205536,
      "learning_rate": 1.115990060005879e-05,
      "loss": 0.4479,
      "step": 79850
    },
    {
      "epoch": 1.8737058099314066,
      "grad_norm": 0.48436230421066284,
      "learning_rate": 1.1140121710812356e-05,
      "loss": 0.4623,
      "step": 79900
    },
    {
      "epoch": 1.8748783490649,
      "grad_norm": 0.26802894473075867,
      "learning_rate": 1.1120350005974497e-05,
      "loss": 0.4462,
      "step": 79950
    },
    {
      "epoch": 1.8760508881983937,
      "grad_norm": 0.42288756370544434,
      "learning_rate": 1.1100585522346376e-05,
      "loss": 0.4352,
      "step": 80000
    },
    {
      "epoch": 1.8760508881983937,
      "eval_loss": 0.7605867385864258,
      "eval_runtime": 161.3066,
      "eval_samples_per_second": 61.994,
      "eval_steps_per_second": 15.498,
      "step": 80000
    },
    {
      "epoch": 1.877223427331887,
      "grad_norm": 0.3909663259983063,
      "learning_rate": 1.1080828296715715e-05,
      "loss": 0.4017,
      "step": 80050
    },
    {
      "epoch": 1.8783959664653809,
      "grad_norm": 0.41769644618034363,
      "learning_rate": 1.1061078365856725e-05,
      "loss": 0.4388,
      "step": 80100
    },
    {
      "epoch": 1.8795685055988742,
      "grad_norm": 0.3291335701942444,
      "learning_rate": 1.104133576653004e-05,
      "loss": 0.453,
      "step": 80150
    },
    {
      "epoch": 1.880741044732368,
      "grad_norm": 0.1900322139263153,
      "learning_rate": 1.1021600535482645e-05,
      "loss": 0.4916,
      "step": 80200
    },
    {
      "epoch": 1.8819135838658614,
      "grad_norm": 0.3667628765106201,
      "learning_rate": 1.1001872709447821e-05,
      "loss": 0.4395,
      "step": 80250
    },
    {
      "epoch": 1.8830861229993552,
      "grad_norm": 0.35000690817832947,
      "learning_rate": 1.0982152325145047e-05,
      "loss": 0.4481,
      "step": 80300
    },
    {
      "epoch": 1.8842586621328485,
      "grad_norm": 0.32867082953453064,
      "learning_rate": 1.0962439419279971e-05,
      "loss": 0.417,
      "step": 80350
    },
    {
      "epoch": 1.8854312012663423,
      "grad_norm": 0.362968385219574,
      "learning_rate": 1.0942734028544306e-05,
      "loss": 0.4845,
      "step": 80400
    },
    {
      "epoch": 1.8866037403998357,
      "grad_norm": 0.36569324135780334,
      "learning_rate": 1.0923036189615786e-05,
      "loss": 0.4821,
      "step": 80450
    },
    {
      "epoch": 1.8877762795333295,
      "grad_norm": 0.44349637627601624,
      "learning_rate": 1.0903345939158084e-05,
      "loss": 0.4627,
      "step": 80500
    },
    {
      "epoch": 1.8889488186668228,
      "grad_norm": 0.3018398880958557,
      "learning_rate": 1.0883663313820754e-05,
      "loss": 0.4579,
      "step": 80550
    },
    {
      "epoch": 1.8901213578003166,
      "grad_norm": 0.3458937108516693,
      "learning_rate": 1.0863988350239154e-05,
      "loss": 0.4593,
      "step": 80600
    },
    {
      "epoch": 1.89129389693381,
      "grad_norm": 0.22602109611034393,
      "learning_rate": 1.0844321085034376e-05,
      "loss": 0.4071,
      "step": 80650
    },
    {
      "epoch": 1.8924664360673038,
      "grad_norm": 0.3786938786506653,
      "learning_rate": 1.082466155481319e-05,
      "loss": 0.4758,
      "step": 80700
    },
    {
      "epoch": 1.8936389752007974,
      "grad_norm": 0.43444541096687317,
      "learning_rate": 1.080500979616797e-05,
      "loss": 0.4909,
      "step": 80750
    },
    {
      "epoch": 1.894811514334291,
      "grad_norm": 0.34603384137153625,
      "learning_rate": 1.0785365845676616e-05,
      "loss": 0.4185,
      "step": 80800
    },
    {
      "epoch": 1.8959840534677845,
      "grad_norm": 0.5004239082336426,
      "learning_rate": 1.0765729739902506e-05,
      "loss": 0.4468,
      "step": 80850
    },
    {
      "epoch": 1.8971565926012781,
      "grad_norm": 0.4454035758972168,
      "learning_rate": 1.0746101515394406e-05,
      "loss": 0.4484,
      "step": 80900
    },
    {
      "epoch": 1.8983291317347717,
      "grad_norm": 0.2308775782585144,
      "learning_rate": 1.0726481208686419e-05,
      "loss": 0.4177,
      "step": 80950
    },
    {
      "epoch": 1.8995016708682653,
      "grad_norm": 0.4208163022994995,
      "learning_rate": 1.0706868856297912e-05,
      "loss": 0.478,
      "step": 81000
    },
    {
      "epoch": 1.9006742100017588,
      "grad_norm": 0.46860578656196594,
      "learning_rate": 1.0687264494733437e-05,
      "loss": 0.4882,
      "step": 81050
    },
    {
      "epoch": 1.9018467491352524,
      "grad_norm": 0.5030437111854553,
      "learning_rate": 1.0667668160482686e-05,
      "loss": 0.435,
      "step": 81100
    },
    {
      "epoch": 1.903019288268746,
      "grad_norm": 0.3001551628112793,
      "learning_rate": 1.0648079890020398e-05,
      "loss": 0.4377,
      "step": 81150
    },
    {
      "epoch": 1.9041918274022396,
      "grad_norm": 0.30235302448272705,
      "learning_rate": 1.0628499719806309e-05,
      "loss": 0.4732,
      "step": 81200
    },
    {
      "epoch": 1.9053643665357332,
      "grad_norm": 0.33415549993515015,
      "learning_rate": 1.0608927686285077e-05,
      "loss": 0.4693,
      "step": 81250
    },
    {
      "epoch": 1.9065369056692267,
      "grad_norm": 0.3471822142601013,
      "learning_rate": 1.0589363825886215e-05,
      "loss": 0.4738,
      "step": 81300
    },
    {
      "epoch": 1.9077094448027203,
      "grad_norm": 0.21629849076271057,
      "learning_rate": 1.0569808175024024e-05,
      "loss": 0.4683,
      "step": 81350
    },
    {
      "epoch": 1.9088819839362139,
      "grad_norm": 0.36667102575302124,
      "learning_rate": 1.055026077009752e-05,
      "loss": 0.4214,
      "step": 81400
    },
    {
      "epoch": 1.9100545230697075,
      "grad_norm": 0.3748807907104492,
      "learning_rate": 1.0530721647490379e-05,
      "loss": 0.4719,
      "step": 81450
    },
    {
      "epoch": 1.911227062203201,
      "grad_norm": 0.550677478313446,
      "learning_rate": 1.0511190843570855e-05,
      "loss": 0.4514,
      "step": 81500
    },
    {
      "epoch": 1.9123996013366946,
      "grad_norm": 0.22444140911102295,
      "learning_rate": 1.0491668394691721e-05,
      "loss": 0.4124,
      "step": 81550
    },
    {
      "epoch": 1.9135721404701882,
      "grad_norm": 0.19359679520130157,
      "learning_rate": 1.0472154337190195e-05,
      "loss": 0.4355,
      "step": 81600
    },
    {
      "epoch": 1.9147446796036818,
      "grad_norm": 0.3109910786151886,
      "learning_rate": 1.0452648707387875e-05,
      "loss": 0.4474,
      "step": 81650
    },
    {
      "epoch": 1.9159172187371754,
      "grad_norm": 0.28720220923423767,
      "learning_rate": 1.0433151541590686e-05,
      "loss": 0.4388,
      "step": 81700
    },
    {
      "epoch": 1.917089757870669,
      "grad_norm": 0.2818675637245178,
      "learning_rate": 1.0413662876088782e-05,
      "loss": 0.4166,
      "step": 81750
    },
    {
      "epoch": 1.9182622970041625,
      "grad_norm": 0.2971869707107544,
      "learning_rate": 1.0394182747156506e-05,
      "loss": 0.4657,
      "step": 81800
    },
    {
      "epoch": 1.919434836137656,
      "grad_norm": 0.43729689717292786,
      "learning_rate": 1.0374711191052306e-05,
      "loss": 0.4651,
      "step": 81850
    },
    {
      "epoch": 1.9206073752711497,
      "grad_norm": 0.318965882062912,
      "learning_rate": 1.035524824401868e-05,
      "loss": 0.4568,
      "step": 81900
    },
    {
      "epoch": 1.9217799144046432,
      "grad_norm": 0.7569279074668884,
      "learning_rate": 1.0335793942282091e-05,
      "loss": 0.4208,
      "step": 81950
    },
    {
      "epoch": 1.9229524535381368,
      "grad_norm": 0.35163187980651855,
      "learning_rate": 1.031634832205292e-05,
      "loss": 0.4315,
      "step": 82000
    },
    {
      "epoch": 1.9229524535381368,
      "eval_loss": 0.7648668885231018,
      "eval_runtime": 158.6012,
      "eval_samples_per_second": 63.051,
      "eval_steps_per_second": 15.763,
      "step": 82000
    },
    {
      "epoch": 1.9241249926716304,
      "grad_norm": 0.39674997329711914,
      "learning_rate": 1.029691141952539e-05,
      "loss": 0.4364,
      "step": 82050
    },
    {
      "epoch": 1.925297531805124,
      "grad_norm": 0.25116318464279175,
      "learning_rate": 1.027748327087749e-05,
      "loss": 0.453,
      "step": 82100
    },
    {
      "epoch": 1.9264700709386176,
      "grad_norm": 0.4396211504936218,
      "learning_rate": 1.0258063912270918e-05,
      "loss": 0.5063,
      "step": 82150
    },
    {
      "epoch": 1.9276426100721111,
      "grad_norm": 0.3074581027030945,
      "learning_rate": 1.0238653379851014e-05,
      "loss": 0.427,
      "step": 82200
    },
    {
      "epoch": 1.9288151492056047,
      "grad_norm": 0.22403939068317413,
      "learning_rate": 1.021925170974669e-05,
      "loss": 0.4588,
      "step": 82250
    },
    {
      "epoch": 1.9299876883390983,
      "grad_norm": 0.3540157675743103,
      "learning_rate": 1.0199858938070358e-05,
      "loss": 0.4498,
      "step": 82300
    },
    {
      "epoch": 1.9311602274725919,
      "grad_norm": 0.4433228075504303,
      "learning_rate": 1.0180475100917872e-05,
      "loss": 0.4899,
      "step": 82350
    },
    {
      "epoch": 1.9323327666060854,
      "grad_norm": 0.2593497633934021,
      "learning_rate": 1.0161100234368452e-05,
      "loss": 0.4616,
      "step": 82400
    },
    {
      "epoch": 1.933505305739579,
      "grad_norm": 0.3603302538394928,
      "learning_rate": 1.0141734374484624e-05,
      "loss": 0.4188,
      "step": 82450
    },
    {
      "epoch": 1.9346778448730726,
      "grad_norm": 0.2802298963069916,
      "learning_rate": 1.012237755731215e-05,
      "loss": 0.4177,
      "step": 82500
    },
    {
      "epoch": 1.9358503840065662,
      "grad_norm": 0.38962066173553467,
      "learning_rate": 1.0103029818879955e-05,
      "loss": 0.4194,
      "step": 82550
    },
    {
      "epoch": 1.9370229231400597,
      "grad_norm": 0.24070575833320618,
      "learning_rate": 1.0083691195200076e-05,
      "loss": 0.4508,
      "step": 82600
    },
    {
      "epoch": 1.9381954622735533,
      "grad_norm": 0.424652636051178,
      "learning_rate": 1.0064361722267577e-05,
      "loss": 0.4664,
      "step": 82650
    },
    {
      "epoch": 1.939368001407047,
      "grad_norm": 0.3242700397968292,
      "learning_rate": 1.004504143606049e-05,
      "loss": 0.4588,
      "step": 82700
    },
    {
      "epoch": 1.9405405405405407,
      "grad_norm": 0.3591204285621643,
      "learning_rate": 1.002573037253975e-05,
      "loss": 0.485,
      "step": 82750
    },
    {
      "epoch": 1.941713079674034,
      "grad_norm": 0.4061959385871887,
      "learning_rate": 1.0006428567649133e-05,
      "loss": 0.4399,
      "step": 82800
    },
    {
      "epoch": 1.9428856188075279,
      "grad_norm": 0.27594926953315735,
      "learning_rate": 9.98713605731516e-06,
      "loss": 0.4301,
      "step": 82850
    },
    {
      "epoch": 1.9440581579410212,
      "grad_norm": 0.37723439931869507,
      "learning_rate": 9.967852877447075e-06,
      "loss": 0.4572,
      "step": 82900
    },
    {
      "epoch": 1.945230697074515,
      "grad_norm": 0.4903812110424042,
      "learning_rate": 9.948579063936745e-06,
      "loss": 0.4502,
      "step": 82950
    },
    {
      "epoch": 1.9464032362080084,
      "grad_norm": 0.6072514057159424,
      "learning_rate": 9.929314652658595e-06,
      "loss": 0.4933,
      "step": 83000
    },
    {
      "epoch": 1.9475757753415022,
      "grad_norm": 0.43858715891838074,
      "learning_rate": 9.910059679469574e-06,
      "loss": 0.4673,
      "step": 83050
    },
    {
      "epoch": 1.9487483144749955,
      "grad_norm": 0.3350450396537781,
      "learning_rate": 9.890814180209046e-06,
      "loss": 0.4468,
      "step": 83100
    },
    {
      "epoch": 1.9499208536084893,
      "grad_norm": 0.2126716524362564,
      "learning_rate": 9.871578190698738e-06,
      "loss": 0.4347,
      "step": 83150
    },
    {
      "epoch": 1.9510933927419827,
      "grad_norm": 0.4271620213985443,
      "learning_rate": 9.852351746742688e-06,
      "loss": 0.4461,
      "step": 83200
    },
    {
      "epoch": 1.9522659318754765,
      "grad_norm": 0.28770601749420166,
      "learning_rate": 9.833134884127158e-06,
      "loss": 0.4247,
      "step": 83250
    },
    {
      "epoch": 1.9534384710089698,
      "grad_norm": 0.37741824984550476,
      "learning_rate": 9.81392763862058e-06,
      "loss": 0.4266,
      "step": 83300
    },
    {
      "epoch": 1.9546110101424636,
      "grad_norm": 0.30372291803359985,
      "learning_rate": 9.794730045973486e-06,
      "loss": 0.4266,
      "step": 83350
    },
    {
      "epoch": 1.955783549275957,
      "grad_norm": 0.2702624201774597,
      "learning_rate": 9.77554214191844e-06,
      "loss": 0.4586,
      "step": 83400
    },
    {
      "epoch": 1.9569560884094508,
      "grad_norm": 0.4465273916721344,
      "learning_rate": 9.756363962169974e-06,
      "loss": 0.5037,
      "step": 83450
    },
    {
      "epoch": 1.9581286275429441,
      "grad_norm": 0.4019373655319214,
      "learning_rate": 9.73719554242452e-06,
      "loss": 0.4025,
      "step": 83500
    },
    {
      "epoch": 1.959301166676438,
      "grad_norm": 0.35712930560112,
      "learning_rate": 9.718036918360339e-06,
      "loss": 0.4623,
      "step": 83550
    },
    {
      "epoch": 1.9604737058099313,
      "grad_norm": 0.44834113121032715,
      "learning_rate": 9.698888125637465e-06,
      "loss": 0.4539,
      "step": 83600
    },
    {
      "epoch": 1.961646244943425,
      "grad_norm": 0.23151154816150665,
      "learning_rate": 9.679749199897631e-06,
      "loss": 0.473,
      "step": 83650
    },
    {
      "epoch": 1.9628187840769185,
      "grad_norm": 0.325586199760437,
      "learning_rate": 9.660620176764202e-06,
      "loss": 0.4472,
      "step": 83700
    },
    {
      "epoch": 1.9639913232104123,
      "grad_norm": 0.34941020607948303,
      "learning_rate": 9.641501091842114e-06,
      "loss": 0.4826,
      "step": 83750
    },
    {
      "epoch": 1.9651638623439056,
      "grad_norm": 0.21318842470645905,
      "learning_rate": 9.62239198071781e-06,
      "loss": 0.4592,
      "step": 83800
    },
    {
      "epoch": 1.9663364014773994,
      "grad_norm": 0.3670065402984619,
      "learning_rate": 9.603292878959157e-06,
      "loss": 0.4542,
      "step": 83850
    },
    {
      "epoch": 1.9675089406108928,
      "grad_norm": 0.554757297039032,
      "learning_rate": 9.5842038221154e-06,
      "loss": 0.458,
      "step": 83900
    },
    {
      "epoch": 1.9686814797443866,
      "grad_norm": 0.4273451864719391,
      "learning_rate": 9.565124845717083e-06,
      "loss": 0.4623,
      "step": 83950
    },
    {
      "epoch": 1.96985401887788,
      "grad_norm": 0.7358681559562683,
      "learning_rate": 9.546055985275997e-06,
      "loss": 0.4514,
      "step": 84000
    },
    {
      "epoch": 1.96985401887788,
      "eval_loss": 0.7629541754722595,
      "eval_runtime": 159.9377,
      "eval_samples_per_second": 62.524,
      "eval_steps_per_second": 15.631,
      "step": 84000
    },
    {
      "epoch": 1.9710265580113737,
      "grad_norm": 0.15931804478168488,
      "learning_rate": 9.526997276285087e-06,
      "loss": 0.4599,
      "step": 84050
    },
    {
      "epoch": 1.972199097144867,
      "grad_norm": 0.38301706314086914,
      "learning_rate": 9.507948754218418e-06,
      "loss": 0.4515,
      "step": 84100
    },
    {
      "epoch": 1.9733716362783609,
      "grad_norm": 0.24245299398899078,
      "learning_rate": 9.488910454531085e-06,
      "loss": 0.4967,
      "step": 84150
    },
    {
      "epoch": 1.9745441754118542,
      "grad_norm": 0.3664095401763916,
      "learning_rate": 9.469882412659166e-06,
      "loss": 0.4476,
      "step": 84200
    },
    {
      "epoch": 1.975716714545348,
      "grad_norm": 0.42921537160873413,
      "learning_rate": 9.450864664019634e-06,
      "loss": 0.4981,
      "step": 84250
    },
    {
      "epoch": 1.9768892536788414,
      "grad_norm": 0.32359257340431213,
      "learning_rate": 9.431857244010308e-06,
      "loss": 0.474,
      "step": 84300
    },
    {
      "epoch": 1.9780617928123352,
      "grad_norm": 0.3880968391895294,
      "learning_rate": 9.412860188009788e-06,
      "loss": 0.4488,
      "step": 84350
    },
    {
      "epoch": 1.9792343319458285,
      "grad_norm": 0.21762703359127045,
      "learning_rate": 9.393873531377376e-06,
      "loss": 0.4841,
      "step": 84400
    },
    {
      "epoch": 1.9804068710793223,
      "grad_norm": 0.26809537410736084,
      "learning_rate": 9.374897309453019e-06,
      "loss": 0.4869,
      "step": 84450
    },
    {
      "epoch": 1.9815794102128157,
      "grad_norm": 0.16927438974380493,
      "learning_rate": 9.355931557557247e-06,
      "loss": 0.4519,
      "step": 84500
    },
    {
      "epoch": 1.9827519493463095,
      "grad_norm": 0.4275870621204376,
      "learning_rate": 9.336976310991092e-06,
      "loss": 0.4599,
      "step": 84550
    },
    {
      "epoch": 1.9839244884798029,
      "grad_norm": 0.1649009883403778,
      "learning_rate": 9.318031605036045e-06,
      "loss": 0.426,
      "step": 84600
    },
    {
      "epoch": 1.9850970276132966,
      "grad_norm": 0.3543689250946045,
      "learning_rate": 9.299097474953966e-06,
      "loss": 0.4743,
      "step": 84650
    },
    {
      "epoch": 1.9862695667467902,
      "grad_norm": 0.22711028158664703,
      "learning_rate": 9.280173955987038e-06,
      "loss": 0.4618,
      "step": 84700
    },
    {
      "epoch": 1.9874421058802838,
      "grad_norm": 0.24647513031959534,
      "learning_rate": 9.261261083357692e-06,
      "loss": 0.4619,
      "step": 84750
    },
    {
      "epoch": 1.9886146450137774,
      "grad_norm": 0.360821008682251,
      "learning_rate": 9.242358892268542e-06,
      "loss": 0.4565,
      "step": 84800
    },
    {
      "epoch": 1.989787184147271,
      "grad_norm": 0.31492096185684204,
      "learning_rate": 9.223467417902314e-06,
      "loss": 0.426,
      "step": 84850
    },
    {
      "epoch": 1.9909597232807645,
      "grad_norm": 0.26210448145866394,
      "learning_rate": 9.204586695421798e-06,
      "loss": 0.4577,
      "step": 84900
    },
    {
      "epoch": 1.9921322624142581,
      "grad_norm": 0.3207606375217438,
      "learning_rate": 9.185716759969766e-06,
      "loss": 0.444,
      "step": 84950
    },
    {
      "epoch": 1.9933048015477517,
      "grad_norm": 0.3531254529953003,
      "learning_rate": 9.166857646668912e-06,
      "loss": 0.4725,
      "step": 85000
    },
    {
      "epoch": 1.9944773406812453,
      "grad_norm": 0.18566837906837463,
      "learning_rate": 9.148009390621786e-06,
      "loss": 0.4666,
      "step": 85050
    },
    {
      "epoch": 1.9956498798147388,
      "grad_norm": 0.35070276260375977,
      "learning_rate": 9.129172026910734e-06,
      "loss": 0.4466,
      "step": 85100
    },
    {
      "epoch": 1.9968224189482324,
      "grad_norm": 0.29211169481277466,
      "learning_rate": 9.11034559059782e-06,
      "loss": 0.4691,
      "step": 85150
    },
    {
      "epoch": 1.997994958081726,
      "grad_norm": 0.20703741908073425,
      "learning_rate": 9.091530116724777e-06,
      "loss": 0.437,
      "step": 85200
    },
    {
      "epoch": 1.9991674972152196,
      "grad_norm": 0.2978875935077667,
      "learning_rate": 9.072725640312927e-06,
      "loss": 0.4354,
      "step": 85250
    },
    {
      "epoch": 2.000351761740048,
      "grad_norm": 0.26158928871154785,
      "learning_rate": 9.05393219636313e-06,
      "loss": 0.4504,
      "step": 85300
    },
    {
      "epoch": 2.0015243008735415,
      "grad_norm": 0.47947973012924194,
      "learning_rate": 9.035149819855702e-06,
      "loss": 0.4331,
      "step": 85350
    },
    {
      "epoch": 2.0026968400070353,
      "grad_norm": 0.24041695892810822,
      "learning_rate": 9.016378545750367e-06,
      "loss": 0.436,
      "step": 85400
    },
    {
      "epoch": 2.0038693791405287,
      "grad_norm": 0.1976260393857956,
      "learning_rate": 8.997618408986178e-06,
      "loss": 0.4226,
      "step": 85450
    },
    {
      "epoch": 2.0050419182740224,
      "grad_norm": 0.32467955350875854,
      "learning_rate": 8.97886944448146e-06,
      "loss": 0.444,
      "step": 85500
    },
    {
      "epoch": 2.006214457407516,
      "grad_norm": 0.31893426179885864,
      "learning_rate": 8.960131687133751e-06,
      "loss": 0.4717,
      "step": 85550
    },
    {
      "epoch": 2.0073869965410096,
      "grad_norm": 0.34007930755615234,
      "learning_rate": 8.941405171819711e-06,
      "loss": 0.4473,
      "step": 85600
    },
    {
      "epoch": 2.008559535674503,
      "grad_norm": 0.33568039536476135,
      "learning_rate": 8.922689933395095e-06,
      "loss": 0.4387,
      "step": 85650
    },
    {
      "epoch": 2.0097320748079968,
      "grad_norm": 0.3825854957103729,
      "learning_rate": 8.903986006694655e-06,
      "loss": 0.4628,
      "step": 85700
    },
    {
      "epoch": 2.01090461394149,
      "grad_norm": 0.33853140473365784,
      "learning_rate": 8.88529342653209e-06,
      "loss": 0.446,
      "step": 85750
    },
    {
      "epoch": 2.012077153074984,
      "grad_norm": 0.29621219635009766,
      "learning_rate": 8.86661222769999e-06,
      "loss": 0.4334,
      "step": 85800
    },
    {
      "epoch": 2.0132496922084773,
      "grad_norm": 0.36291664838790894,
      "learning_rate": 8.847942444969748e-06,
      "loss": 0.485,
      "step": 85850
    },
    {
      "epoch": 2.014422231341971,
      "grad_norm": 0.5606473088264465,
      "learning_rate": 8.829284113091514e-06,
      "loss": 0.4329,
      "step": 85900
    },
    {
      "epoch": 2.0155947704754644,
      "grad_norm": 0.39065688848495483,
      "learning_rate": 8.810637266794124e-06,
      "loss": 0.4499,
      "step": 85950
    },
    {
      "epoch": 2.0167673096089582,
      "grad_norm": 0.38148653507232666,
      "learning_rate": 8.792001940785037e-06,
      "loss": 0.4705,
      "step": 86000
    },
    {
      "epoch": 2.0167673096089582,
      "eval_loss": 0.7627174854278564,
      "eval_runtime": 159.5225,
      "eval_samples_per_second": 62.687,
      "eval_steps_per_second": 15.672,
      "step": 86000
    },
    {
      "epoch": 2.0179398487424516,
      "grad_norm": 0.45129138231277466,
      "learning_rate": 8.773378169750266e-06,
      "loss": 0.4448,
      "step": 86050
    },
    {
      "epoch": 2.0191123878759454,
      "grad_norm": 0.26646432280540466,
      "learning_rate": 8.75476598835432e-06,
      "loss": 0.4507,
      "step": 86100
    },
    {
      "epoch": 2.0202849270094387,
      "grad_norm": 0.48937252163887024,
      "learning_rate": 8.736165431240137e-06,
      "loss": 0.4624,
      "step": 86150
    },
    {
      "epoch": 2.0214574661429325,
      "grad_norm": 0.23556287586688995,
      "learning_rate": 8.717576533029008e-06,
      "loss": 0.4553,
      "step": 86200
    },
    {
      "epoch": 2.022630005276426,
      "grad_norm": 0.21500803530216217,
      "learning_rate": 8.698999328320545e-06,
      "loss": 0.4386,
      "step": 86250
    },
    {
      "epoch": 2.0238025444099197,
      "grad_norm": 0.35770183801651,
      "learning_rate": 8.680433851692565e-06,
      "loss": 0.4595,
      "step": 86300
    },
    {
      "epoch": 2.024975083543413,
      "grad_norm": 0.23286369442939758,
      "learning_rate": 8.661880137701089e-06,
      "loss": 0.4355,
      "step": 86350
    },
    {
      "epoch": 2.026147622676907,
      "grad_norm": 0.19819292426109314,
      "learning_rate": 8.643338220880212e-06,
      "loss": 0.4836,
      "step": 86400
    },
    {
      "epoch": 2.0273201618104,
      "grad_norm": 0.3371947109699249,
      "learning_rate": 8.624808135742098e-06,
      "loss": 0.4117,
      "step": 86450
    },
    {
      "epoch": 2.028492700943894,
      "grad_norm": 0.35646477341651917,
      "learning_rate": 8.606289916776868e-06,
      "loss": 0.5016,
      "step": 86500
    },
    {
      "epoch": 2.029665240077388,
      "grad_norm": 0.2500779330730438,
      "learning_rate": 8.587783598452575e-06,
      "loss": 0.4404,
      "step": 86550
    },
    {
      "epoch": 2.030837779210881,
      "grad_norm": 0.24350285530090332,
      "learning_rate": 8.569289215215098e-06,
      "loss": 0.4548,
      "step": 86600
    },
    {
      "epoch": 2.032010318344375,
      "grad_norm": 0.3422778248786926,
      "learning_rate": 8.55080680148812e-06,
      "loss": 0.4647,
      "step": 86650
    },
    {
      "epoch": 2.0331828574778683,
      "grad_norm": 0.334005206823349,
      "learning_rate": 8.53233639167305e-06,
      "loss": 0.431,
      "step": 86700
    },
    {
      "epoch": 2.034355396611362,
      "grad_norm": 0.274249792098999,
      "learning_rate": 8.513878020148928e-06,
      "loss": 0.4203,
      "step": 86750
    },
    {
      "epoch": 2.0355279357448555,
      "grad_norm": 0.2774704098701477,
      "learning_rate": 8.495431721272415e-06,
      "loss": 0.4409,
      "step": 86800
    },
    {
      "epoch": 2.0367004748783493,
      "grad_norm": 0.32122790813446045,
      "learning_rate": 8.47699752937768e-06,
      "loss": 0.4567,
      "step": 86850
    },
    {
      "epoch": 2.0378730140118426,
      "grad_norm": 0.2655850946903229,
      "learning_rate": 8.458575478776373e-06,
      "loss": 0.4355,
      "step": 86900
    },
    {
      "epoch": 2.0390455531453364,
      "grad_norm": 0.3449765741825104,
      "learning_rate": 8.440165603757531e-06,
      "loss": 0.4581,
      "step": 86950
    },
    {
      "epoch": 2.0402180922788298,
      "grad_norm": 0.3211236894130707,
      "learning_rate": 8.421767938587545e-06,
      "loss": 0.4695,
      "step": 87000
    },
    {
      "epoch": 2.0413906314123236,
      "grad_norm": 0.26171210408210754,
      "learning_rate": 8.40338251751006e-06,
      "loss": 0.4762,
      "step": 87050
    },
    {
      "epoch": 2.042563170545817,
      "grad_norm": 0.36472463607788086,
      "learning_rate": 8.385009374745956e-06,
      "loss": 0.4673,
      "step": 87100
    },
    {
      "epoch": 2.0437357096793107,
      "grad_norm": 0.6326845288276672,
      "learning_rate": 8.36664854449323e-06,
      "loss": 0.4608,
      "step": 87150
    },
    {
      "epoch": 2.044908248812804,
      "grad_norm": 0.2893472909927368,
      "learning_rate": 8.34830006092699e-06,
      "loss": 0.4712,
      "step": 87200
    },
    {
      "epoch": 2.046080787946298,
      "grad_norm": 0.445681631565094,
      "learning_rate": 8.329963958199335e-06,
      "loss": 0.4241,
      "step": 87250
    },
    {
      "epoch": 2.0472533270797912,
      "grad_norm": 0.2686435282230377,
      "learning_rate": 8.311640270439349e-06,
      "loss": 0.4464,
      "step": 87300
    },
    {
      "epoch": 2.048425866213285,
      "grad_norm": 0.3412652611732483,
      "learning_rate": 8.293329031752983e-06,
      "loss": 0.4673,
      "step": 87350
    },
    {
      "epoch": 2.0495984053467784,
      "grad_norm": 0.400246262550354,
      "learning_rate": 8.275030276223036e-06,
      "loss": 0.4495,
      "step": 87400
    },
    {
      "epoch": 2.050770944480272,
      "grad_norm": 0.3469572961330414,
      "learning_rate": 8.25674403790906e-06,
      "loss": 0.4359,
      "step": 87450
    },
    {
      "epoch": 2.0519434836137656,
      "grad_norm": 0.24249623715877533,
      "learning_rate": 8.238470350847302e-06,
      "loss": 0.4363,
      "step": 87500
    },
    {
      "epoch": 2.0531160227472594,
      "grad_norm": 0.3649761378765106,
      "learning_rate": 8.220209249050667e-06,
      "loss": 0.4203,
      "step": 87550
    },
    {
      "epoch": 2.0542885618807527,
      "grad_norm": 0.3245883584022522,
      "learning_rate": 8.20196076650862e-06,
      "loss": 0.4465,
      "step": 87600
    },
    {
      "epoch": 2.0554611010142465,
      "grad_norm": 0.34245818853378296,
      "learning_rate": 8.183724937187146e-06,
      "loss": 0.4246,
      "step": 87650
    },
    {
      "epoch": 2.05663364014774,
      "grad_norm": 0.24231800436973572,
      "learning_rate": 8.16550179502867e-06,
      "loss": 0.4611,
      "step": 87700
    },
    {
      "epoch": 2.0578061792812337,
      "grad_norm": 0.3940911889076233,
      "learning_rate": 8.147291373952015e-06,
      "loss": 0.4418,
      "step": 87750
    },
    {
      "epoch": 2.058978718414727,
      "grad_norm": 0.4142378270626068,
      "learning_rate": 8.129093707852309e-06,
      "loss": 0.4408,
      "step": 87800
    },
    {
      "epoch": 2.060151257548221,
      "grad_norm": 0.43434441089630127,
      "learning_rate": 8.11090883060096e-06,
      "loss": 0.4778,
      "step": 87850
    },
    {
      "epoch": 2.061323796681714,
      "grad_norm": 0.23592574894428253,
      "learning_rate": 8.092736776045547e-06,
      "loss": 0.4459,
      "step": 87900
    },
    {
      "epoch": 2.062496335815208,
      "grad_norm": 0.3174763321876526,
      "learning_rate": 8.074577578009809e-06,
      "loss": 0.4728,
      "step": 87950
    },
    {
      "epoch": 2.0636688749487013,
      "grad_norm": 0.44223281741142273,
      "learning_rate": 8.056431270293529e-06,
      "loss": 0.4587,
      "step": 88000
    },
    {
      "epoch": 2.0636688749487013,
      "eval_loss": 0.7614777088165283,
      "eval_runtime": 159.8253,
      "eval_samples_per_second": 62.568,
      "eval_steps_per_second": 15.642,
      "step": 88000
    },
    {
      "epoch": 2.064841414082195,
      "grad_norm": 0.21628330647945404,
      "learning_rate": 8.038297886672525e-06,
      "loss": 0.4118,
      "step": 88050
    },
    {
      "epoch": 2.0660139532156885,
      "grad_norm": 0.7282050848007202,
      "learning_rate": 8.020177460898528e-06,
      "loss": 0.4521,
      "step": 88100
    },
    {
      "epoch": 2.0671864923491823,
      "grad_norm": 0.3242964744567871,
      "learning_rate": 8.002070026699184e-06,
      "loss": 0.4629,
      "step": 88150
    },
    {
      "epoch": 2.0683590314826756,
      "grad_norm": 0.34303683042526245,
      "learning_rate": 7.983975617777922e-06,
      "loss": 0.4499,
      "step": 88200
    },
    {
      "epoch": 2.0695315706161694,
      "grad_norm": 0.3408103585243225,
      "learning_rate": 7.965894267813964e-06,
      "loss": 0.4627,
      "step": 88250
    },
    {
      "epoch": 2.070704109749663,
      "grad_norm": 0.2251395881175995,
      "learning_rate": 7.94782601046219e-06,
      "loss": 0.4551,
      "step": 88300
    },
    {
      "epoch": 2.0718766488831566,
      "grad_norm": 0.33662348985671997,
      "learning_rate": 7.929770879353144e-06,
      "loss": 0.4086,
      "step": 88350
    },
    {
      "epoch": 2.07304918801665,
      "grad_norm": 0.7235216498374939,
      "learning_rate": 7.911728908092905e-06,
      "loss": 0.456,
      "step": 88400
    },
    {
      "epoch": 2.0742217271501437,
      "grad_norm": 0.1832900047302246,
      "learning_rate": 7.893700130263088e-06,
      "loss": 0.4761,
      "step": 88450
    },
    {
      "epoch": 2.075394266283637,
      "grad_norm": 0.4607270658016205,
      "learning_rate": 7.875684579420727e-06,
      "loss": 0.4993,
      "step": 88500
    },
    {
      "epoch": 2.076566805417131,
      "grad_norm": 0.3121365010738373,
      "learning_rate": 7.857682289098257e-06,
      "loss": 0.4468,
      "step": 88550
    },
    {
      "epoch": 2.0777393445506243,
      "grad_norm": 0.3902507722377777,
      "learning_rate": 7.839693292803406e-06,
      "loss": 0.4839,
      "step": 88600
    },
    {
      "epoch": 2.078911883684118,
      "grad_norm": 0.3028443455696106,
      "learning_rate": 7.82171762401919e-06,
      "loss": 0.4634,
      "step": 88650
    },
    {
      "epoch": 2.0800844228176114,
      "grad_norm": 0.3093830943107605,
      "learning_rate": 7.803755316203793e-06,
      "loss": 0.4648,
      "step": 88700
    },
    {
      "epoch": 2.081256961951105,
      "grad_norm": 0.32980602979660034,
      "learning_rate": 7.78580640279053e-06,
      "loss": 0.4564,
      "step": 88750
    },
    {
      "epoch": 2.0824295010845986,
      "grad_norm": 0.374990850687027,
      "learning_rate": 7.767870917187808e-06,
      "loss": 0.4517,
      "step": 88800
    },
    {
      "epoch": 2.0836020402180924,
      "grad_norm": 0.40672022104263306,
      "learning_rate": 7.749948892779011e-06,
      "loss": 0.4879,
      "step": 88850
    },
    {
      "epoch": 2.0847745793515857,
      "grad_norm": 0.27554380893707275,
      "learning_rate": 7.732040362922497e-06,
      "loss": 0.3999,
      "step": 88900
    },
    {
      "epoch": 2.0859471184850795,
      "grad_norm": 0.29506582021713257,
      "learning_rate": 7.714145360951479e-06,
      "loss": 0.4299,
      "step": 88950
    },
    {
      "epoch": 2.087119657618573,
      "grad_norm": 0.26039546728134155,
      "learning_rate": 7.696263920174017e-06,
      "loss": 0.4614,
      "step": 89000
    },
    {
      "epoch": 2.0882921967520667,
      "grad_norm": 0.4444635510444641,
      "learning_rate": 7.678396073872903e-06,
      "loss": 0.4768,
      "step": 89050
    },
    {
      "epoch": 2.08946473588556,
      "grad_norm": 0.2505185902118683,
      "learning_rate": 7.660541855305646e-06,
      "loss": 0.4648,
      "step": 89100
    },
    {
      "epoch": 2.090637275019054,
      "grad_norm": 0.38653564453125,
      "learning_rate": 7.64270129770438e-06,
      "loss": 0.4768,
      "step": 89150
    },
    {
      "epoch": 2.091809814152547,
      "grad_norm": 0.2899700403213501,
      "learning_rate": 7.624874434275818e-06,
      "loss": 0.4768,
      "step": 89200
    },
    {
      "epoch": 2.092982353286041,
      "grad_norm": 0.29788777232170105,
      "learning_rate": 7.60706129820117e-06,
      "loss": 0.4169,
      "step": 89250
    },
    {
      "epoch": 2.0941548924195343,
      "grad_norm": 0.2708686888217926,
      "learning_rate": 7.589261922636118e-06,
      "loss": 0.4605,
      "step": 89300
    },
    {
      "epoch": 2.095327431553028,
      "grad_norm": 0.3336002230644226,
      "learning_rate": 7.571476340710708e-06,
      "loss": 0.4554,
      "step": 89350
    },
    {
      "epoch": 2.0964999706865215,
      "grad_norm": 0.3240853250026703,
      "learning_rate": 7.553704585529332e-06,
      "loss": 0.4275,
      "step": 89400
    },
    {
      "epoch": 2.0976725098200153,
      "grad_norm": 0.3731333315372467,
      "learning_rate": 7.535946690170624e-06,
      "loss": 0.4644,
      "step": 89450
    },
    {
      "epoch": 2.0988450489535087,
      "grad_norm": 0.40814247727394104,
      "learning_rate": 7.518202687687447e-06,
      "loss": 0.4488,
      "step": 89500
    },
    {
      "epoch": 2.1000175880870025,
      "grad_norm": 0.22332224249839783,
      "learning_rate": 7.50047261110678e-06,
      "loss": 0.4615,
      "step": 89550
    },
    {
      "epoch": 2.101190127220496,
      "grad_norm": 0.18113769590854645,
      "learning_rate": 7.482756493429704e-06,
      "loss": 0.4214,
      "step": 89600
    },
    {
      "epoch": 2.1023626663539896,
      "grad_norm": 0.30220240354537964,
      "learning_rate": 7.465054367631298e-06,
      "loss": 0.4153,
      "step": 89650
    },
    {
      "epoch": 2.103535205487483,
      "grad_norm": 0.32522326707839966,
      "learning_rate": 7.447366266660615e-06,
      "loss": 0.4663,
      "step": 89700
    },
    {
      "epoch": 2.1047077446209768,
      "grad_norm": 0.2943207025527954,
      "learning_rate": 7.429692223440586e-06,
      "loss": 0.4374,
      "step": 89750
    },
    {
      "epoch": 2.10588028375447,
      "grad_norm": 0.14197388291358948,
      "learning_rate": 7.4120322708679994e-06,
      "loss": 0.4516,
      "step": 89800
    },
    {
      "epoch": 2.107052822887964,
      "grad_norm": 0.4026564061641693,
      "learning_rate": 7.39438644181339e-06,
      "loss": 0.4743,
      "step": 89850
    },
    {
      "epoch": 2.1082253620214573,
      "grad_norm": 0.20716887712478638,
      "learning_rate": 7.376754769121027e-06,
      "loss": 0.4819,
      "step": 89900
    },
    {
      "epoch": 2.109397901154951,
      "grad_norm": 0.5253917574882507,
      "learning_rate": 7.3591372856088134e-06,
      "loss": 0.4388,
      "step": 89950
    },
    {
      "epoch": 2.1105704402884444,
      "grad_norm": 0.3228725492954254,
      "learning_rate": 7.341534024068259e-06,
      "loss": 0.4502,
      "step": 90000
    },
    {
      "epoch": 2.1105704402884444,
      "eval_loss": 0.7609566450119019,
      "eval_runtime": 187.9301,
      "eval_samples_per_second": 53.211,
      "eval_steps_per_second": 13.303,
      "step": 90000
    },
    {
      "epoch": 2.1117429794219382,
      "grad_norm": 0.5566502213478088,
      "learning_rate": 7.3239450172643865e-06,
      "loss": 0.4525,
      "step": 90050
    },
    {
      "epoch": 2.1129155185554316,
      "grad_norm": 0.3188021183013916,
      "learning_rate": 7.306370297935688e-06,
      "loss": 0.47,
      "step": 90100
    },
    {
      "epoch": 2.1140880576889254,
      "grad_norm": 0.348750501871109,
      "learning_rate": 7.288809898794076e-06,
      "loss": 0.4242,
      "step": 90150
    },
    {
      "epoch": 2.1152605968224187,
      "grad_norm": 0.5090613961219788,
      "learning_rate": 7.271263852524784e-06,
      "loss": 0.4926,
      "step": 90200
    },
    {
      "epoch": 2.1164331359559125,
      "grad_norm": 0.40931862592697144,
      "learning_rate": 7.2537321917863685e-06,
      "loss": 0.4528,
      "step": 90250
    },
    {
      "epoch": 2.117605675089406,
      "grad_norm": 0.39700812101364136,
      "learning_rate": 7.236214949210568e-06,
      "loss": 0.4244,
      "step": 90300
    },
    {
      "epoch": 2.1187782142228997,
      "grad_norm": 0.3441408574581146,
      "learning_rate": 7.218712157402318e-06,
      "loss": 0.4421,
      "step": 90350
    },
    {
      "epoch": 2.1199507533563935,
      "grad_norm": 0.3230225145816803,
      "learning_rate": 7.201223848939628e-06,
      "loss": 0.4356,
      "step": 90400
    },
    {
      "epoch": 2.121123292489887,
      "grad_norm": 0.279313862323761,
      "learning_rate": 7.183750056373578e-06,
      "loss": 0.4437,
      "step": 90450
    },
    {
      "epoch": 2.12229583162338,
      "grad_norm": 0.34312155842781067,
      "learning_rate": 7.166290812228204e-06,
      "loss": 0.4333,
      "step": 90500
    },
    {
      "epoch": 2.123468370756874,
      "grad_norm": 0.4061514139175415,
      "learning_rate": 7.148846149000485e-06,
      "loss": 0.4226,
      "step": 90550
    },
    {
      "epoch": 2.124640909890368,
      "grad_norm": 0.3754187524318695,
      "learning_rate": 7.13141609916024e-06,
      "loss": 0.4015,
      "step": 90600
    },
    {
      "epoch": 2.125813449023861,
      "grad_norm": 0.37542152404785156,
      "learning_rate": 7.114000695150107e-06,
      "loss": 0.4757,
      "step": 90650
    },
    {
      "epoch": 2.126985988157355,
      "grad_norm": 0.32674112915992737,
      "learning_rate": 7.096599969385451e-06,
      "loss": 0.435,
      "step": 90700
    },
    {
      "epoch": 2.1281585272908483,
      "grad_norm": 0.35460612177848816,
      "learning_rate": 7.079213954254315e-06,
      "loss": 0.4614,
      "step": 90750
    },
    {
      "epoch": 2.129331066424342,
      "grad_norm": 0.3285317122936249,
      "learning_rate": 7.0618426821173745e-06,
      "loss": 0.442,
      "step": 90800
    },
    {
      "epoch": 2.1305036055578355,
      "grad_norm": 0.4775228798389435,
      "learning_rate": 7.044486185307847e-06,
      "loss": 0.4394,
      "step": 90850
    },
    {
      "epoch": 2.1316761446913293,
      "grad_norm": 0.48183882236480713,
      "learning_rate": 7.027144496131468e-06,
      "loss": 0.4898,
      "step": 90900
    },
    {
      "epoch": 2.1328486838248226,
      "grad_norm": 0.5442840456962585,
      "learning_rate": 7.009817646866386e-06,
      "loss": 0.4591,
      "step": 90950
    },
    {
      "epoch": 2.1340212229583164,
      "grad_norm": 0.30431801080703735,
      "learning_rate": 6.992505669763157e-06,
      "loss": 0.4377,
      "step": 91000
    },
    {
      "epoch": 2.1351937620918098,
      "grad_norm": 0.3873680830001831,
      "learning_rate": 6.975208597044629e-06,
      "loss": 0.46,
      "step": 91050
    },
    {
      "epoch": 2.1363663012253036,
      "grad_norm": 0.45145317912101746,
      "learning_rate": 6.957926460905929e-06,
      "loss": 0.4553,
      "step": 91100
    },
    {
      "epoch": 2.137538840358797,
      "grad_norm": 0.3548334240913391,
      "learning_rate": 6.940659293514366e-06,
      "loss": 0.4878,
      "step": 91150
    },
    {
      "epoch": 2.1387113794922907,
      "grad_norm": 0.3476397395133972,
      "learning_rate": 6.923407127009404e-06,
      "loss": 0.4849,
      "step": 91200
    },
    {
      "epoch": 2.139883918625784,
      "grad_norm": 0.2864704728126526,
      "learning_rate": 6.906169993502567e-06,
      "loss": 0.4315,
      "step": 91250
    },
    {
      "epoch": 2.141056457759278,
      "grad_norm": 0.36681798100471497,
      "learning_rate": 6.888947925077418e-06,
      "loss": 0.4693,
      "step": 91300
    },
    {
      "epoch": 2.1422289968927712,
      "grad_norm": 0.6689220070838928,
      "learning_rate": 6.871740953789459e-06,
      "loss": 0.4296,
      "step": 91350
    },
    {
      "epoch": 2.143401536026265,
      "grad_norm": 0.3772116005420685,
      "learning_rate": 6.854549111666114e-06,
      "loss": 0.455,
      "step": 91400
    },
    {
      "epoch": 2.1445740751597584,
      "grad_norm": 0.4001595079898834,
      "learning_rate": 6.837372430706621e-06,
      "loss": 0.4717,
      "step": 91450
    },
    {
      "epoch": 2.145746614293252,
      "grad_norm": 0.3204444348812103,
      "learning_rate": 6.820210942882027e-06,
      "loss": 0.4215,
      "step": 91500
    },
    {
      "epoch": 2.1469191534267456,
      "grad_norm": 0.28224629163742065,
      "learning_rate": 6.803064680135071e-06,
      "loss": 0.4202,
      "step": 91550
    },
    {
      "epoch": 2.1480916925602394,
      "grad_norm": 0.37626391649246216,
      "learning_rate": 6.785933674380183e-06,
      "loss": 0.4566,
      "step": 91600
    },
    {
      "epoch": 2.1492642316937327,
      "grad_norm": 0.35066425800323486,
      "learning_rate": 6.7688179575033655e-06,
      "loss": 0.4231,
      "step": 91650
    },
    {
      "epoch": 2.1504367708272265,
      "grad_norm": 0.36517539620399475,
      "learning_rate": 6.75171756136219e-06,
      "loss": 0.4596,
      "step": 91700
    },
    {
      "epoch": 2.15160930996072,
      "grad_norm": 0.490153968334198,
      "learning_rate": 6.734632517785689e-06,
      "loss": 0.4524,
      "step": 91750
    },
    {
      "epoch": 2.1527818490942137,
      "grad_norm": 0.2074151188135147,
      "learning_rate": 6.717562858574342e-06,
      "loss": 0.4215,
      "step": 91800
    },
    {
      "epoch": 2.153954388227707,
      "grad_norm": 0.35903698205947876,
      "learning_rate": 6.700508615499968e-06,
      "loss": 0.4801,
      "step": 91850
    },
    {
      "epoch": 2.155126927361201,
      "grad_norm": 0.3658217191696167,
      "learning_rate": 6.683469820305719e-06,
      "loss": 0.4504,
      "step": 91900
    },
    {
      "epoch": 2.156299466494694,
      "grad_norm": 0.3419053852558136,
      "learning_rate": 6.666446504705971e-06,
      "loss": 0.4607,
      "step": 91950
    },
    {
      "epoch": 2.157472005628188,
      "grad_norm": 0.33497363328933716,
      "learning_rate": 6.649438700386294e-06,
      "loss": 0.4499,
      "step": 92000
    },
    {
      "epoch": 2.157472005628188,
      "eval_loss": 0.7604782581329346,
      "eval_runtime": 179.8234,
      "eval_samples_per_second": 55.61,
      "eval_steps_per_second": 13.903,
      "step": 92000
    },
    {
      "epoch": 2.1586445447616813,
      "grad_norm": 0.4305398762226105,
      "learning_rate": 6.6324464390034e-06,
      "loss": 0.4335,
      "step": 92050
    },
    {
      "epoch": 2.159817083895175,
      "grad_norm": 0.2939351797103882,
      "learning_rate": 6.61546975218505e-06,
      "loss": 0.447,
      "step": 92100
    },
    {
      "epoch": 2.1609896230286685,
      "grad_norm": 0.357816606760025,
      "learning_rate": 6.598508671530037e-06,
      "loss": 0.4585,
      "step": 92150
    },
    {
      "epoch": 2.1621621621621623,
      "grad_norm": 0.22519253194332123,
      "learning_rate": 6.581563228608084e-06,
      "loss": 0.4726,
      "step": 92200
    },
    {
      "epoch": 2.1633347012956556,
      "grad_norm": 0.4868007302284241,
      "learning_rate": 6.56463345495983e-06,
      "loss": 0.4644,
      "step": 92250
    },
    {
      "epoch": 2.1645072404291494,
      "grad_norm": 0.5203395485877991,
      "learning_rate": 6.547719382096729e-06,
      "loss": 0.4409,
      "step": 92300
    },
    {
      "epoch": 2.165679779562643,
      "grad_norm": 0.4017561972141266,
      "learning_rate": 6.530821041501027e-06,
      "loss": 0.4425,
      "step": 92350
    },
    {
      "epoch": 2.1668523186961366,
      "grad_norm": 0.23478500545024872,
      "learning_rate": 6.513938464625673e-06,
      "loss": 0.4527,
      "step": 92400
    },
    {
      "epoch": 2.16802485782963,
      "grad_norm": 0.5286062955856323,
      "learning_rate": 6.497071682894291e-06,
      "loss": 0.4753,
      "step": 92450
    },
    {
      "epoch": 2.1691973969631237,
      "grad_norm": 0.23000621795654297,
      "learning_rate": 6.480220727701088e-06,
      "loss": 0.4402,
      "step": 92500
    },
    {
      "epoch": 2.170369936096617,
      "grad_norm": 0.19802813231945038,
      "learning_rate": 6.463385630410828e-06,
      "loss": 0.441,
      "step": 92550
    },
    {
      "epoch": 2.171542475230111,
      "grad_norm": 0.43707436323165894,
      "learning_rate": 6.446566422358746e-06,
      "loss": 0.4061,
      "step": 92600
    },
    {
      "epoch": 2.1727150143636043,
      "grad_norm": 0.30481553077697754,
      "learning_rate": 6.429763134850519e-06,
      "loss": 0.4596,
      "step": 92650
    },
    {
      "epoch": 2.173887553497098,
      "grad_norm": 0.27434343099594116,
      "learning_rate": 6.412975799162166e-06,
      "loss": 0.4645,
      "step": 92700
    },
    {
      "epoch": 2.1750600926305914,
      "grad_norm": 0.1634727418422699,
      "learning_rate": 6.396204446540043e-06,
      "loss": 0.4639,
      "step": 92750
    },
    {
      "epoch": 2.176232631764085,
      "grad_norm": 0.38382017612457275,
      "learning_rate": 6.3794491082007325e-06,
      "loss": 0.4605,
      "step": 92800
    },
    {
      "epoch": 2.1774051708975786,
      "grad_norm": 0.2577320635318756,
      "learning_rate": 6.36270981533103e-06,
      "loss": 0.4628,
      "step": 92850
    },
    {
      "epoch": 2.1785777100310724,
      "grad_norm": 0.3057740330696106,
      "learning_rate": 6.345986599087844e-06,
      "loss": 0.47,
      "step": 92900
    },
    {
      "epoch": 2.1797502491645657,
      "grad_norm": 0.6153709888458252,
      "learning_rate": 6.329279490598183e-06,
      "loss": 0.4562,
      "step": 92950
    },
    {
      "epoch": 2.1809227882980595,
      "grad_norm": 0.35160720348358154,
      "learning_rate": 6.312588520959051e-06,
      "loss": 0.4702,
      "step": 93000
    },
    {
      "epoch": 2.182095327431553,
      "grad_norm": 0.17889225482940674,
      "learning_rate": 6.2959137212374355e-06,
      "loss": 0.4655,
      "step": 93050
    },
    {
      "epoch": 2.1832678665650467,
      "grad_norm": 0.34106916189193726,
      "learning_rate": 6.279255122470203e-06,
      "loss": 0.4528,
      "step": 93100
    },
    {
      "epoch": 2.18444040569854,
      "grad_norm": 0.24559178948402405,
      "learning_rate": 6.262612755664087e-06,
      "loss": 0.5186,
      "step": 93150
    },
    {
      "epoch": 2.185612944832034,
      "grad_norm": 0.5824068188667297,
      "learning_rate": 6.245986651795592e-06,
      "loss": 0.4435,
      "step": 93200
    },
    {
      "epoch": 2.186785483965527,
      "grad_norm": 0.32448118925094604,
      "learning_rate": 6.229376841810965e-06,
      "loss": 0.4411,
      "step": 93250
    },
    {
      "epoch": 2.187958023099021,
      "grad_norm": 0.4297894239425659,
      "learning_rate": 6.212783356626117e-06,
      "loss": 0.452,
      "step": 93300
    },
    {
      "epoch": 2.1891305622325143,
      "grad_norm": 0.26742467284202576,
      "learning_rate": 6.196206227126568e-06,
      "loss": 0.424,
      "step": 93350
    },
    {
      "epoch": 2.190303101366008,
      "grad_norm": 0.44863638281822205,
      "learning_rate": 6.179645484167414e-06,
      "loss": 0.4549,
      "step": 93400
    },
    {
      "epoch": 2.1914756404995015,
      "grad_norm": 0.2818913757801056,
      "learning_rate": 6.163101158573229e-06,
      "loss": 0.4415,
      "step": 93450
    },
    {
      "epoch": 2.1926481796329953,
      "grad_norm": 0.3379995822906494,
      "learning_rate": 6.146573281138049e-06,
      "loss": 0.4596,
      "step": 93500
    },
    {
      "epoch": 2.1938207187664887,
      "grad_norm": 0.3231220245361328,
      "learning_rate": 6.130061882625277e-06,
      "loss": 0.4313,
      "step": 93550
    },
    {
      "epoch": 2.1949932578999825,
      "grad_norm": 0.41789913177490234,
      "learning_rate": 6.113566993767663e-06,
      "loss": 0.4325,
      "step": 93600
    },
    {
      "epoch": 2.196165797033476,
      "grad_norm": 0.19638611376285553,
      "learning_rate": 6.097088645267203e-06,
      "loss": 0.481,
      "step": 93650
    },
    {
      "epoch": 2.1973383361669696,
      "grad_norm": 0.28660687804222107,
      "learning_rate": 6.080626867795134e-06,
      "loss": 0.4789,
      "step": 93700
    },
    {
      "epoch": 2.198510875300463,
      "grad_norm": 0.3645288050174713,
      "learning_rate": 6.064181691991817e-06,
      "loss": 0.4773,
      "step": 93750
    },
    {
      "epoch": 2.1996834144339568,
      "grad_norm": 0.2807657718658447,
      "learning_rate": 6.047753148466754e-06,
      "loss": 0.4392,
      "step": 93800
    },
    {
      "epoch": 2.20085595356745,
      "grad_norm": 0.20907257497310638,
      "learning_rate": 6.031341267798449e-06,
      "loss": 0.4142,
      "step": 93850
    },
    {
      "epoch": 2.202028492700944,
      "grad_norm": 0.45026570558547974,
      "learning_rate": 6.014946080534419e-06,
      "loss": 0.5258,
      "step": 93900
    },
    {
      "epoch": 2.2032010318344373,
      "grad_norm": 0.34699347615242004,
      "learning_rate": 5.998567617191095e-06,
      "loss": 0.5151,
      "step": 93950
    },
    {
      "epoch": 2.204373570967931,
      "grad_norm": 0.44949594140052795,
      "learning_rate": 5.982205908253775e-06,
      "loss": 0.4399,
      "step": 94000
    },
    {
      "epoch": 2.204373570967931,
      "eval_loss": 0.7607921361923218,
      "eval_runtime": 185.8825,
      "eval_samples_per_second": 53.797,
      "eval_steps_per_second": 13.449,
      "step": 94000
    },
    {
      "epoch": 2.2055461101014244,
      "grad_norm": 0.2078845351934433,
      "learning_rate": 5.9658609841765935e-06,
      "loss": 0.4598,
      "step": 94050
    },
    {
      "epoch": 2.2067186492349182,
      "grad_norm": 0.17628422379493713,
      "learning_rate": 5.949532875382417e-06,
      "loss": 0.4588,
      "step": 94100
    },
    {
      "epoch": 2.2078911883684116,
      "grad_norm": 0.3362213671207428,
      "learning_rate": 5.9332216122628405e-06,
      "loss": 0.487,
      "step": 94150
    },
    {
      "epoch": 2.2090637275019054,
      "grad_norm": 0.2463763952255249,
      "learning_rate": 5.916927225178078e-06,
      "loss": 0.421,
      "step": 94200
    },
    {
      "epoch": 2.210236266635399,
      "grad_norm": 0.30877336859703064,
      "learning_rate": 5.9006497444569556e-06,
      "loss": 0.414,
      "step": 94250
    },
    {
      "epoch": 2.2114088057688925,
      "grad_norm": 0.30949667096138,
      "learning_rate": 5.884389200396809e-06,
      "loss": 0.4471,
      "step": 94300
    },
    {
      "epoch": 2.212581344902386,
      "grad_norm": 0.3699880838394165,
      "learning_rate": 5.868145623263473e-06,
      "loss": 0.4386,
      "step": 94350
    },
    {
      "epoch": 2.2137538840358797,
      "grad_norm": 0.3604932427406311,
      "learning_rate": 5.8519190432911764e-06,
      "loss": 0.4351,
      "step": 94400
    },
    {
      "epoch": 2.2149264231693735,
      "grad_norm": 0.4123213589191437,
      "learning_rate": 5.8357094906825385e-06,
      "loss": 0.4732,
      "step": 94450
    },
    {
      "epoch": 2.216098962302867,
      "grad_norm": 0.5274869799613953,
      "learning_rate": 5.819516995608459e-06,
      "loss": 0.433,
      "step": 94500
    },
    {
      "epoch": 2.21727150143636,
      "grad_norm": 0.7112593054771423,
      "learning_rate": 5.803341588208111e-06,
      "loss": 0.4262,
      "step": 94550
    },
    {
      "epoch": 2.218444040569854,
      "grad_norm": 0.2398093342781067,
      "learning_rate": 5.787183298588844e-06,
      "loss": 0.442,
      "step": 94600
    },
    {
      "epoch": 2.219616579703348,
      "grad_norm": 0.48848211765289307,
      "learning_rate": 5.771042156826161e-06,
      "loss": 0.4888,
      "step": 94650
    },
    {
      "epoch": 2.220789118836841,
      "grad_norm": 0.46012458205223083,
      "learning_rate": 5.754918192963633e-06,
      "loss": 0.4431,
      "step": 94700
    },
    {
      "epoch": 2.221961657970335,
      "grad_norm": 0.2900211811065674,
      "learning_rate": 5.738811437012875e-06,
      "loss": 0.4559,
      "step": 94750
    },
    {
      "epoch": 2.2231341971038283,
      "grad_norm": 0.34931325912475586,
      "learning_rate": 5.722721918953451e-06,
      "loss": 0.4884,
      "step": 94800
    },
    {
      "epoch": 2.224306736237322,
      "grad_norm": 0.41856932640075684,
      "learning_rate": 5.706649668732866e-06,
      "loss": 0.4663,
      "step": 94850
    },
    {
      "epoch": 2.2254792753708155,
      "grad_norm": 0.25347399711608887,
      "learning_rate": 5.690594716266458e-06,
      "loss": 0.4169,
      "step": 94900
    },
    {
      "epoch": 2.2266518145043093,
      "grad_norm": 0.4380110502243042,
      "learning_rate": 5.6745570914373936e-06,
      "loss": 0.4542,
      "step": 94950
    },
    {
      "epoch": 2.2278243536378026,
      "grad_norm": 0.33395254611968994,
      "learning_rate": 5.658536824096562e-06,
      "loss": 0.4664,
      "step": 95000
    },
    {
      "epoch": 2.2289968927712964,
      "grad_norm": 0.28265148401260376,
      "learning_rate": 5.64253394406257e-06,
      "loss": 0.4634,
      "step": 95050
    },
    {
      "epoch": 2.23016943190479,
      "grad_norm": 0.5594154000282288,
      "learning_rate": 5.626548481121639e-06,
      "loss": 0.4284,
      "step": 95100
    },
    {
      "epoch": 2.2313419710382836,
      "grad_norm": 0.43859872221946716,
      "learning_rate": 5.610580465027591e-06,
      "loss": 0.4639,
      "step": 95150
    },
    {
      "epoch": 2.232514510171777,
      "grad_norm": 0.37532180547714233,
      "learning_rate": 5.594629925501756e-06,
      "loss": 0.4758,
      "step": 95200
    },
    {
      "epoch": 2.2336870493052707,
      "grad_norm": 0.47067806124687195,
      "learning_rate": 5.5786968922329525e-06,
      "loss": 0.4348,
      "step": 95250
    },
    {
      "epoch": 2.234859588438764,
      "grad_norm": 0.24685138463974,
      "learning_rate": 5.562781394877403e-06,
      "loss": 0.4446,
      "step": 95300
    },
    {
      "epoch": 2.236032127572258,
      "grad_norm": 0.39484903216362,
      "learning_rate": 5.546883463058688e-06,
      "loss": 0.4191,
      "step": 95350
    },
    {
      "epoch": 2.2372046667057512,
      "grad_norm": 0.3876955807209015,
      "learning_rate": 5.531003126367709e-06,
      "loss": 0.4608,
      "step": 95400
    },
    {
      "epoch": 2.238377205839245,
      "grad_norm": 0.2773965895175934,
      "learning_rate": 5.515140414362593e-06,
      "loss": 0.4653,
      "step": 95450
    },
    {
      "epoch": 2.2395497449727384,
      "grad_norm": 0.4641968905925751,
      "learning_rate": 5.499295356568692e-06,
      "loss": 0.5002,
      "step": 95500
    },
    {
      "epoch": 2.240722284106232,
      "grad_norm": 0.23521441221237183,
      "learning_rate": 5.483467982478468e-06,
      "loss": 0.4829,
      "step": 95550
    },
    {
      "epoch": 2.2418948232397256,
      "grad_norm": 0.37983524799346924,
      "learning_rate": 5.467658321551495e-06,
      "loss": 0.4466,
      "step": 95600
    },
    {
      "epoch": 2.2430673623732194,
      "grad_norm": 0.2272849828004837,
      "learning_rate": 5.451866403214353e-06,
      "loss": 0.4878,
      "step": 95650
    },
    {
      "epoch": 2.2442399015067127,
      "grad_norm": 0.38856247067451477,
      "learning_rate": 5.436092256860624e-06,
      "loss": 0.4412,
      "step": 95700
    },
    {
      "epoch": 2.2454124406402065,
      "grad_norm": 0.19988103210926056,
      "learning_rate": 5.420335911850779e-06,
      "loss": 0.4423,
      "step": 95750
    },
    {
      "epoch": 2.2465849797737,
      "grad_norm": 0.2574673295021057,
      "learning_rate": 5.404597397512191e-06,
      "loss": 0.4084,
      "step": 95800
    },
    {
      "epoch": 2.2477575189071937,
      "grad_norm": 0.32620689272880554,
      "learning_rate": 5.388876743139012e-06,
      "loss": 0.4466,
      "step": 95850
    },
    {
      "epoch": 2.248930058040687,
      "grad_norm": 0.3113793134689331,
      "learning_rate": 5.373173977992174e-06,
      "loss": 0.4648,
      "step": 95900
    },
    {
      "epoch": 2.250102597174181,
      "grad_norm": 0.18490268290042877,
      "learning_rate": 5.357489131299299e-06,
      "loss": 0.4636,
      "step": 95950
    },
    {
      "epoch": 2.251275136307674,
      "grad_norm": 0.4529779255390167,
      "learning_rate": 5.341822232254668e-06,
      "loss": 0.4636,
      "step": 96000
    },
    {
      "epoch": 2.251275136307674,
      "eval_loss": 0.7612720727920532,
      "eval_runtime": 187.1583,
      "eval_samples_per_second": 53.431,
      "eval_steps_per_second": 13.358,
      "step": 96000
    },
    {
      "epoch": 2.252447675441168,
      "grad_norm": 0.18594926595687866,
      "learning_rate": 5.326173310019137e-06,
      "loss": 0.4738,
      "step": 96050
    },
    {
      "epoch": 2.2536202145746613,
      "grad_norm": 0.3385220170021057,
      "learning_rate": 5.310542393720131e-06,
      "loss": 0.5087,
      "step": 96100
    },
    {
      "epoch": 2.254792753708155,
      "grad_norm": 0.24537892639636993,
      "learning_rate": 5.294929512451528e-06,
      "loss": 0.4213,
      "step": 96150
    },
    {
      "epoch": 2.2559652928416485,
      "grad_norm": 0.27848973870277405,
      "learning_rate": 5.279334695273668e-06,
      "loss": 0.4281,
      "step": 96200
    },
    {
      "epoch": 2.2571378319751423,
      "grad_norm": 0.32184115052223206,
      "learning_rate": 5.263757971213239e-06,
      "loss": 0.4188,
      "step": 96250
    },
    {
      "epoch": 2.2583103711086356,
      "grad_norm": 0.1989937424659729,
      "learning_rate": 5.2481993692632804e-06,
      "loss": 0.4007,
      "step": 96300
    },
    {
      "epoch": 2.2594829102421294,
      "grad_norm": 0.26121509075164795,
      "learning_rate": 5.232658918383073e-06,
      "loss": 0.4311,
      "step": 96350
    },
    {
      "epoch": 2.260655449375623,
      "grad_norm": 0.44645023345947266,
      "learning_rate": 5.21713664749814e-06,
      "loss": 0.4124,
      "step": 96400
    },
    {
      "epoch": 2.2618279885091166,
      "grad_norm": 0.24853850901126862,
      "learning_rate": 5.201632585500143e-06,
      "loss": 0.4782,
      "step": 96450
    },
    {
      "epoch": 2.26300052764261,
      "grad_norm": 0.5734841227531433,
      "learning_rate": 5.1861467612468685e-06,
      "loss": 0.4666,
      "step": 96500
    },
    {
      "epoch": 2.2641730667761037,
      "grad_norm": 0.6320497989654541,
      "learning_rate": 5.170679203562148e-06,
      "loss": 0.4537,
      "step": 96550
    },
    {
      "epoch": 2.265345605909597,
      "grad_norm": 0.30861806869506836,
      "learning_rate": 5.155229941235809e-06,
      "loss": 0.4787,
      "step": 96600
    },
    {
      "epoch": 2.266518145043091,
      "grad_norm": 0.22352957725524902,
      "learning_rate": 5.139799003023643e-06,
      "loss": 0.4122,
      "step": 96650
    },
    {
      "epoch": 2.2676906841765843,
      "grad_norm": 0.40908166766166687,
      "learning_rate": 5.124386417647311e-06,
      "loss": 0.4512,
      "step": 96700
    },
    {
      "epoch": 2.268863223310078,
      "grad_norm": 0.4623326063156128,
      "learning_rate": 5.108992213794337e-06,
      "loss": 0.4714,
      "step": 96750
    },
    {
      "epoch": 2.2700357624435714,
      "grad_norm": 0.3549787402153015,
      "learning_rate": 5.093616420118015e-06,
      "loss": 0.4359,
      "step": 96800
    },
    {
      "epoch": 2.271208301577065,
      "grad_norm": 0.24443146586418152,
      "learning_rate": 5.078259065237383e-06,
      "loss": 0.4381,
      "step": 96850
    },
    {
      "epoch": 2.2723808407105586,
      "grad_norm": 0.24833475053310394,
      "learning_rate": 5.062920177737146e-06,
      "loss": 0.4272,
      "step": 96900
    },
    {
      "epoch": 2.2735533798440524,
      "grad_norm": 0.34654128551483154,
      "learning_rate": 5.047599786167653e-06,
      "loss": 0.4241,
      "step": 96950
    },
    {
      "epoch": 2.2747259189775457,
      "grad_norm": 0.3774588406085968,
      "learning_rate": 5.032297919044808e-06,
      "loss": 0.4457,
      "step": 97000
    },
    {
      "epoch": 2.2758984581110395,
      "grad_norm": 0.4356120526790619,
      "learning_rate": 5.017014604850051e-06,
      "loss": 0.4309,
      "step": 97050
    },
    {
      "epoch": 2.277070997244533,
      "grad_norm": 0.3673931360244751,
      "learning_rate": 5.001749872030276e-06,
      "loss": 0.4787,
      "step": 97100
    },
    {
      "epoch": 2.2782435363780267,
      "grad_norm": 0.520423948764801,
      "learning_rate": 4.986503748997806e-06,
      "loss": 0.4248,
      "step": 97150
    },
    {
      "epoch": 2.27941607551152,
      "grad_norm": 0.23141971230506897,
      "learning_rate": 4.971276264130308e-06,
      "loss": 0.4327,
      "step": 97200
    },
    {
      "epoch": 2.280588614645014,
      "grad_norm": 0.561988353729248,
      "learning_rate": 4.956067445770777e-06,
      "loss": 0.4426,
      "step": 97250
    },
    {
      "epoch": 2.281761153778507,
      "grad_norm": 0.33279791474342346,
      "learning_rate": 4.940877322227444e-06,
      "loss": 0.4521,
      "step": 97300
    },
    {
      "epoch": 2.282933692912001,
      "grad_norm": 0.28071117401123047,
      "learning_rate": 4.9257059217737586e-06,
      "loss": 0.474,
      "step": 97350
    },
    {
      "epoch": 2.2841062320454943,
      "grad_norm": 0.2920573055744171,
      "learning_rate": 4.910553272648318e-06,
      "loss": 0.4067,
      "step": 97400
    },
    {
      "epoch": 2.285278771178988,
      "grad_norm": 0.35335955023765564,
      "learning_rate": 4.89541940305481e-06,
      "loss": 0.447,
      "step": 97450
    },
    {
      "epoch": 2.2864513103124815,
      "grad_norm": 0.5464457869529724,
      "learning_rate": 4.880304341161981e-06,
      "loss": 0.4927,
      "step": 97500
    },
    {
      "epoch": 2.2876238494459753,
      "grad_norm": 0.46794649958610535,
      "learning_rate": 4.865208115103551e-06,
      "loss": 0.4602,
      "step": 97550
    },
    {
      "epoch": 2.2887963885794687,
      "grad_norm": 0.384474515914917,
      "learning_rate": 4.850130752978204e-06,
      "loss": 0.441,
      "step": 97600
    },
    {
      "epoch": 2.2899689277129625,
      "grad_norm": 0.23898614943027496,
      "learning_rate": 4.835072282849492e-06,
      "loss": 0.4544,
      "step": 97650
    },
    {
      "epoch": 2.291141466846456,
      "grad_norm": 0.21346856653690338,
      "learning_rate": 4.820032732745821e-06,
      "loss": 0.4476,
      "step": 97700
    },
    {
      "epoch": 2.2923140059799496,
      "grad_norm": 0.3934641182422638,
      "learning_rate": 4.805012130660362e-06,
      "loss": 0.4452,
      "step": 97750
    },
    {
      "epoch": 2.293486545113443,
      "grad_norm": 0.49791663885116577,
      "learning_rate": 4.79001050455104e-06,
      "loss": 0.475,
      "step": 97800
    },
    {
      "epoch": 2.2946590842469368,
      "grad_norm": 0.38120341300964355,
      "learning_rate": 4.775027882340435e-06,
      "loss": 0.475,
      "step": 97850
    },
    {
      "epoch": 2.2958316233804306,
      "grad_norm": 0.2881353199481964,
      "learning_rate": 4.760064291915781e-06,
      "loss": 0.4468,
      "step": 97900
    },
    {
      "epoch": 2.297004162513924,
      "grad_norm": 0.521329402923584,
      "learning_rate": 4.7451197611288664e-06,
      "loss": 0.474,
      "step": 97950
    },
    {
      "epoch": 2.2981767016474173,
      "grad_norm": 0.43216991424560547,
      "learning_rate": 4.7301943177960175e-06,
      "loss": 0.4602,
      "step": 98000
    },
    {
      "epoch": 2.2981767016474173,
      "eval_loss": 0.7601485252380371,
      "eval_runtime": 127.1435,
      "eval_samples_per_second": 78.651,
      "eval_steps_per_second": 19.663,
      "step": 98000
    },
    {
      "epoch": 2.299349240780911,
      "grad_norm": 0.2653510570526123,
      "learning_rate": 4.715287989698023e-06,
      "loss": 0.4653,
      "step": 98050
    },
    {
      "epoch": 2.300521779914405,
      "grad_norm": 0.49055927991867065,
      "learning_rate": 4.700400804580106e-06,
      "loss": 0.4494,
      "step": 98100
    },
    {
      "epoch": 2.3016943190478982,
      "grad_norm": 0.2140306830406189,
      "learning_rate": 4.685532790151839e-06,
      "loss": 0.4338,
      "step": 98150
    },
    {
      "epoch": 2.3028668581813916,
      "grad_norm": 0.29277801513671875,
      "learning_rate": 4.670683974087135e-06,
      "loss": 0.4434,
      "step": 98200
    },
    {
      "epoch": 2.3040393973148854,
      "grad_norm": 0.16144943237304688,
      "learning_rate": 4.65585438402415e-06,
      "loss": 0.4143,
      "step": 98250
    },
    {
      "epoch": 2.305211936448379,
      "grad_norm": 0.1937749981880188,
      "learning_rate": 4.641044047565275e-06,
      "loss": 0.4586,
      "step": 98300
    },
    {
      "epoch": 2.3063844755818725,
      "grad_norm": 0.4025241434574127,
      "learning_rate": 4.626252992277048e-06,
      "loss": 0.4782,
      "step": 98350
    },
    {
      "epoch": 2.307557014715366,
      "grad_norm": 0.6147443652153015,
      "learning_rate": 4.611481245690131e-06,
      "loss": 0.4312,
      "step": 98400
    },
    {
      "epoch": 2.3087295538488597,
      "grad_norm": 0.359316885471344,
      "learning_rate": 4.596728835299238e-06,
      "loss": 0.4539,
      "step": 98450
    },
    {
      "epoch": 2.3099020929823535,
      "grad_norm": 0.4926530718803406,
      "learning_rate": 4.581995788563099e-06,
      "loss": 0.4286,
      "step": 98500
    },
    {
      "epoch": 2.311074632115847,
      "grad_norm": 0.3384706676006317,
      "learning_rate": 4.567282132904399e-06,
      "loss": 0.4269,
      "step": 98550
    },
    {
      "epoch": 2.31224717124934,
      "grad_norm": 0.34869250655174255,
      "learning_rate": 4.552587895709722e-06,
      "loss": 0.4738,
      "step": 98600
    },
    {
      "epoch": 2.313419710382834,
      "grad_norm": 0.37809669971466064,
      "learning_rate": 4.537913104329531e-06,
      "loss": 0.4461,
      "step": 98650
    },
    {
      "epoch": 2.314592249516328,
      "grad_norm": 0.32979342341423035,
      "learning_rate": 4.523257786078067e-06,
      "loss": 0.482,
      "step": 98700
    },
    {
      "epoch": 2.315764788649821,
      "grad_norm": 0.7383145689964294,
      "learning_rate": 4.5086219682333495e-06,
      "loss": 0.4265,
      "step": 98750
    },
    {
      "epoch": 2.316937327783315,
      "grad_norm": 0.21376240253448486,
      "learning_rate": 4.4940056780370816e-06,
      "loss": 0.4318,
      "step": 98800
    },
    {
      "epoch": 2.3181098669168083,
      "grad_norm": 0.2375200241804123,
      "learning_rate": 4.4794089426946385e-06,
      "loss": 0.4628,
      "step": 98850
    },
    {
      "epoch": 2.319282406050302,
      "grad_norm": 0.2635316550731659,
      "learning_rate": 4.464831789374979e-06,
      "loss": 0.4376,
      "step": 98900
    },
    {
      "epoch": 2.3204549451837955,
      "grad_norm": 0.2827608287334442,
      "learning_rate": 4.450274245210633e-06,
      "loss": 0.4417,
      "step": 98950
    },
    {
      "epoch": 2.3216274843172893,
      "grad_norm": 0.3269745409488678,
      "learning_rate": 4.435736337297613e-06,
      "loss": 0.4928,
      "step": 99000
    },
    {
      "epoch": 2.3228000234507826,
      "grad_norm": 0.2920849621295929,
      "learning_rate": 4.4212180926954e-06,
      "loss": 0.4847,
      "step": 99050
    },
    {
      "epoch": 2.3239725625842764,
      "grad_norm": 0.45803892612457275,
      "learning_rate": 4.406719538426857e-06,
      "loss": 0.4558,
      "step": 99100
    },
    {
      "epoch": 2.32514510171777,
      "grad_norm": 0.2765592038631439,
      "learning_rate": 4.392240701478218e-06,
      "loss": 0.4709,
      "step": 99150
    },
    {
      "epoch": 2.3263176408512636,
      "grad_norm": 0.2604270875453949,
      "learning_rate": 4.377781608798998e-06,
      "loss": 0.4448,
      "step": 99200
    },
    {
      "epoch": 2.327490179984757,
      "grad_norm": 0.3509502410888672,
      "learning_rate": 4.363342287301978e-06,
      "loss": 0.4232,
      "step": 99250
    },
    {
      "epoch": 2.3286627191182507,
      "grad_norm": 0.30502399802207947,
      "learning_rate": 4.348922763863121e-06,
      "loss": 0.4639,
      "step": 99300
    },
    {
      "epoch": 2.329835258251744,
      "grad_norm": 0.2464340776205063,
      "learning_rate": 4.3345230653215615e-06,
      "loss": 0.4599,
      "step": 99350
    },
    {
      "epoch": 2.331007797385238,
      "grad_norm": 0.3907088041305542,
      "learning_rate": 4.3201432184795115e-06,
      "loss": 0.4589,
      "step": 99400
    },
    {
      "epoch": 2.3321803365187312,
      "grad_norm": 0.27462002635002136,
      "learning_rate": 4.305783250102252e-06,
      "loss": 0.4285,
      "step": 99450
    },
    {
      "epoch": 2.333352875652225,
      "grad_norm": 0.3185849189758301,
      "learning_rate": 4.29144318691805e-06,
      "loss": 0.4565,
      "step": 99500
    },
    {
      "epoch": 2.3345254147857184,
      "grad_norm": 0.39072293043136597,
      "learning_rate": 4.277123055618134e-06,
      "loss": 0.4731,
      "step": 99550
    },
    {
      "epoch": 2.335697953919212,
      "grad_norm": 0.1919449120759964,
      "learning_rate": 4.262822882856622e-06,
      "loss": 0.4787,
      "step": 99600
    },
    {
      "epoch": 2.3368704930527056,
      "grad_norm": 0.365945428609848,
      "learning_rate": 4.248542695250497e-06,
      "loss": 0.4211,
      "step": 99650
    },
    {
      "epoch": 2.3380430321861994,
      "grad_norm": 0.2779797613620758,
      "learning_rate": 4.234282519379524e-06,
      "loss": 0.4338,
      "step": 99700
    },
    {
      "epoch": 2.3392155713196927,
      "grad_norm": 0.5561568737030029,
      "learning_rate": 4.220042381786244e-06,
      "loss": 0.4468,
      "step": 99750
    },
    {
      "epoch": 2.3403881104531865,
      "grad_norm": 0.696711003780365,
      "learning_rate": 4.2058223089758815e-06,
      "loss": 0.4321,
      "step": 99800
    },
    {
      "epoch": 2.34156064958668,
      "grad_norm": 0.3284892737865448,
      "learning_rate": 4.191622327416319e-06,
      "loss": 0.4541,
      "step": 99850
    },
    {
      "epoch": 2.3427331887201737,
      "grad_norm": 0.45841875672340393,
      "learning_rate": 4.177442463538052e-06,
      "loss": 0.4514,
      "step": 99900
    },
    {
      "epoch": 2.343905727853667,
      "grad_norm": 0.1990693062543869,
      "learning_rate": 4.163282743734115e-06,
      "loss": 0.4288,
      "step": 99950
    },
    {
      "epoch": 2.345078266987161,
      "grad_norm": 0.42920151352882385,
      "learning_rate": 4.149143194360069e-06,
      "loss": 0.4232,
      "step": 100000
    },
    {
      "epoch": 2.345078266987161,
      "eval_loss": 0.7617712616920471,
      "eval_runtime": 179.797,
      "eval_samples_per_second": 55.618,
      "eval_steps_per_second": 13.905,
      "step": 100000
    },
    {
      "epoch": 2.346250806120654,
      "grad_norm": 0.4631969630718231,
      "learning_rate": 4.135023841733906e-06,
      "loss": 0.4456,
      "step": 100050
    },
    {
      "epoch": 2.347423345254148,
      "grad_norm": 0.3724854290485382,
      "learning_rate": 4.120924712136053e-06,
      "loss": 0.4286,
      "step": 100100
    },
    {
      "epoch": 2.3485958843876413,
      "grad_norm": 0.29747486114501953,
      "learning_rate": 4.106845831809272e-06,
      "loss": 0.4677,
      "step": 100150
    },
    {
      "epoch": 2.349768423521135,
      "grad_norm": 0.3733832538127899,
      "learning_rate": 4.092787226958655e-06,
      "loss": 0.479,
      "step": 100200
    },
    {
      "epoch": 2.3509409626546285,
      "grad_norm": 0.43179601430892944,
      "learning_rate": 4.078748923751535e-06,
      "loss": 0.4528,
      "step": 100250
    },
    {
      "epoch": 2.3521135017881223,
      "grad_norm": 0.32585248351097107,
      "learning_rate": 4.064730948317479e-06,
      "loss": 0.468,
      "step": 100300
    },
    {
      "epoch": 2.3532860409216156,
      "grad_norm": 0.291378378868103,
      "learning_rate": 4.050733326748194e-06,
      "loss": 0.4341,
      "step": 100350
    },
    {
      "epoch": 2.3544585800551094,
      "grad_norm": 0.2388451099395752,
      "learning_rate": 4.036756085097529e-06,
      "loss": 0.4073,
      "step": 100400
    },
    {
      "epoch": 2.355631119188603,
      "grad_norm": 0.58428955078125,
      "learning_rate": 4.022799249381375e-06,
      "loss": 0.4868,
      "step": 100450
    },
    {
      "epoch": 2.3568036583220966,
      "grad_norm": 0.34579697251319885,
      "learning_rate": 4.008862845577661e-06,
      "loss": 0.4257,
      "step": 100500
    },
    {
      "epoch": 2.35797619745559,
      "grad_norm": 0.46187344193458557,
      "learning_rate": 3.994946899626267e-06,
      "loss": 0.4446,
      "step": 100550
    },
    {
      "epoch": 2.3591487365890838,
      "grad_norm": 0.30808690190315247,
      "learning_rate": 3.981051437429016e-06,
      "loss": 0.4346,
      "step": 100600
    },
    {
      "epoch": 2.360321275722577,
      "grad_norm": 0.30723249912261963,
      "learning_rate": 3.967176484849584e-06,
      "loss": 0.457,
      "step": 100650
    },
    {
      "epoch": 2.361493814856071,
      "grad_norm": 0.2671467959880829,
      "learning_rate": 3.953322067713488e-06,
      "loss": 0.4364,
      "step": 100700
    },
    {
      "epoch": 2.3626663539895643,
      "grad_norm": 0.2402406483888626,
      "learning_rate": 3.939488211808012e-06,
      "loss": 0.4404,
      "step": 100750
    },
    {
      "epoch": 2.363838893123058,
      "grad_norm": 0.32226014137268066,
      "learning_rate": 3.925674942882178e-06,
      "loss": 0.4317,
      "step": 100800
    },
    {
      "epoch": 2.3650114322565514,
      "grad_norm": 0.4820030927658081,
      "learning_rate": 3.9118822866466776e-06,
      "loss": 0.4579,
      "step": 100850
    },
    {
      "epoch": 2.366183971390045,
      "grad_norm": 0.4048016667366028,
      "learning_rate": 3.8981102687738505e-06,
      "loss": 0.4615,
      "step": 100900
    },
    {
      "epoch": 2.3673565105235386,
      "grad_norm": 0.321681410074234,
      "learning_rate": 3.8843589148976096e-06,
      "loss": 0.4324,
      "step": 100950
    },
    {
      "epoch": 2.3685290496570324,
      "grad_norm": 0.4439840614795685,
      "learning_rate": 3.870628250613411e-06,
      "loss": 0.4513,
      "step": 101000
    },
    {
      "epoch": 2.3697015887905257,
      "grad_norm": 0.26712653040885925,
      "learning_rate": 3.856918301478207e-06,
      "loss": 0.4248,
      "step": 101050
    },
    {
      "epoch": 2.3708741279240195,
      "grad_norm": 0.5087352991104126,
      "learning_rate": 3.843229093010379e-06,
      "loss": 0.4608,
      "step": 101100
    },
    {
      "epoch": 2.372046667057513,
      "grad_norm": 0.3736400902271271,
      "learning_rate": 3.829560650689717e-06,
      "loss": 0.4712,
      "step": 101150
    },
    {
      "epoch": 2.3732192061910067,
      "grad_norm": 0.2686685621738434,
      "learning_rate": 3.815912999957349e-06,
      "loss": 0.4518,
      "step": 101200
    },
    {
      "epoch": 2.3743917453245,
      "grad_norm": 0.34210458397865295,
      "learning_rate": 3.802286166215713e-06,
      "loss": 0.4664,
      "step": 101250
    },
    {
      "epoch": 2.375564284457994,
      "grad_norm": 0.644382655620575,
      "learning_rate": 3.7886801748284877e-06,
      "loss": 0.478,
      "step": 101300
    },
    {
      "epoch": 2.376736823591487,
      "grad_norm": 0.2981678545475006,
      "learning_rate": 3.775095051120575e-06,
      "loss": 0.4588,
      "step": 101350
    },
    {
      "epoch": 2.377909362724981,
      "grad_norm": 0.3563733696937561,
      "learning_rate": 3.761530820378015e-06,
      "loss": 0.4934,
      "step": 101400
    },
    {
      "epoch": 2.3790819018584743,
      "grad_norm": 0.34975728392601013,
      "learning_rate": 3.747987507847981e-06,
      "loss": 0.4412,
      "step": 101450
    },
    {
      "epoch": 2.380254440991968,
      "grad_norm": 0.2764623463153839,
      "learning_rate": 3.7344651387386906e-06,
      "loss": 0.4236,
      "step": 101500
    },
    {
      "epoch": 2.3814269801254615,
      "grad_norm": 0.3316473364830017,
      "learning_rate": 3.7209637382194e-06,
      "loss": 0.442,
      "step": 101550
    },
    {
      "epoch": 2.3825995192589553,
      "grad_norm": 0.26546934247016907,
      "learning_rate": 3.707483331420315e-06,
      "loss": 0.465,
      "step": 101600
    },
    {
      "epoch": 2.3837720583924487,
      "grad_norm": 0.4534422755241394,
      "learning_rate": 3.6940239434325863e-06,
      "loss": 0.4737,
      "step": 101650
    },
    {
      "epoch": 2.3849445975259425,
      "grad_norm": 0.31464648246765137,
      "learning_rate": 3.6805855993082225e-06,
      "loss": 0.4123,
      "step": 101700
    },
    {
      "epoch": 2.3861171366594363,
      "grad_norm": 0.23393073678016663,
      "learning_rate": 3.6671683240600844e-06,
      "loss": 0.4263,
      "step": 101750
    },
    {
      "epoch": 2.3872896757929296,
      "grad_norm": 0.4331141412258148,
      "learning_rate": 3.6537721426618e-06,
      "loss": 0.471,
      "step": 101800
    },
    {
      "epoch": 2.388462214926423,
      "grad_norm": 0.38717344403266907,
      "learning_rate": 3.6403970800477366e-06,
      "loss": 0.4655,
      "step": 101850
    },
    {
      "epoch": 2.3896347540599168,
      "grad_norm": 0.46993908286094666,
      "learning_rate": 3.6270431611129658e-06,
      "loss": 0.4389,
      "step": 101900
    },
    {
      "epoch": 2.3908072931934106,
      "grad_norm": 0.35070690512657166,
      "learning_rate": 3.613710410713188e-06,
      "loss": 0.4359,
      "step": 101950
    },
    {
      "epoch": 2.391979832326904,
      "grad_norm": 0.5554884076118469,
      "learning_rate": 3.600398853664721e-06,
      "loss": 0.4813,
      "step": 102000
    },
    {
      "epoch": 2.391979832326904,
      "eval_loss": 0.762208104133606,
      "eval_runtime": 149.4588,
      "eval_samples_per_second": 66.908,
      "eval_steps_per_second": 16.727,
      "step": 102000
    },
    {
      "epoch": 2.3931523714603973,
      "grad_norm": 0.2657112181186676,
      "learning_rate": 3.5871085147444133e-06,
      "loss": 0.4348,
      "step": 102050
    },
    {
      "epoch": 2.394324910593891,
      "grad_norm": 0.3607896566390991,
      "learning_rate": 3.573839418689644e-06,
      "loss": 0.4574,
      "step": 102100
    },
    {
      "epoch": 2.395497449727385,
      "grad_norm": 0.2934688329696655,
      "learning_rate": 3.560591590198228e-06,
      "loss": 0.4542,
      "step": 102150
    },
    {
      "epoch": 2.3966699888608782,
      "grad_norm": 0.39165443181991577,
      "learning_rate": 3.547365053928417e-06,
      "loss": 0.4478,
      "step": 102200
    },
    {
      "epoch": 2.3978425279943716,
      "grad_norm": 0.5337212085723877,
      "learning_rate": 3.5341598344988136e-06,
      "loss": 0.4776,
      "step": 102250
    },
    {
      "epoch": 2.3990150671278654,
      "grad_norm": 0.30831170082092285,
      "learning_rate": 3.5209759564883605e-06,
      "loss": 0.4723,
      "step": 102300
    },
    {
      "epoch": 2.400187606261359,
      "grad_norm": 0.3343472480773926,
      "learning_rate": 3.50781344443626e-06,
      "loss": 0.4598,
      "step": 102350
    },
    {
      "epoch": 2.4013601453948525,
      "grad_norm": 0.22572040557861328,
      "learning_rate": 3.494672322841959e-06,
      "loss": 0.4156,
      "step": 102400
    },
    {
      "epoch": 2.402532684528346,
      "grad_norm": 0.2628036439418793,
      "learning_rate": 3.481552616165081e-06,
      "loss": 0.4377,
      "step": 102450
    },
    {
      "epoch": 2.4037052236618397,
      "grad_norm": 0.2239713966846466,
      "learning_rate": 3.468454348825402e-06,
      "loss": 0.4463,
      "step": 102500
    },
    {
      "epoch": 2.4048777627953335,
      "grad_norm": 0.29030972719192505,
      "learning_rate": 3.455377545202775e-06,
      "loss": 0.432,
      "step": 102550
    },
    {
      "epoch": 2.406050301928827,
      "grad_norm": 0.4267069399356842,
      "learning_rate": 3.442322229637122e-06,
      "loss": 0.4708,
      "step": 102600
    },
    {
      "epoch": 2.40722284106232,
      "grad_norm": 0.19595715403556824,
      "learning_rate": 3.4292884264283497e-06,
      "loss": 0.423,
      "step": 102650
    },
    {
      "epoch": 2.408395380195814,
      "grad_norm": 0.3002226650714874,
      "learning_rate": 3.4162761598363458e-06,
      "loss": 0.463,
      "step": 102700
    },
    {
      "epoch": 2.409567919329308,
      "grad_norm": 0.32228487730026245,
      "learning_rate": 3.4032854540808896e-06,
      "loss": 0.4568,
      "step": 102750
    },
    {
      "epoch": 2.410740458462801,
      "grad_norm": 0.3510333001613617,
      "learning_rate": 3.390316333341646e-06,
      "loss": 0.4365,
      "step": 102800
    },
    {
      "epoch": 2.411912997596295,
      "grad_norm": 0.24915145337581635,
      "learning_rate": 3.3773688217580906e-06,
      "loss": 0.461,
      "step": 102850
    },
    {
      "epoch": 2.4130855367297883,
      "grad_norm": 0.19348903000354767,
      "learning_rate": 3.364442943429491e-06,
      "loss": 0.4092,
      "step": 102900
    },
    {
      "epoch": 2.414258075863282,
      "grad_norm": 0.46962544322013855,
      "learning_rate": 3.3515387224148353e-06,
      "loss": 0.4511,
      "step": 102950
    },
    {
      "epoch": 2.4154306149967755,
      "grad_norm": 0.21404416859149933,
      "learning_rate": 3.3386561827328156e-06,
      "loss": 0.4712,
      "step": 103000
    },
    {
      "epoch": 2.4166031541302693,
      "grad_norm": 0.18005861341953278,
      "learning_rate": 3.3257953483617543e-06,
      "loss": 0.48,
      "step": 103050
    },
    {
      "epoch": 2.4177756932637626,
      "grad_norm": 0.428001344203949,
      "learning_rate": 3.3129562432395766e-06,
      "loss": 0.46,
      "step": 103100
    },
    {
      "epoch": 2.4189482323972564,
      "grad_norm": 1.258649230003357,
      "learning_rate": 3.300138891263773e-06,
      "loss": 0.4619,
      "step": 103150
    },
    {
      "epoch": 2.42012077153075,
      "grad_norm": 0.34013697504997253,
      "learning_rate": 3.28734331629133e-06,
      "loss": 0.4539,
      "step": 103200
    },
    {
      "epoch": 2.4212933106642436,
      "grad_norm": 0.4385107457637787,
      "learning_rate": 3.274569542138715e-06,
      "loss": 0.4354,
      "step": 103250
    },
    {
      "epoch": 2.422465849797737,
      "grad_norm": 0.5785348415374756,
      "learning_rate": 3.2618175925818055e-06,
      "loss": 0.45,
      "step": 103300
    },
    {
      "epoch": 2.4236383889312307,
      "grad_norm": 0.2046646922826767,
      "learning_rate": 3.2490874913558673e-06,
      "loss": 0.4942,
      "step": 103350
    },
    {
      "epoch": 2.424810928064724,
      "grad_norm": 0.31854233145713806,
      "learning_rate": 3.236379262155486e-06,
      "loss": 0.4369,
      "step": 103400
    },
    {
      "epoch": 2.425983467198218,
      "grad_norm": 0.2152794450521469,
      "learning_rate": 3.2236929286345546e-06,
      "loss": 0.4513,
      "step": 103450
    },
    {
      "epoch": 2.4271560063317112,
      "grad_norm": 0.5731802582740784,
      "learning_rate": 3.2110285144061923e-06,
      "loss": 0.4661,
      "step": 103500
    },
    {
      "epoch": 2.428328545465205,
      "grad_norm": 0.38627490401268005,
      "learning_rate": 3.1983860430427388e-06,
      "loss": 0.4252,
      "step": 103550
    },
    {
      "epoch": 2.4295010845986984,
      "grad_norm": 0.43008285760879517,
      "learning_rate": 3.1857655380756733e-06,
      "loss": 0.515,
      "step": 103600
    },
    {
      "epoch": 2.430673623732192,
      "grad_norm": 0.31662747263908386,
      "learning_rate": 3.1731670229956063e-06,
      "loss": 0.4569,
      "step": 103650
    },
    {
      "epoch": 2.4318461628656856,
      "grad_norm": 0.3467966318130493,
      "learning_rate": 3.1605905212522008e-06,
      "loss": 0.4516,
      "step": 103700
    },
    {
      "epoch": 2.4330187019991794,
      "grad_norm": 0.25166693329811096,
      "learning_rate": 3.1480360562541655e-06,
      "loss": 0.4773,
      "step": 103750
    },
    {
      "epoch": 2.4341912411326727,
      "grad_norm": 0.1998295933008194,
      "learning_rate": 3.1355036513691697e-06,
      "loss": 0.4612,
      "step": 103800
    },
    {
      "epoch": 2.4353637802661665,
      "grad_norm": 0.30600354075431824,
      "learning_rate": 3.122993329923846e-06,
      "loss": 0.4257,
      "step": 103850
    },
    {
      "epoch": 2.43653631939966,
      "grad_norm": 0.25577059388160706,
      "learning_rate": 3.1105051152037027e-06,
      "loss": 0.4467,
      "step": 103900
    },
    {
      "epoch": 2.4377088585331537,
      "grad_norm": 0.28714117407798767,
      "learning_rate": 3.098039030453115e-06,
      "loss": 0.4503,
      "step": 103950
    },
    {
      "epoch": 2.438881397666647,
      "grad_norm": 0.33065491914749146,
      "learning_rate": 3.0855950988752548e-06,
      "loss": 0.4307,
      "step": 104000
    },
    {
      "epoch": 2.438881397666647,
      "eval_loss": 0.761972188949585,
      "eval_runtime": 122.4234,
      "eval_samples_per_second": 81.684,
      "eval_steps_per_second": 20.421,
      "step": 104000
    },
    {
      "epoch": 2.440053936800141,
      "grad_norm": 0.48148834705352783,
      "learning_rate": 3.0731733436320764e-06,
      "loss": 0.4475,
      "step": 104050
    },
    {
      "epoch": 2.441226475933634,
      "grad_norm": 0.1865122765302658,
      "learning_rate": 3.060773787844239e-06,
      "loss": 0.4298,
      "step": 104100
    },
    {
      "epoch": 2.442399015067128,
      "grad_norm": 0.4725492298603058,
      "learning_rate": 3.048396454591097e-06,
      "loss": 0.4009,
      "step": 104150
    },
    {
      "epoch": 2.4435715542006213,
      "grad_norm": 0.2330094873905182,
      "learning_rate": 3.036041366910631e-06,
      "loss": 0.4625,
      "step": 104200
    },
    {
      "epoch": 2.444744093334115,
      "grad_norm": 0.26440244913101196,
      "learning_rate": 3.0237085477994265e-06,
      "loss": 0.4586,
      "step": 104250
    },
    {
      "epoch": 2.4459166324676085,
      "grad_norm": 0.2338389903306961,
      "learning_rate": 3.0113980202126117e-06,
      "loss": 0.4626,
      "step": 104300
    },
    {
      "epoch": 2.4470891716011023,
      "grad_norm": 0.33661216497421265,
      "learning_rate": 2.9991098070638196e-06,
      "loss": 0.4426,
      "step": 104350
    },
    {
      "epoch": 2.4482617107345956,
      "grad_norm": 0.2015494853258133,
      "learning_rate": 2.9868439312251657e-06,
      "loss": 0.4518,
      "step": 104400
    },
    {
      "epoch": 2.4494342498680894,
      "grad_norm": 0.5272841453552246,
      "learning_rate": 2.9746004155271723e-06,
      "loss": 0.4574,
      "step": 104450
    },
    {
      "epoch": 2.450606789001583,
      "grad_norm": 0.2921980321407318,
      "learning_rate": 2.9623792827587554e-06,
      "loss": 0.4418,
      "step": 104500
    },
    {
      "epoch": 2.4517793281350766,
      "grad_norm": 0.2866359353065491,
      "learning_rate": 2.9501805556671507e-06,
      "loss": 0.4591,
      "step": 104550
    },
    {
      "epoch": 2.45295186726857,
      "grad_norm": 0.2833079695701599,
      "learning_rate": 2.9380042569579225e-06,
      "loss": 0.452,
      "step": 104600
    },
    {
      "epoch": 2.4541244064020638,
      "grad_norm": 0.23077607154846191,
      "learning_rate": 2.9258504092948545e-06,
      "loss": 0.4533,
      "step": 104650
    },
    {
      "epoch": 2.455296945535557,
      "grad_norm": 0.2886004149913788,
      "learning_rate": 2.9137190352999664e-06,
      "loss": 0.4442,
      "step": 104700
    },
    {
      "epoch": 2.456469484669051,
      "grad_norm": 0.36411237716674805,
      "learning_rate": 2.9016101575534308e-06,
      "loss": 0.4448,
      "step": 104750
    },
    {
      "epoch": 2.4576420238025443,
      "grad_norm": 0.32104945182800293,
      "learning_rate": 2.889523798593564e-06,
      "loss": 0.4207,
      "step": 104800
    },
    {
      "epoch": 2.458814562936038,
      "grad_norm": 0.446078896522522,
      "learning_rate": 2.8774599809167506e-06,
      "loss": 0.4753,
      "step": 104850
    },
    {
      "epoch": 2.4599871020695314,
      "grad_norm": 0.3144786059856415,
      "learning_rate": 2.865418726977434e-06,
      "loss": 0.4654,
      "step": 104900
    },
    {
      "epoch": 2.461159641203025,
      "grad_norm": 0.3529551327228546,
      "learning_rate": 2.8534000591880493e-06,
      "loss": 0.432,
      "step": 104950
    },
    {
      "epoch": 2.4623321803365186,
      "grad_norm": 0.43729814887046814,
      "learning_rate": 2.841403999919002e-06,
      "loss": 0.4604,
      "step": 105000
    },
    {
      "epoch": 2.4635047194700124,
      "grad_norm": 0.36247965693473816,
      "learning_rate": 2.829430571498607e-06,
      "loss": 0.4833,
      "step": 105050
    },
    {
      "epoch": 2.4646772586035057,
      "grad_norm": 0.6901759505271912,
      "learning_rate": 2.817479796213056e-06,
      "loss": 0.4738,
      "step": 105100
    },
    {
      "epoch": 2.4658497977369995,
      "grad_norm": 0.29281240701675415,
      "learning_rate": 2.8055516963063883e-06,
      "loss": 0.464,
      "step": 105150
    },
    {
      "epoch": 2.467022336870493,
      "grad_norm": 0.3349969983100891,
      "learning_rate": 2.7936462939804185e-06,
      "loss": 0.4743,
      "step": 105200
    },
    {
      "epoch": 2.4681948760039867,
      "grad_norm": 0.3625410199165344,
      "learning_rate": 2.7817636113947347e-06,
      "loss": 0.4315,
      "step": 105250
    },
    {
      "epoch": 2.46936741513748,
      "grad_norm": 0.22282837331295013,
      "learning_rate": 2.76990367066662e-06,
      "loss": 0.4198,
      "step": 105300
    },
    {
      "epoch": 2.470539954270974,
      "grad_norm": 0.39083802700042725,
      "learning_rate": 2.758066493871042e-06,
      "loss": 0.4324,
      "step": 105350
    },
    {
      "epoch": 2.471712493404467,
      "grad_norm": 0.3465236723423004,
      "learning_rate": 2.746252103040581e-06,
      "loss": 0.4466,
      "step": 105400
    },
    {
      "epoch": 2.472885032537961,
      "grad_norm": 0.2544362246990204,
      "learning_rate": 2.7344605201654223e-06,
      "loss": 0.4094,
      "step": 105450
    },
    {
      "epoch": 2.4740575716714543,
      "grad_norm": 0.29610520601272583,
      "learning_rate": 2.7226917671932867e-06,
      "loss": 0.4725,
      "step": 105500
    },
    {
      "epoch": 2.475230110804948,
      "grad_norm": 0.29936930537223816,
      "learning_rate": 2.710945866029412e-06,
      "loss": 0.4239,
      "step": 105550
    },
    {
      "epoch": 2.4764026499384415,
      "grad_norm": 0.44068607687950134,
      "learning_rate": 2.6992228385364873e-06,
      "loss": 0.405,
      "step": 105600
    },
    {
      "epoch": 2.4775751890719353,
      "grad_norm": 0.22463059425354004,
      "learning_rate": 2.6875227065346464e-06,
      "loss": 0.3961,
      "step": 105650
    },
    {
      "epoch": 2.4787477282054287,
      "grad_norm": 0.3191624581813812,
      "learning_rate": 2.6758454918013866e-06,
      "loss": 0.469,
      "step": 105700
    },
    {
      "epoch": 2.4799202673389225,
      "grad_norm": 0.5257608890533447,
      "learning_rate": 2.664191216071568e-06,
      "loss": 0.4473,
      "step": 105750
    },
    {
      "epoch": 2.4810928064724163,
      "grad_norm": 0.30393949151039124,
      "learning_rate": 2.6525599010373397e-06,
      "loss": 0.4478,
      "step": 105800
    },
    {
      "epoch": 2.4822653456059096,
      "grad_norm": 0.2913753390312195,
      "learning_rate": 2.640951568348125e-06,
      "loss": 0.5063,
      "step": 105850
    },
    {
      "epoch": 2.483437884739403,
      "grad_norm": 0.3321346044540405,
      "learning_rate": 2.629366239610559e-06,
      "loss": 0.4244,
      "step": 105900
    },
    {
      "epoch": 2.4846104238728968,
      "grad_norm": 0.21789880096912384,
      "learning_rate": 2.6178039363884722e-06,
      "loss": 0.4011,
      "step": 105950
    },
    {
      "epoch": 2.4857829630063906,
      "grad_norm": 0.1409015655517578,
      "learning_rate": 2.606264680202823e-06,
      "loss": 0.4898,
      "step": 106000
    },
    {
      "epoch": 2.4857829630063906,
      "eval_loss": 0.7622289061546326,
      "eval_runtime": 122.9963,
      "eval_samples_per_second": 81.303,
      "eval_steps_per_second": 20.326,
      "step": 106000
    },
    {
      "epoch": 2.486955502139884,
      "grad_norm": 0.4187328517436981,
      "learning_rate": 2.594748492531689e-06,
      "loss": 0.4803,
      "step": 106050
    },
    {
      "epoch": 2.4881280412733773,
      "grad_norm": 0.22980067133903503,
      "learning_rate": 2.583255394810192e-06,
      "loss": 0.4293,
      "step": 106100
    },
    {
      "epoch": 2.489300580406871,
      "grad_norm": 0.4997883141040802,
      "learning_rate": 2.571785408430496e-06,
      "loss": 0.4582,
      "step": 106150
    },
    {
      "epoch": 2.490473119540365,
      "grad_norm": 0.3045777976512909,
      "learning_rate": 2.5603385547417274e-06,
      "loss": 0.4481,
      "step": 106200
    },
    {
      "epoch": 2.4916456586738582,
      "grad_norm": 0.309637188911438,
      "learning_rate": 2.5489148550499737e-06,
      "loss": 0.4532,
      "step": 106250
    },
    {
      "epoch": 2.4928181978073516,
      "grad_norm": 0.299113005399704,
      "learning_rate": 2.537514330618216e-06,
      "loss": 0.4635,
      "step": 106300
    },
    {
      "epoch": 2.4939907369408454,
      "grad_norm": 0.3928845524787903,
      "learning_rate": 2.526137002666295e-06,
      "loss": 0.4548,
      "step": 106350
    },
    {
      "epoch": 2.495163276074339,
      "grad_norm": 0.3169875144958496,
      "learning_rate": 2.5147828923708897e-06,
      "loss": 0.4493,
      "step": 106400
    },
    {
      "epoch": 2.4963358152078325,
      "grad_norm": 0.2977133095264435,
      "learning_rate": 2.503452020865449e-06,
      "loss": 0.4764,
      "step": 106450
    },
    {
      "epoch": 2.497508354341326,
      "grad_norm": 0.3481851816177368,
      "learning_rate": 2.492144409240181e-06,
      "loss": 0.4181,
      "step": 106500
    },
    {
      "epoch": 2.4986808934748197,
      "grad_norm": 0.29776862263679504,
      "learning_rate": 2.480860078541987e-06,
      "loss": 0.4312,
      "step": 106550
    },
    {
      "epoch": 2.4998534326083135,
      "grad_norm": 0.30891117453575134,
      "learning_rate": 2.4695990497744484e-06,
      "loss": 0.492,
      "step": 106600
    },
    {
      "epoch": 2.501025971741807,
      "grad_norm": 0.18784944713115692,
      "learning_rate": 2.4583613438977593e-06,
      "loss": 0.45,
      "step": 106650
    },
    {
      "epoch": 2.5021985108753,
      "grad_norm": 0.27623963356018066,
      "learning_rate": 2.4471469818287217e-06,
      "loss": 0.4689,
      "step": 106700
    },
    {
      "epoch": 2.503371050008794,
      "grad_norm": 0.35813915729522705,
      "learning_rate": 2.4359559844406694e-06,
      "loss": 0.4578,
      "step": 106750
    },
    {
      "epoch": 2.504543589142288,
      "grad_norm": 0.34861481189727783,
      "learning_rate": 2.4247883725634614e-06,
      "loss": 0.4477,
      "step": 106800
    },
    {
      "epoch": 2.505716128275781,
      "grad_norm": 0.42088302969932556,
      "learning_rate": 2.413644166983417e-06,
      "loss": 0.416,
      "step": 106850
    },
    {
      "epoch": 2.5068886674092745,
      "grad_norm": 0.28827959299087524,
      "learning_rate": 2.4025233884433006e-06,
      "loss": 0.419,
      "step": 106900
    },
    {
      "epoch": 2.5080612065427683,
      "grad_norm": 0.21316932141780853,
      "learning_rate": 2.3914260576422632e-06,
      "loss": 0.4418,
      "step": 106950
    },
    {
      "epoch": 2.509233745676262,
      "grad_norm": 0.33042585849761963,
      "learning_rate": 2.38035219523582e-06,
      "loss": 0.455,
      "step": 107000
    },
    {
      "epoch": 2.5104062848097555,
      "grad_norm": 0.5072457790374756,
      "learning_rate": 2.3693018218357933e-06,
      "loss": 0.4338,
      "step": 107050
    },
    {
      "epoch": 2.5115788239432493,
      "grad_norm": 0.2883874773979187,
      "learning_rate": 2.3582749580103003e-06,
      "loss": 0.4798,
      "step": 107100
    },
    {
      "epoch": 2.5127513630767426,
      "grad_norm": 0.2635791599750519,
      "learning_rate": 2.347271624283685e-06,
      "loss": 0.449,
      "step": 107150
    },
    {
      "epoch": 2.5139239022102364,
      "grad_norm": 0.2916083335876465,
      "learning_rate": 2.3362918411365065e-06,
      "loss": 0.4175,
      "step": 107200
    },
    {
      "epoch": 2.51509644134373,
      "grad_norm": 0.4451220631599426,
      "learning_rate": 2.325335629005478e-06,
      "loss": 0.4618,
      "step": 107250
    },
    {
      "epoch": 2.5162689804772236,
      "grad_norm": 0.2659814953804016,
      "learning_rate": 2.3144030082834534e-06,
      "loss": 0.428,
      "step": 107300
    },
    {
      "epoch": 2.517441519610717,
      "grad_norm": 0.19833552837371826,
      "learning_rate": 2.303493999319359e-06,
      "loss": 0.4777,
      "step": 107350
    },
    {
      "epoch": 2.5186140587442107,
      "grad_norm": 0.4322039484977722,
      "learning_rate": 2.292608622418189e-06,
      "loss": 0.4425,
      "step": 107400
    },
    {
      "epoch": 2.519786597877704,
      "grad_norm": 0.4142301082611084,
      "learning_rate": 2.281746897840939e-06,
      "loss": 0.4457,
      "step": 107450
    },
    {
      "epoch": 2.520959137011198,
      "grad_norm": 0.3842400014400482,
      "learning_rate": 2.2709088458045898e-06,
      "loss": 0.4465,
      "step": 107500
    },
    {
      "epoch": 2.5221316761446912,
      "grad_norm": 0.32947295904159546,
      "learning_rate": 2.2600944864820493e-06,
      "loss": 0.4498,
      "step": 107550
    },
    {
      "epoch": 2.523304215278185,
      "grad_norm": 0.33010244369506836,
      "learning_rate": 2.24930384000214e-06,
      "loss": 0.446,
      "step": 107600
    },
    {
      "epoch": 2.5244767544116784,
      "grad_norm": 0.3683831989765167,
      "learning_rate": 2.2385369264495366e-06,
      "loss": 0.4893,
      "step": 107650
    },
    {
      "epoch": 2.525649293545172,
      "grad_norm": 0.5403821468353271,
      "learning_rate": 2.2277937658647383e-06,
      "loss": 0.4536,
      "step": 107700
    },
    {
      "epoch": 2.5268218326786656,
      "grad_norm": 0.40060508251190186,
      "learning_rate": 2.2170743782440462e-06,
      "loss": 0.4404,
      "step": 107750
    },
    {
      "epoch": 2.5279943718121594,
      "grad_norm": 0.4161960184574127,
      "learning_rate": 2.2063787835394943e-06,
      "loss": 0.4964,
      "step": 107800
    },
    {
      "epoch": 2.5291669109456527,
      "grad_norm": 0.4399522542953491,
      "learning_rate": 2.1957070016588505e-06,
      "loss": 0.4489,
      "step": 107850
    },
    {
      "epoch": 2.5303394500791465,
      "grad_norm": 0.49203723669052124,
      "learning_rate": 2.1850590524655418e-06,
      "loss": 0.4558,
      "step": 107900
    },
    {
      "epoch": 2.53151198921264,
      "grad_norm": 0.3373318314552307,
      "learning_rate": 2.1744349557786496e-06,
      "loss": 0.4874,
      "step": 107950
    },
    {
      "epoch": 2.5326845283461337,
      "grad_norm": 0.5420023202896118,
      "learning_rate": 2.1638347313728464e-06,
      "loss": 0.4739,
      "step": 108000
    },
    {
      "epoch": 2.5326845283461337,
      "eval_loss": 0.7596141695976257,
      "eval_runtime": 121.4902,
      "eval_samples_per_second": 82.311,
      "eval_steps_per_second": 20.578,
      "step": 108000
    },
    {
      "epoch": 2.533857067479627,
      "grad_norm": 0.48695698380470276,
      "learning_rate": 2.153258398978384e-06,
      "loss": 0.4479,
      "step": 108050
    },
    {
      "epoch": 2.535029606613121,
      "grad_norm": 0.3303055167198181,
      "learning_rate": 2.142705978281027e-06,
      "loss": 0.4324,
      "step": 108100
    },
    {
      "epoch": 2.536202145746614,
      "grad_norm": 0.4557577073574066,
      "learning_rate": 2.132177488922048e-06,
      "loss": 0.4121,
      "step": 108150
    },
    {
      "epoch": 2.537374684880108,
      "grad_norm": 0.3937544524669647,
      "learning_rate": 2.1216729504981726e-06,
      "loss": 0.464,
      "step": 108200
    },
    {
      "epoch": 2.5385472240136013,
      "grad_norm": 0.44425925612449646,
      "learning_rate": 2.1111923825615455e-06,
      "loss": 0.4668,
      "step": 108250
    },
    {
      "epoch": 2.539719763147095,
      "grad_norm": 0.44053423404693604,
      "learning_rate": 2.100735804619692e-06,
      "loss": 0.4853,
      "step": 108300
    },
    {
      "epoch": 2.5408923022805885,
      "grad_norm": 0.30418699979782104,
      "learning_rate": 2.0903032361354846e-06,
      "loss": 0.4655,
      "step": 108350
    },
    {
      "epoch": 2.5420648414140823,
      "grad_norm": 0.4238549470901489,
      "learning_rate": 2.079894696527114e-06,
      "loss": 0.4265,
      "step": 108400
    },
    {
      "epoch": 2.5432373805475756,
      "grad_norm": 0.3182120621204376,
      "learning_rate": 2.0695102051680377e-06,
      "loss": 0.4373,
      "step": 108450
    },
    {
      "epoch": 2.5444099196810694,
      "grad_norm": 0.32981741428375244,
      "learning_rate": 2.059149781386958e-06,
      "loss": 0.4568,
      "step": 108500
    },
    {
      "epoch": 2.545582458814563,
      "grad_norm": 0.4517298936843872,
      "learning_rate": 2.0488134444677738e-06,
      "loss": 0.4582,
      "step": 108550
    },
    {
      "epoch": 2.5467549979480566,
      "grad_norm": 0.38714414834976196,
      "learning_rate": 2.0385012136495596e-06,
      "loss": 0.4203,
      "step": 108600
    },
    {
      "epoch": 2.54792753708155,
      "grad_norm": 0.36658382415771484,
      "learning_rate": 2.028213108126512e-06,
      "loss": 0.4375,
      "step": 108650
    },
    {
      "epoch": 2.5491000762150438,
      "grad_norm": 0.17250117659568787,
      "learning_rate": 2.0179491470479335e-06,
      "loss": 0.4426,
      "step": 108700
    },
    {
      "epoch": 2.550272615348537,
      "grad_norm": 0.5684683918952942,
      "learning_rate": 2.0077093495181713e-06,
      "loss": 0.4404,
      "step": 108750
    },
    {
      "epoch": 2.551445154482031,
      "grad_norm": 0.2833324074745178,
      "learning_rate": 1.9974937345966156e-06,
      "loss": 0.4768,
      "step": 108800
    },
    {
      "epoch": 2.5526176936155243,
      "grad_norm": 0.4517638683319092,
      "learning_rate": 1.9873023212976273e-06,
      "loss": 0.4371,
      "step": 108850
    },
    {
      "epoch": 2.553790232749018,
      "grad_norm": 0.2051849067211151,
      "learning_rate": 1.9771351285905357e-06,
      "loss": 0.4568,
      "step": 108900
    },
    {
      "epoch": 2.5549627718825114,
      "grad_norm": 0.2938075363636017,
      "learning_rate": 1.9669921753995773e-06,
      "loss": 0.4419,
      "step": 108950
    },
    {
      "epoch": 2.556135311016005,
      "grad_norm": 0.33373773097991943,
      "learning_rate": 1.9568734806038796e-06,
      "loss": 0.455,
      "step": 109000
    },
    {
      "epoch": 2.5573078501494986,
      "grad_norm": 0.2939085364341736,
      "learning_rate": 1.9467790630374106e-06,
      "loss": 0.41,
      "step": 109050
    },
    {
      "epoch": 2.5584803892829924,
      "grad_norm": 0.30175185203552246,
      "learning_rate": 1.936708941488957e-06,
      "loss": 0.4287,
      "step": 109100
    },
    {
      "epoch": 2.5596529284164857,
      "grad_norm": 0.41736069321632385,
      "learning_rate": 1.926663134702078e-06,
      "loss": 0.4264,
      "step": 109150
    },
    {
      "epoch": 2.5608254675499795,
      "grad_norm": 0.4721659719944,
      "learning_rate": 1.9166416613750825e-06,
      "loss": 0.4724,
      "step": 109200
    },
    {
      "epoch": 2.5619980066834733,
      "grad_norm": 0.32222679257392883,
      "learning_rate": 1.9066445401609783e-06,
      "loss": 0.4035,
      "step": 109250
    },
    {
      "epoch": 2.5631705458169667,
      "grad_norm": 0.4276781380176544,
      "learning_rate": 1.8966717896674556e-06,
      "loss": 0.4666,
      "step": 109300
    },
    {
      "epoch": 2.56434308495046,
      "grad_norm": 0.4486157298088074,
      "learning_rate": 1.8867234284568364e-06,
      "loss": 0.4946,
      "step": 109350
    },
    {
      "epoch": 2.565515624083954,
      "grad_norm": 0.3176290988922119,
      "learning_rate": 1.8767994750460516e-06,
      "loss": 0.4342,
      "step": 109400
    },
    {
      "epoch": 2.5666881632174476,
      "grad_norm": 0.34903672337532043,
      "learning_rate": 1.8668999479065974e-06,
      "loss": 0.4516,
      "step": 109450
    },
    {
      "epoch": 2.567860702350941,
      "grad_norm": 0.4350941777229309,
      "learning_rate": 1.857024865464511e-06,
      "loss": 0.4578,
      "step": 109500
    },
    {
      "epoch": 2.5690332414844343,
      "grad_norm": 0.3661687672138214,
      "learning_rate": 1.8471742461003216e-06,
      "loss": 0.4119,
      "step": 109550
    },
    {
      "epoch": 2.570205780617928,
      "grad_norm": 0.29685142636299133,
      "learning_rate": 1.8373481081490373e-06,
      "loss": 0.4796,
      "step": 109600
    },
    {
      "epoch": 2.571378319751422,
      "grad_norm": 0.27720868587493896,
      "learning_rate": 1.8275464699000883e-06,
      "loss": 0.4425,
      "step": 109650
    },
    {
      "epoch": 2.5725508588849153,
      "grad_norm": 0.23798805475234985,
      "learning_rate": 1.8177693495973057e-06,
      "loss": 0.4496,
      "step": 109700
    },
    {
      "epoch": 2.5737233980184087,
      "grad_norm": 0.25820299983024597,
      "learning_rate": 1.808016765438893e-06,
      "loss": 0.4375,
      "step": 109750
    },
    {
      "epoch": 2.5748959371519025,
      "grad_norm": 0.2675066888332367,
      "learning_rate": 1.7982887355773708e-06,
      "loss": 0.4059,
      "step": 109800
    },
    {
      "epoch": 2.5760684762853963,
      "grad_norm": 0.2939395010471344,
      "learning_rate": 1.7885852781195704e-06,
      "loss": 0.4406,
      "step": 109850
    },
    {
      "epoch": 2.5772410154188896,
      "grad_norm": 0.4801378548145294,
      "learning_rate": 1.7789064111265763e-06,
      "loss": 0.4291,
      "step": 109900
    },
    {
      "epoch": 2.578413554552383,
      "grad_norm": 0.15862227976322174,
      "learning_rate": 1.769252152613709e-06,
      "loss": 0.4454,
      "step": 109950
    },
    {
      "epoch": 2.5795860936858768,
      "grad_norm": 0.25031647086143494,
      "learning_rate": 1.7596225205504795e-06,
      "loss": 0.4042,
      "step": 110000
    },
    {
      "epoch": 2.5795860936858768,
      "eval_loss": 0.7601399421691895,
      "eval_runtime": 120.3782,
      "eval_samples_per_second": 83.071,
      "eval_steps_per_second": 20.768,
      "step": 110000
    },
    {
      "epoch": 2.5807586328193706,
      "grad_norm": 0.4392326772212982,
      "learning_rate": 1.7500175328605705e-06,
      "loss": 0.4605,
      "step": 110050
    },
    {
      "epoch": 2.581931171952864,
      "grad_norm": 0.45125341415405273,
      "learning_rate": 1.7404372074217812e-06,
      "loss": 0.4577,
      "step": 110100
    },
    {
      "epoch": 2.5831037110863573,
      "grad_norm": 0.4602501690387726,
      "learning_rate": 1.7308815620660208e-06,
      "loss": 0.4238,
      "step": 110150
    },
    {
      "epoch": 2.584276250219851,
      "grad_norm": 0.4228656589984894,
      "learning_rate": 1.7213506145792468e-06,
      "loss": 0.4805,
      "step": 110200
    },
    {
      "epoch": 2.585448789353345,
      "grad_norm": 0.22486446797847748,
      "learning_rate": 1.7118443827014607e-06,
      "loss": 0.4464,
      "step": 110250
    },
    {
      "epoch": 2.5866213284868382,
      "grad_norm": 0.19482481479644775,
      "learning_rate": 1.7023628841266486e-06,
      "loss": 0.4518,
      "step": 110300
    },
    {
      "epoch": 2.5877938676203316,
      "grad_norm": 0.2573409676551819,
      "learning_rate": 1.692906136502772e-06,
      "loss": 0.4759,
      "step": 110350
    },
    {
      "epoch": 2.5889664067538254,
      "grad_norm": 0.37301304936408997,
      "learning_rate": 1.683474157431711e-06,
      "loss": 0.4445,
      "step": 110400
    },
    {
      "epoch": 2.590138945887319,
      "grad_norm": 0.2644633948802948,
      "learning_rate": 1.6740669644692552e-06,
      "loss": 0.4367,
      "step": 110450
    },
    {
      "epoch": 2.5913114850208125,
      "grad_norm": 0.4335726201534271,
      "learning_rate": 1.664684575125049e-06,
      "loss": 0.4607,
      "step": 110500
    },
    {
      "epoch": 2.592484024154306,
      "grad_norm": 0.6015585660934448,
      "learning_rate": 1.6553270068625825e-06,
      "loss": 0.4423,
      "step": 110550
    },
    {
      "epoch": 2.5936565632877997,
      "grad_norm": 0.2570255696773529,
      "learning_rate": 1.6459942770991305e-06,
      "loss": 0.4426,
      "step": 110600
    },
    {
      "epoch": 2.5948291024212935,
      "grad_norm": 0.35748690366744995,
      "learning_rate": 1.6366864032057522e-06,
      "loss": 0.4326,
      "step": 110650
    },
    {
      "epoch": 2.596001641554787,
      "grad_norm": 0.2313963919878006,
      "learning_rate": 1.6274034025072293e-06,
      "loss": 0.4291,
      "step": 110700
    },
    {
      "epoch": 2.59717418068828,
      "grad_norm": 0.3803277909755707,
      "learning_rate": 1.6181452922820556e-06,
      "loss": 0.4368,
      "step": 110750
    },
    {
      "epoch": 2.598346719821774,
      "grad_norm": 0.2648526132106781,
      "learning_rate": 1.6089120897623882e-06,
      "loss": 0.4355,
      "step": 110800
    },
    {
      "epoch": 2.599519258955268,
      "grad_norm": 0.23618300259113312,
      "learning_rate": 1.599703812134035e-06,
      "loss": 0.4781,
      "step": 110850
    },
    {
      "epoch": 2.600691798088761,
      "grad_norm": 0.5314489006996155,
      "learning_rate": 1.5905204765363995e-06,
      "loss": 0.4864,
      "step": 110900
    },
    {
      "epoch": 2.6018643372222545,
      "grad_norm": 0.3309595584869385,
      "learning_rate": 1.5813621000624617e-06,
      "loss": 0.4254,
      "step": 110950
    },
    {
      "epoch": 2.6030368763557483,
      "grad_norm": 0.3609229028224945,
      "learning_rate": 1.572228699758756e-06,
      "loss": 0.476,
      "step": 111000
    },
    {
      "epoch": 2.604209415489242,
      "grad_norm": 0.512877881526947,
      "learning_rate": 1.563120292625312e-06,
      "loss": 0.43,
      "step": 111050
    },
    {
      "epoch": 2.6053819546227355,
      "grad_norm": 0.2966357469558716,
      "learning_rate": 1.5540368956156563e-06,
      "loss": 0.4369,
      "step": 111100
    },
    {
      "epoch": 2.6065544937562293,
      "grad_norm": 0.17771057784557343,
      "learning_rate": 1.5449785256367504e-06,
      "loss": 0.4105,
      "step": 111150
    },
    {
      "epoch": 2.6077270328897226,
      "grad_norm": 0.28736039996147156,
      "learning_rate": 1.5359451995489853e-06,
      "loss": 0.4543,
      "step": 111200
    },
    {
      "epoch": 2.6088995720232164,
      "grad_norm": 0.588637113571167,
      "learning_rate": 1.5269369341661217e-06,
      "loss": 0.4448,
      "step": 111250
    },
    {
      "epoch": 2.61007211115671,
      "grad_norm": 0.21191035211086273,
      "learning_rate": 1.5179537462552918e-06,
      "loss": 0.4437,
      "step": 111300
    },
    {
      "epoch": 2.6112446502902036,
      "grad_norm": 0.3065680265426636,
      "learning_rate": 1.5089956525369376e-06,
      "loss": 0.4584,
      "step": 111350
    },
    {
      "epoch": 2.612417189423697,
      "grad_norm": 0.255293607711792,
      "learning_rate": 1.5000626696848046e-06,
      "loss": 0.4474,
      "step": 111400
    },
    {
      "epoch": 2.6135897285571907,
      "grad_norm": 0.256208211183548,
      "learning_rate": 1.4911548143258864e-06,
      "loss": 0.4107,
      "step": 111450
    },
    {
      "epoch": 2.614762267690684,
      "grad_norm": 0.2743164598941803,
      "learning_rate": 1.4822721030404197e-06,
      "loss": 0.4546,
      "step": 111500
    },
    {
      "epoch": 2.615934806824178,
      "grad_norm": 0.30595290660858154,
      "learning_rate": 1.4734145523618292e-06,
      "loss": 0.4448,
      "step": 111550
    },
    {
      "epoch": 2.6171073459576712,
      "grad_norm": 0.38558539748191833,
      "learning_rate": 1.464582178776719e-06,
      "loss": 0.4751,
      "step": 111600
    },
    {
      "epoch": 2.618279885091165,
      "grad_norm": 0.6274518370628357,
      "learning_rate": 1.4557749987248197e-06,
      "loss": 0.4444,
      "step": 111650
    },
    {
      "epoch": 2.6194524242246584,
      "grad_norm": 0.17838767170906067,
      "learning_rate": 1.4469930285989791e-06,
      "loss": 0.4032,
      "step": 111700
    },
    {
      "epoch": 2.620624963358152,
      "grad_norm": 0.3117484748363495,
      "learning_rate": 1.438236284745113e-06,
      "loss": 0.4743,
      "step": 111750
    },
    {
      "epoch": 2.6217975024916456,
      "grad_norm": 0.48597002029418945,
      "learning_rate": 1.4295047834621887e-06,
      "loss": 0.4499,
      "step": 111800
    },
    {
      "epoch": 2.6229700416251394,
      "grad_norm": 0.21257025003433228,
      "learning_rate": 1.4207985410021929e-06,
      "loss": 0.4823,
      "step": 111850
    },
    {
      "epoch": 2.6241425807586327,
      "grad_norm": 0.28345075249671936,
      "learning_rate": 1.4121175735700876e-06,
      "loss": 0.4312,
      "step": 111900
    },
    {
      "epoch": 2.6253151198921265,
      "grad_norm": 0.34406864643096924,
      "learning_rate": 1.4034618973237995e-06,
      "loss": 0.4539,
      "step": 111950
    },
    {
      "epoch": 2.62648765902562,
      "grad_norm": 0.338092178106308,
      "learning_rate": 1.394831528374173e-06,
      "loss": 0.4645,
      "step": 112000
    },
    {
      "epoch": 2.62648765902562,
      "eval_loss": 0.7604480385780334,
      "eval_runtime": 120.1281,
      "eval_samples_per_second": 83.244,
      "eval_steps_per_second": 20.811,
      "step": 112000
    },
    {
      "epoch": 2.6276601981591137,
      "grad_norm": 0.2766839265823364,
      "learning_rate": 1.386226482784957e-06,
      "loss": 0.4396,
      "step": 112050
    },
    {
      "epoch": 2.628832737292607,
      "grad_norm": 0.401177316904068,
      "learning_rate": 1.3776467765727573e-06,
      "loss": 0.4279,
      "step": 112100
    },
    {
      "epoch": 2.630005276426101,
      "grad_norm": 0.24089358747005463,
      "learning_rate": 1.3690924257070219e-06,
      "loss": 0.4605,
      "step": 112150
    },
    {
      "epoch": 2.631177815559594,
      "grad_norm": 0.4922913908958435,
      "learning_rate": 1.3605634461099992e-06,
      "loss": 0.4601,
      "step": 112200
    },
    {
      "epoch": 2.632350354693088,
      "grad_norm": 0.2591620981693268,
      "learning_rate": 1.3520598536567202e-06,
      "loss": 0.4231,
      "step": 112250
    },
    {
      "epoch": 2.6335228938265813,
      "grad_norm": 0.32473495602607727,
      "learning_rate": 1.3435816641749544e-06,
      "loss": 0.4394,
      "step": 112300
    },
    {
      "epoch": 2.634695432960075,
      "grad_norm": 0.4058184027671814,
      "learning_rate": 1.3351288934452e-06,
      "loss": 0.4672,
      "step": 112350
    },
    {
      "epoch": 2.6358679720935685,
      "grad_norm": 0.3252493739128113,
      "learning_rate": 1.3267015572006303e-06,
      "loss": 0.4288,
      "step": 112400
    },
    {
      "epoch": 2.6370405112270623,
      "grad_norm": 0.46908074617385864,
      "learning_rate": 1.3182996711270879e-06,
      "loss": 0.4518,
      "step": 112450
    },
    {
      "epoch": 2.6382130503605556,
      "grad_norm": 0.30049848556518555,
      "learning_rate": 1.3099232508630383e-06,
      "loss": 0.4798,
      "step": 112500
    },
    {
      "epoch": 2.6393855894940494,
      "grad_norm": 0.40161803364753723,
      "learning_rate": 1.3015723119995516e-06,
      "loss": 0.4266,
      "step": 112550
    },
    {
      "epoch": 2.640558128627543,
      "grad_norm": 0.24197891354560852,
      "learning_rate": 1.2932468700802625e-06,
      "loss": 0.4773,
      "step": 112600
    },
    {
      "epoch": 2.6417306677610366,
      "grad_norm": 0.24134118854999542,
      "learning_rate": 1.284946940601362e-06,
      "loss": 0.4336,
      "step": 112650
    },
    {
      "epoch": 2.64290320689453,
      "grad_norm": 0.49564453959465027,
      "learning_rate": 1.2766725390115357e-06,
      "loss": 0.4421,
      "step": 112700
    },
    {
      "epoch": 2.6440757460280238,
      "grad_norm": 0.4778376519680023,
      "learning_rate": 1.268423680711971e-06,
      "loss": 0.4426,
      "step": 112750
    },
    {
      "epoch": 2.645248285161517,
      "grad_norm": 0.2649730145931244,
      "learning_rate": 1.2602003810563006e-06,
      "loss": 0.435,
      "step": 112800
    },
    {
      "epoch": 2.646420824295011,
      "grad_norm": 0.22166375815868378,
      "learning_rate": 1.2520026553505914e-06,
      "loss": 0.4499,
      "step": 112850
    },
    {
      "epoch": 2.6475933634285043,
      "grad_norm": 0.3655090928077698,
      "learning_rate": 1.243830518853306e-06,
      "loss": 0.4633,
      "step": 112900
    },
    {
      "epoch": 2.648765902561998,
      "grad_norm": 0.4284992516040802,
      "learning_rate": 1.235683986775274e-06,
      "loss": 0.4599,
      "step": 112950
    },
    {
      "epoch": 2.6499384416954914,
      "grad_norm": 0.47078195214271545,
      "learning_rate": 1.227563074279679e-06,
      "loss": 0.4163,
      "step": 113000
    },
    {
      "epoch": 2.651110980828985,
      "grad_norm": 0.4579053819179535,
      "learning_rate": 1.219467796482005e-06,
      "loss": 0.4411,
      "step": 113050
    },
    {
      "epoch": 2.652283519962479,
      "grad_norm": 0.3051629066467285,
      "learning_rate": 1.2113981684500325e-06,
      "loss": 0.4095,
      "step": 113100
    },
    {
      "epoch": 2.6534560590959724,
      "grad_norm": 0.1969480663537979,
      "learning_rate": 1.2033542052037932e-06,
      "loss": 0.4153,
      "step": 113150
    },
    {
      "epoch": 2.6546285982294657,
      "grad_norm": 0.5482661724090576,
      "learning_rate": 1.1953359217155541e-06,
      "loss": 0.4291,
      "step": 113200
    },
    {
      "epoch": 2.6558011373629595,
      "grad_norm": 0.4328313171863556,
      "learning_rate": 1.1873433329097783e-06,
      "loss": 0.4651,
      "step": 113250
    },
    {
      "epoch": 2.6569736764964533,
      "grad_norm": 0.5103317499160767,
      "learning_rate": 1.1793764536631124e-06,
      "loss": 0.4352,
      "step": 113300
    },
    {
      "epoch": 2.6581462156299467,
      "grad_norm": 0.35350096225738525,
      "learning_rate": 1.171435298804338e-06,
      "loss": 0.4382,
      "step": 113350
    },
    {
      "epoch": 2.65931875476344,
      "grad_norm": 0.4845135509967804,
      "learning_rate": 1.1635198831143668e-06,
      "loss": 0.4314,
      "step": 113400
    },
    {
      "epoch": 2.660491293896934,
      "grad_norm": 0.4101630449295044,
      "learning_rate": 1.1556302213261933e-06,
      "loss": 0.4409,
      "step": 113450
    },
    {
      "epoch": 2.6616638330304276,
      "grad_norm": 0.4817342162132263,
      "learning_rate": 1.1477663281248824e-06,
      "loss": 0.4857,
      "step": 113500
    },
    {
      "epoch": 2.662836372163921,
      "grad_norm": 0.23567914962768555,
      "learning_rate": 1.1399282181475323e-06,
      "loss": 0.4404,
      "step": 113550
    },
    {
      "epoch": 2.6640089112974144,
      "grad_norm": 0.5225167274475098,
      "learning_rate": 1.1321159059832531e-06,
      "loss": 0.4453,
      "step": 113600
    },
    {
      "epoch": 2.665181450430908,
      "grad_norm": 0.4087280333042145,
      "learning_rate": 1.124329406173133e-06,
      "loss": 0.4752,
      "step": 113650
    },
    {
      "epoch": 2.666353989564402,
      "grad_norm": 0.5479738116264343,
      "learning_rate": 1.1165687332102236e-06,
      "loss": 0.4586,
      "step": 113700
    },
    {
      "epoch": 2.6675265286978953,
      "grad_norm": 0.3673045039176941,
      "learning_rate": 1.1088339015394938e-06,
      "loss": 0.4439,
      "step": 113750
    },
    {
      "epoch": 2.6686990678313887,
      "grad_norm": 0.24642911553382874,
      "learning_rate": 1.1011249255578271e-06,
      "loss": 0.4714,
      "step": 113800
    },
    {
      "epoch": 2.6698716069648825,
      "grad_norm": 0.4746417999267578,
      "learning_rate": 1.0934418196139673e-06,
      "loss": 0.4338,
      "step": 113850
    },
    {
      "epoch": 2.6710441460983763,
      "grad_norm": 0.317945659160614,
      "learning_rate": 1.0857845980085202e-06,
      "loss": 0.4186,
      "step": 113900
    },
    {
      "epoch": 2.6722166852318696,
      "grad_norm": 0.31580206751823425,
      "learning_rate": 1.0781532749939e-06,
      "loss": 0.4531,
      "step": 113950
    },
    {
      "epoch": 2.673389224365363,
      "grad_norm": 0.36482056975364685,
      "learning_rate": 1.0705478647743278e-06,
      "loss": 0.4382,
      "step": 114000
    },
    {
      "epoch": 2.673389224365363,
      "eval_loss": 0.7605780959129333,
      "eval_runtime": 120.306,
      "eval_samples_per_second": 83.121,
      "eval_steps_per_second": 20.78,
      "step": 114000
    },
    {
      "epoch": 2.6745617634988568,
      "grad_norm": 0.2618211805820465,
      "learning_rate": 1.0629683815057833e-06,
      "loss": 0.4677,
      "step": 114050
    },
    {
      "epoch": 2.6757343026323506,
      "grad_norm": 0.43895527720451355,
      "learning_rate": 1.0554148392959968e-06,
      "loss": 0.4825,
      "step": 114100
    },
    {
      "epoch": 2.676906841765844,
      "grad_norm": 0.37153732776641846,
      "learning_rate": 1.0478872522044065e-06,
      "loss": 0.4674,
      "step": 114150
    },
    {
      "epoch": 2.6780793808993373,
      "grad_norm": 0.36059144139289856,
      "learning_rate": 1.0403856342421413e-06,
      "loss": 0.4535,
      "step": 114200
    },
    {
      "epoch": 2.679251920032831,
      "grad_norm": 0.4330964684486389,
      "learning_rate": 1.0329099993720025e-06,
      "loss": 0.4276,
      "step": 114250
    },
    {
      "epoch": 2.680424459166325,
      "grad_norm": 0.39433833956718445,
      "learning_rate": 1.0254603615084163e-06,
      "loss": 0.4761,
      "step": 114300
    },
    {
      "epoch": 2.6815969982998182,
      "grad_norm": 0.35907211899757385,
      "learning_rate": 1.0180367345174307e-06,
      "loss": 0.4622,
      "step": 114350
    },
    {
      "epoch": 2.6827695374333116,
      "grad_norm": 0.24847599864006042,
      "learning_rate": 1.0106391322166731e-06,
      "loss": 0.4393,
      "step": 114400
    },
    {
      "epoch": 2.6839420765668054,
      "grad_norm": 0.34075719118118286,
      "learning_rate": 1.0032675683753356e-06,
      "loss": 0.458,
      "step": 114450
    },
    {
      "epoch": 2.685114615700299,
      "grad_norm": 0.18606413900852203,
      "learning_rate": 9.959220567141397e-07,
      "loss": 0.416,
      "step": 114500
    },
    {
      "epoch": 2.6862871548337925,
      "grad_norm": 0.4286447763442993,
      "learning_rate": 9.88602610905322e-07,
      "loss": 0.4364,
      "step": 114550
    },
    {
      "epoch": 2.687459693967286,
      "grad_norm": 0.48511120676994324,
      "learning_rate": 9.813092445725963e-07,
      "loss": 0.414,
      "step": 114600
    },
    {
      "epoch": 2.6886322331007797,
      "grad_norm": 0.39729177951812744,
      "learning_rate": 9.74041971291142e-07,
      "loss": 0.4663,
      "step": 114650
    },
    {
      "epoch": 2.6898047722342735,
      "grad_norm": 0.35178059339523315,
      "learning_rate": 9.66800804587562e-07,
      "loss": 0.4775,
      "step": 114700
    },
    {
      "epoch": 2.690977311367767,
      "grad_norm": 0.29236966371536255,
      "learning_rate": 9.59585757939877e-07,
      "loss": 0.4001,
      "step": 114750
    },
    {
      "epoch": 2.69214985050126,
      "grad_norm": 0.3083474934101105,
      "learning_rate": 9.523968447774811e-07,
      "loss": 0.4191,
      "step": 114800
    },
    {
      "epoch": 2.693322389634754,
      "grad_norm": 0.31600192189216614,
      "learning_rate": 9.452340784811325e-07,
      "loss": 0.4553,
      "step": 114850
    },
    {
      "epoch": 2.694494928768248,
      "grad_norm": 0.27664491534233093,
      "learning_rate": 9.380974723829183e-07,
      "loss": 0.443,
      "step": 114900
    },
    {
      "epoch": 2.695667467901741,
      "grad_norm": 0.2439890205860138,
      "learning_rate": 9.309870397662395e-07,
      "loss": 0.4135,
      "step": 114950
    },
    {
      "epoch": 2.696840007035235,
      "grad_norm": 0.30855485796928406,
      "learning_rate": 9.239027938657696e-07,
      "loss": 0.4564,
      "step": 115000
    },
    {
      "epoch": 2.6980125461687283,
      "grad_norm": 0.3810189962387085,
      "learning_rate": 9.168447478674507e-07,
      "loss": 0.4996,
      "step": 115050
    },
    {
      "epoch": 2.699185085302222,
      "grad_norm": 0.21068093180656433,
      "learning_rate": 9.098129149084528e-07,
      "loss": 0.4042,
      "step": 115100
    },
    {
      "epoch": 2.7003576244357155,
      "grad_norm": 0.24688518047332764,
      "learning_rate": 9.028073080771626e-07,
      "loss": 0.4675,
      "step": 115150
    },
    {
      "epoch": 2.7015301635692093,
      "grad_norm": 0.277484655380249,
      "learning_rate": 8.958279404131398e-07,
      "loss": 0.4656,
      "step": 115200
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 0.38800302147865295,
      "learning_rate": 8.888748249071194e-07,
      "loss": 0.3988,
      "step": 115250
    },
    {
      "epoch": 2.7038752418361964,
      "grad_norm": 0.35175368189811707,
      "learning_rate": 8.819479745009606e-07,
      "loss": 0.4372,
      "step": 115300
    },
    {
      "epoch": 2.70504778096969,
      "grad_norm": 0.28176772594451904,
      "learning_rate": 8.750474020876437e-07,
      "loss": 0.4512,
      "step": 115350
    },
    {
      "epoch": 2.7062203201031836,
      "grad_norm": 0.24590659141540527,
      "learning_rate": 8.681731205112375e-07,
      "loss": 0.4411,
      "step": 115400
    },
    {
      "epoch": 2.707392859236677,
      "grad_norm": 0.2667090892791748,
      "learning_rate": 8.613251425668655e-07,
      "loss": 0.4333,
      "step": 115450
    },
    {
      "epoch": 2.7085653983701707,
      "grad_norm": 0.2971816658973694,
      "learning_rate": 8.545034810007068e-07,
      "loss": 0.4487,
      "step": 115500
    },
    {
      "epoch": 2.709737937503664,
      "grad_norm": 0.37053239345550537,
      "learning_rate": 8.477081485099447e-07,
      "loss": 0.4472,
      "step": 115550
    },
    {
      "epoch": 2.710910476637158,
      "grad_norm": 0.3643589913845062,
      "learning_rate": 8.40939157742765e-07,
      "loss": 0.4431,
      "step": 115600
    },
    {
      "epoch": 2.7120830157706513,
      "grad_norm": 0.31855759024620056,
      "learning_rate": 8.341965212983177e-07,
      "loss": 0.4541,
      "step": 115650
    },
    {
      "epoch": 2.713255554904145,
      "grad_norm": 0.33617687225341797,
      "learning_rate": 8.274802517267072e-07,
      "loss": 0.4203,
      "step": 115700
    },
    {
      "epoch": 2.7144280940376384,
      "grad_norm": 0.22674153745174408,
      "learning_rate": 8.207903615289486e-07,
      "loss": 0.441,
      "step": 115750
    },
    {
      "epoch": 2.715600633171132,
      "grad_norm": 0.445305198431015,
      "learning_rate": 8.141268631569698e-07,
      "loss": 0.4415,
      "step": 115800
    },
    {
      "epoch": 2.7167731723046256,
      "grad_norm": 0.49731484055519104,
      "learning_rate": 8.074897690135663e-07,
      "loss": 0.454,
      "step": 115850
    },
    {
      "epoch": 2.7179457114381194,
      "grad_norm": 0.20175763964653015,
      "learning_rate": 8.008790914523928e-07,
      "loss": 0.4615,
      "step": 115900
    },
    {
      "epoch": 2.7191182505716127,
      "grad_norm": 0.24727466702461243,
      "learning_rate": 7.942948427779318e-07,
      "loss": 0.4559,
      "step": 115950
    },
    {
      "epoch": 2.7202907897051065,
      "grad_norm": 0.39432913064956665,
      "learning_rate": 7.877370352454766e-07,
      "loss": 0.428,
      "step": 116000
    },
    {
      "epoch": 2.7202907897051065,
      "eval_loss": 0.7605932354927063,
      "eval_runtime": 120.1265,
      "eval_samples_per_second": 83.246,
      "eval_steps_per_second": 20.811,
      "step": 116000
    },
    {
      "epoch": 2.7214633288386,
      "grad_norm": 0.36692777276039124,
      "learning_rate": 7.812056810611001e-07,
      "loss": 0.4229,
      "step": 116050
    },
    {
      "epoch": 2.7226358679720937,
      "grad_norm": 0.49705183506011963,
      "learning_rate": 7.747007923816445e-07,
      "loss": 0.4108,
      "step": 116100
    },
    {
      "epoch": 2.723808407105587,
      "grad_norm": 0.3181207776069641,
      "learning_rate": 7.682223813146849e-07,
      "loss": 0.4686,
      "step": 116150
    },
    {
      "epoch": 2.724980946239081,
      "grad_norm": 0.3524797856807709,
      "learning_rate": 7.617704599185172e-07,
      "loss": 0.496,
      "step": 116200
    },
    {
      "epoch": 2.726153485372574,
      "grad_norm": 0.2660093307495117,
      "learning_rate": 7.553450402021333e-07,
      "loss": 0.4081,
      "step": 116250
    },
    {
      "epoch": 2.727326024506068,
      "grad_norm": 0.22030077874660492,
      "learning_rate": 7.489461341251919e-07,
      "loss": 0.4857,
      "step": 116300
    },
    {
      "epoch": 2.7284985636395613,
      "grad_norm": 0.36882832646369934,
      "learning_rate": 7.425737535980087e-07,
      "loss": 0.411,
      "step": 116350
    },
    {
      "epoch": 2.729671102773055,
      "grad_norm": 0.6412743926048279,
      "learning_rate": 7.362279104815211e-07,
      "loss": 0.4959,
      "step": 116400
    },
    {
      "epoch": 2.7308436419065485,
      "grad_norm": 0.3851150572299957,
      "learning_rate": 7.299086165872793e-07,
      "loss": 0.4399,
      "step": 116450
    },
    {
      "epoch": 2.7320161810400423,
      "grad_norm": 0.4946553111076355,
      "learning_rate": 7.23615883677406e-07,
      "loss": 0.4374,
      "step": 116500
    },
    {
      "epoch": 2.7331887201735356,
      "grad_norm": 0.4744415879249573,
      "learning_rate": 7.173497234646004e-07,
      "loss": 0.5091,
      "step": 116550
    },
    {
      "epoch": 2.7343612593070294,
      "grad_norm": 0.4725281894207001,
      "learning_rate": 7.11110147612088e-07,
      "loss": 0.4237,
      "step": 116600
    },
    {
      "epoch": 2.735533798440523,
      "grad_norm": 0.3754351735115051,
      "learning_rate": 7.048971677336236e-07,
      "loss": 0.4466,
      "step": 116650
    },
    {
      "epoch": 2.7367063375740166,
      "grad_norm": 0.32938095927238464,
      "learning_rate": 6.987107953934518e-07,
      "loss": 0.4402,
      "step": 116700
    },
    {
      "epoch": 2.73787887670751,
      "grad_norm": 0.2837025821208954,
      "learning_rate": 6.925510421062952e-07,
      "loss": 0.4398,
      "step": 116750
    },
    {
      "epoch": 2.7390514158410038,
      "grad_norm": 0.46226873993873596,
      "learning_rate": 6.86417919337331e-07,
      "loss": 0.4453,
      "step": 116800
    },
    {
      "epoch": 2.740223954974497,
      "grad_norm": 0.3018479645252228,
      "learning_rate": 6.803114385021675e-07,
      "loss": 0.4353,
      "step": 116850
    },
    {
      "epoch": 2.741396494107991,
      "grad_norm": 0.30671167373657227,
      "learning_rate": 6.742316109668211e-07,
      "loss": 0.4583,
      "step": 116900
    },
    {
      "epoch": 2.7425690332414847,
      "grad_norm": 0.28398552536964417,
      "learning_rate": 6.681784480477083e-07,
      "loss": 0.4493,
      "step": 116950
    },
    {
      "epoch": 2.743741572374978,
      "grad_norm": 0.4326847493648529,
      "learning_rate": 6.621519610116028e-07,
      "loss": 0.4802,
      "step": 117000
    },
    {
      "epoch": 2.7449141115084714,
      "grad_norm": 0.3638678193092346,
      "learning_rate": 6.561521610756338e-07,
      "loss": 0.446,
      "step": 117050
    },
    {
      "epoch": 2.746086650641965,
      "grad_norm": 0.5198617577552795,
      "learning_rate": 6.501790594072549e-07,
      "loss": 0.4282,
      "step": 117100
    },
    {
      "epoch": 2.747259189775459,
      "grad_norm": 0.31587666273117065,
      "learning_rate": 6.442326671242277e-07,
      "loss": 0.4527,
      "step": 117150
    },
    {
      "epoch": 2.7484317289089524,
      "grad_norm": 0.37923163175582886,
      "learning_rate": 6.383129952945954e-07,
      "loss": 0.4299,
      "step": 117200
    },
    {
      "epoch": 2.7496042680424457,
      "grad_norm": 0.26564833521842957,
      "learning_rate": 6.324200549366743e-07,
      "loss": 0.4139,
      "step": 117250
    },
    {
      "epoch": 2.7507768071759395,
      "grad_norm": 0.46068984270095825,
      "learning_rate": 6.265538570190154e-07,
      "loss": 0.4178,
      "step": 117300
    },
    {
      "epoch": 2.7519493463094333,
      "grad_norm": 0.4903983175754547,
      "learning_rate": 6.207144124604047e-07,
      "loss": 0.456,
      "step": 117350
    },
    {
      "epoch": 2.7531218854429267,
      "grad_norm": 0.3785475492477417,
      "learning_rate": 6.149017321298211e-07,
      "loss": 0.4492,
      "step": 117400
    },
    {
      "epoch": 2.75429442457642,
      "grad_norm": 0.42708781361579895,
      "learning_rate": 6.091158268464336e-07,
      "loss": 0.4472,
      "step": 117450
    },
    {
      "epoch": 2.755466963709914,
      "grad_norm": 0.3635287582874298,
      "learning_rate": 6.033567073795759e-07,
      "loss": 0.4462,
      "step": 117500
    },
    {
      "epoch": 2.7566395028434076,
      "grad_norm": 0.34959572553634644,
      "learning_rate": 5.976243844487184e-07,
      "loss": 0.4045,
      "step": 117550
    },
    {
      "epoch": 2.757812041976901,
      "grad_norm": 0.34141790866851807,
      "learning_rate": 5.919188687234627e-07,
      "loss": 0.4618,
      "step": 117600
    },
    {
      "epoch": 2.7589845811103944,
      "grad_norm": 0.2788066864013672,
      "learning_rate": 5.862401708235077e-07,
      "loss": 0.4471,
      "step": 117650
    },
    {
      "epoch": 2.760157120243888,
      "grad_norm": 0.24345402419567108,
      "learning_rate": 5.805883013186414e-07,
      "loss": 0.3926,
      "step": 117700
    },
    {
      "epoch": 2.761329659377382,
      "grad_norm": 0.3251081705093384,
      "learning_rate": 5.749632707287122e-07,
      "loss": 0.4699,
      "step": 117750
    },
    {
      "epoch": 2.7625021985108753,
      "grad_norm": 0.31520941853523254,
      "learning_rate": 5.693650895236152e-07,
      "loss": 0.4756,
      "step": 117800
    },
    {
      "epoch": 2.7636747376443687,
      "grad_norm": 0.39179277420043945,
      "learning_rate": 5.637937681232702e-07,
      "loss": 0.4344,
      "step": 117850
    },
    {
      "epoch": 2.7648472767778625,
      "grad_norm": 0.5771012306213379,
      "learning_rate": 5.582493168976022e-07,
      "loss": 0.509,
      "step": 117900
    },
    {
      "epoch": 2.7660198159113563,
      "grad_norm": 0.41858261823654175,
      "learning_rate": 5.527317461665194e-07,
      "loss": 0.4472,
      "step": 117950
    },
    {
      "epoch": 2.7671923550448496,
      "grad_norm": 0.36486420035362244,
      "learning_rate": 5.472410661999067e-07,
      "loss": 0.4322,
      "step": 118000
    },
    {
      "epoch": 2.7671923550448496,
      "eval_loss": 0.7604544758796692,
      "eval_runtime": 153.3839,
      "eval_samples_per_second": 65.196,
      "eval_steps_per_second": 16.299,
      "step": 118000
    },
    {
      "epoch": 2.768364894178343,
      "grad_norm": 0.29223397374153137,
      "learning_rate": 5.417772872175874e-07,
      "loss": 0.4438,
      "step": 118050
    },
    {
      "epoch": 2.7695374333118368,
      "grad_norm": 0.2611752450466156,
      "learning_rate": 5.363404193893184e-07,
      "loss": 0.4563,
      "step": 118100
    },
    {
      "epoch": 2.7707099724453306,
      "grad_norm": 0.2800996005535126,
      "learning_rate": 5.309304728347647e-07,
      "loss": 0.4518,
      "step": 118150
    },
    {
      "epoch": 2.771882511578824,
      "grad_norm": 0.3100888133049011,
      "learning_rate": 5.255474576234881e-07,
      "loss": 0.4251,
      "step": 118200
    },
    {
      "epoch": 2.7730550507123173,
      "grad_norm": 0.3349675238132477,
      "learning_rate": 5.201913837749123e-07,
      "loss": 0.4546,
      "step": 118250
    },
    {
      "epoch": 2.774227589845811,
      "grad_norm": 0.16044177114963531,
      "learning_rate": 5.148622612583259e-07,
      "loss": 0.4386,
      "step": 118300
    },
    {
      "epoch": 2.775400128979305,
      "grad_norm": 0.3597758710384369,
      "learning_rate": 5.095600999928445e-07,
      "loss": 0.4468,
      "step": 118350
    },
    {
      "epoch": 2.7765726681127982,
      "grad_norm": 0.4175521731376648,
      "learning_rate": 5.042849098474106e-07,
      "loss": 0.4448,
      "step": 118400
    },
    {
      "epoch": 2.7777452072462916,
      "grad_norm": 0.3369097411632538,
      "learning_rate": 4.990367006407515e-07,
      "loss": 0.4111,
      "step": 118450
    },
    {
      "epoch": 2.7789177463797854,
      "grad_norm": 0.4788096249103546,
      "learning_rate": 4.938154821413898e-07,
      "loss": 0.4564,
      "step": 118500
    },
    {
      "epoch": 2.780090285513279,
      "grad_norm": 0.39139455556869507,
      "learning_rate": 4.886212640675953e-07,
      "loss": 0.4487,
      "step": 118550
    },
    {
      "epoch": 2.7812628246467725,
      "grad_norm": 0.19084282219409943,
      "learning_rate": 4.83454056087394e-07,
      "loss": 0.461,
      "step": 118600
    },
    {
      "epoch": 2.782435363780266,
      "grad_norm": 0.426729679107666,
      "learning_rate": 4.783138678185328e-07,
      "loss": 0.4446,
      "step": 118650
    },
    {
      "epoch": 2.7836079029137597,
      "grad_norm": 0.39380595088005066,
      "learning_rate": 4.7320070882846345e-07,
      "loss": 0.4441,
      "step": 118700
    },
    {
      "epoch": 2.7847804420472535,
      "grad_norm": 0.30051031708717346,
      "learning_rate": 4.681145886343363e-07,
      "loss": 0.4299,
      "step": 118750
    },
    {
      "epoch": 2.785952981180747,
      "grad_norm": 0.22145739197731018,
      "learning_rate": 4.6305551670296377e-07,
      "loss": 0.4543,
      "step": 118800
    },
    {
      "epoch": 2.78712552031424,
      "grad_norm": 0.3962026536464691,
      "learning_rate": 4.5802350245082505e-07,
      "loss": 0.4744,
      "step": 118850
    },
    {
      "epoch": 2.788298059447734,
      "grad_norm": 0.6005562543869019,
      "learning_rate": 4.530185552440247e-07,
      "loss": 0.4724,
      "step": 118900
    },
    {
      "epoch": 2.789470598581228,
      "grad_norm": 0.3911191523075104,
      "learning_rate": 4.4804068439829594e-07,
      "loss": 0.4935,
      "step": 118950
    },
    {
      "epoch": 2.790643137714721,
      "grad_norm": 0.35149627923965454,
      "learning_rate": 4.430898991789722e-07,
      "loss": 0.46,
      "step": 119000
    },
    {
      "epoch": 2.791815676848215,
      "grad_norm": 0.3960738182067871,
      "learning_rate": 4.381662088009741e-07,
      "loss": 0.4643,
      "step": 119050
    },
    {
      "epoch": 2.7929882159817083,
      "grad_norm": 0.3013269305229187,
      "learning_rate": 4.332696224287841e-07,
      "loss": 0.5105,
      "step": 119100
    },
    {
      "epoch": 2.794160755115202,
      "grad_norm": 0.3210190534591675,
      "learning_rate": 4.284001491764433e-07,
      "loss": 0.4651,
      "step": 119150
    },
    {
      "epoch": 2.7953332942486955,
      "grad_norm": 0.2698346674442291,
      "learning_rate": 4.2355779810752005e-07,
      "loss": 0.4491,
      "step": 119200
    },
    {
      "epoch": 2.7965058333821893,
      "grad_norm": 0.39560815691947937,
      "learning_rate": 4.1874257823510954e-07,
      "loss": 0.4761,
      "step": 119250
    },
    {
      "epoch": 2.7976783725156826,
      "grad_norm": 0.40117958188056946,
      "learning_rate": 4.139544985217958e-07,
      "loss": 0.4459,
      "step": 119300
    },
    {
      "epoch": 2.7988509116491764,
      "grad_norm": 0.42156824469566345,
      "learning_rate": 4.091935678796566e-07,
      "loss": 0.4665,
      "step": 119350
    },
    {
      "epoch": 2.80002345078267,
      "grad_norm": 0.33493685722351074,
      "learning_rate": 4.0445979517023335e-07,
      "loss": 0.4435,
      "step": 119400
    },
    {
      "epoch": 2.8011959899161636,
      "grad_norm": 0.3936793804168701,
      "learning_rate": 3.997531892045164e-07,
      "loss": 0.4203,
      "step": 119450
    },
    {
      "epoch": 2.802368529049657,
      "grad_norm": 0.17558225989341736,
      "learning_rate": 3.9507375874293316e-07,
      "loss": 0.456,
      "step": 119500
    },
    {
      "epoch": 2.8035410681831507,
      "grad_norm": 0.719352662563324,
      "learning_rate": 3.904215124953281e-07,
      "loss": 0.4476,
      "step": 119550
    },
    {
      "epoch": 2.804713607316644,
      "grad_norm": 0.28253671526908875,
      "learning_rate": 3.8579645912094783e-07,
      "loss": 0.4261,
      "step": 119600
    },
    {
      "epoch": 2.805886146450138,
      "grad_norm": 0.3792881965637207,
      "learning_rate": 3.811986072284246e-07,
      "loss": 0.4227,
      "step": 119650
    },
    {
      "epoch": 2.8070586855836313,
      "grad_norm": 0.2329983115196228,
      "learning_rate": 3.7662796537576095e-07,
      "loss": 0.431,
      "step": 119700
    },
    {
      "epoch": 2.808231224717125,
      "grad_norm": 0.3041106164455414,
      "learning_rate": 3.720845420703117e-07,
      "loss": 0.4664,
      "step": 119750
    },
    {
      "epoch": 2.8094037638506184,
      "grad_norm": 0.33695608377456665,
      "learning_rate": 3.6756834576877397e-07,
      "loss": 0.4297,
      "step": 119800
    },
    {
      "epoch": 2.810576302984112,
      "grad_norm": 0.43767425417900085,
      "learning_rate": 3.630793848771585e-07,
      "loss": 0.439,
      "step": 119850
    },
    {
      "epoch": 2.8117488421176056,
      "grad_norm": 0.4500817656517029,
      "learning_rate": 3.586176677507935e-07,
      "loss": 0.475,
      "step": 119900
    },
    {
      "epoch": 2.8129213812510994,
      "grad_norm": 0.21667850017547607,
      "learning_rate": 3.541832026942909e-07,
      "loss": 0.4731,
      "step": 119950
    },
    {
      "epoch": 2.8140939203845927,
      "grad_norm": 0.31868141889572144,
      "learning_rate": 3.4977599796153834e-07,
      "loss": 0.4704,
      "step": 120000
    },
    {
      "epoch": 2.8140939203845927,
      "eval_loss": 0.760302722454071,
      "eval_runtime": 126.9019,
      "eval_samples_per_second": 78.801,
      "eval_steps_per_second": 19.7,
      "step": 120000
    },
    {
      "epoch": 2.8152664595180865,
      "grad_norm": 0.2851298153400421,
      "learning_rate": 3.453960617556873e-07,
      "loss": 0.4613,
      "step": 120050
    },
    {
      "epoch": 2.81643899865158,
      "grad_norm": 0.5402149558067322,
      "learning_rate": 3.410434022291348e-07,
      "loss": 0.4993,
      "step": 120100
    },
    {
      "epoch": 2.8176115377850737,
      "grad_norm": 0.379361629486084,
      "learning_rate": 3.3671802748350023e-07,
      "loss": 0.4279,
      "step": 120150
    },
    {
      "epoch": 2.818784076918567,
      "grad_norm": 0.41728854179382324,
      "learning_rate": 3.324199455696253e-07,
      "loss": 0.4431,
      "step": 120200
    },
    {
      "epoch": 2.819956616052061,
      "grad_norm": 0.3575127422809601,
      "learning_rate": 3.2814916448754896e-07,
      "loss": 0.4274,
      "step": 120250
    },
    {
      "epoch": 2.821129155185554,
      "grad_norm": 0.35575464367866516,
      "learning_rate": 3.239056921864975e-07,
      "loss": 0.4286,
      "step": 120300
    },
    {
      "epoch": 2.822301694319048,
      "grad_norm": 0.2302055060863495,
      "learning_rate": 3.196895365648594e-07,
      "loss": 0.4601,
      "step": 120350
    },
    {
      "epoch": 2.8234742334525413,
      "grad_norm": 0.4180549383163452,
      "learning_rate": 3.155007054701875e-07,
      "loss": 0.4686,
      "step": 120400
    },
    {
      "epoch": 2.824646772586035,
      "grad_norm": 0.3268042206764221,
      "learning_rate": 3.113392066991699e-07,
      "loss": 0.4551,
      "step": 120450
    },
    {
      "epoch": 2.8258193117195285,
      "grad_norm": 0.6235744953155518,
      "learning_rate": 3.072050479976257e-07,
      "loss": 0.4432,
      "step": 120500
    },
    {
      "epoch": 2.8269918508530223,
      "grad_norm": 0.36757954955101013,
      "learning_rate": 3.0309823706047955e-07,
      "loss": 0.4199,
      "step": 120550
    },
    {
      "epoch": 2.8281643899865156,
      "grad_norm": 0.25212380290031433,
      "learning_rate": 2.9901878153175856e-07,
      "loss": 0.4415,
      "step": 120600
    },
    {
      "epoch": 2.8293369291200094,
      "grad_norm": 0.2914559245109558,
      "learning_rate": 2.949666890045705e-07,
      "loss": 0.4488,
      "step": 120650
    },
    {
      "epoch": 2.830509468253503,
      "grad_norm": 0.36811453104019165,
      "learning_rate": 2.909419670210939e-07,
      "loss": 0.4077,
      "step": 120700
    },
    {
      "epoch": 2.8316820073869966,
      "grad_norm": 0.3065442740917206,
      "learning_rate": 2.8694462307256144e-07,
      "loss": 0.4537,
      "step": 120750
    },
    {
      "epoch": 2.83285454652049,
      "grad_norm": 0.18906259536743164,
      "learning_rate": 2.8297466459924646e-07,
      "loss": 0.4642,
      "step": 120800
    },
    {
      "epoch": 2.8340270856539838,
      "grad_norm": 0.43981626629829407,
      "learning_rate": 2.790320989904516e-07,
      "loss": 0.439,
      "step": 120850
    },
    {
      "epoch": 2.835199624787477,
      "grad_norm": 0.29611825942993164,
      "learning_rate": 2.751169335844883e-07,
      "loss": 0.4095,
      "step": 120900
    },
    {
      "epoch": 2.836372163920971,
      "grad_norm": 0.35202863812446594,
      "learning_rate": 2.7122917566867745e-07,
      "loss": 0.4434,
      "step": 120950
    },
    {
      "epoch": 2.8375447030544647,
      "grad_norm": 0.3014179468154907,
      "learning_rate": 2.6736883247931386e-07,
      "loss": 0.4316,
      "step": 121000
    },
    {
      "epoch": 2.838717242187958,
      "grad_norm": 0.3597581684589386,
      "learning_rate": 2.635359112016766e-07,
      "loss": 0.445,
      "step": 121050
    },
    {
      "epoch": 2.8398897813214514,
      "grad_norm": 0.44413867592811584,
      "learning_rate": 2.5973041896999396e-07,
      "loss": 0.4433,
      "step": 121100
    },
    {
      "epoch": 2.8410623204549452,
      "grad_norm": 0.2992899715900421,
      "learning_rate": 2.5595236286744983e-07,
      "loss": 0.4508,
      "step": 121150
    },
    {
      "epoch": 2.842234859588439,
      "grad_norm": 0.29224005341529846,
      "learning_rate": 2.522017499261542e-07,
      "loss": 0.3796,
      "step": 121200
    },
    {
      "epoch": 2.8434073987219324,
      "grad_norm": 0.45419633388519287,
      "learning_rate": 2.484785871271411e-07,
      "loss": 0.4417,
      "step": 121250
    },
    {
      "epoch": 2.8445799378554257,
      "grad_norm": 0.35271915793418884,
      "learning_rate": 2.4478288140034887e-07,
      "loss": 0.4151,
      "step": 121300
    },
    {
      "epoch": 2.8457524769889195,
      "grad_norm": 0.21427050232887268,
      "learning_rate": 2.411146396246117e-07,
      "loss": 0.4343,
      "step": 121350
    },
    {
      "epoch": 2.8469250161224133,
      "grad_norm": 0.2775605618953705,
      "learning_rate": 2.3747386862764286e-07,
      "loss": 0.4439,
      "step": 121400
    },
    {
      "epoch": 2.8480975552559067,
      "grad_norm": 0.21113553643226624,
      "learning_rate": 2.3386057518602845e-07,
      "loss": 0.4795,
      "step": 121450
    },
    {
      "epoch": 2.8492700943894,
      "grad_norm": 0.42563310265541077,
      "learning_rate": 2.3027476602520525e-07,
      "loss": 0.4513,
      "step": 121500
    },
    {
      "epoch": 2.850442633522894,
      "grad_norm": 0.44506320357322693,
      "learning_rate": 2.267164478194561e-07,
      "loss": 0.4464,
      "step": 121550
    },
    {
      "epoch": 2.8516151726563876,
      "grad_norm": 0.34536466002464294,
      "learning_rate": 2.2318562719189306e-07,
      "loss": 0.4313,
      "step": 121600
    },
    {
      "epoch": 2.852787711789881,
      "grad_norm": 0.522240936756134,
      "learning_rate": 2.1968231071445245e-07,
      "loss": 0.459,
      "step": 121650
    },
    {
      "epoch": 2.8539602509233744,
      "grad_norm": 0.23070359230041504,
      "learning_rate": 2.162065049078682e-07,
      "loss": 0.4318,
      "step": 121700
    },
    {
      "epoch": 2.855132790056868,
      "grad_norm": 0.45966511964797974,
      "learning_rate": 2.1275821624167692e-07,
      "loss": 0.4725,
      "step": 121750
    },
    {
      "epoch": 2.856305329190362,
      "grad_norm": 0.26394546031951904,
      "learning_rate": 2.0933745113418946e-07,
      "loss": 0.4535,
      "step": 121800
    },
    {
      "epoch": 2.8574778683238553,
      "grad_norm": 0.6088124513626099,
      "learning_rate": 2.0594421595249434e-07,
      "loss": 0.4447,
      "step": 121850
    },
    {
      "epoch": 2.8586504074573487,
      "grad_norm": 0.34002959728240967,
      "learning_rate": 2.0257851701243447e-07,
      "loss": 0.4213,
      "step": 121900
    },
    {
      "epoch": 2.8598229465908425,
      "grad_norm": 0.30365216732025146,
      "learning_rate": 1.9924036057860028e-07,
      "loss": 0.4594,
      "step": 121950
    },
    {
      "epoch": 2.8609954857243363,
      "grad_norm": 0.26748672127723694,
      "learning_rate": 1.959297528643167e-07,
      "loss": 0.4458,
      "step": 122000
    },
    {
      "epoch": 2.8609954857243363,
      "eval_loss": 0.7605468034744263,
      "eval_runtime": 149.9909,
      "eval_samples_per_second": 66.671,
      "eval_steps_per_second": 16.668,
      "step": 122000
    },
    {
      "epoch": 2.8621680248578296,
      "grad_norm": 0.24818964302539825,
      "learning_rate": 1.9264670003163122e-07,
      "loss": 0.3967,
      "step": 122050
    },
    {
      "epoch": 2.863340563991323,
      "grad_norm": 0.2628260552883148,
      "learning_rate": 1.8939120819130739e-07,
      "loss": 0.4062,
      "step": 122100
    },
    {
      "epoch": 2.8645131031248168,
      "grad_norm": 0.46461305022239685,
      "learning_rate": 1.8616328340280487e-07,
      "loss": 0.4196,
      "step": 122150
    },
    {
      "epoch": 2.8656856422583106,
      "grad_norm": 0.4432223439216614,
      "learning_rate": 1.829629316742759e-07,
      "loss": 0.4417,
      "step": 122200
    },
    {
      "epoch": 2.866858181391804,
      "grad_norm": 0.3733809292316437,
      "learning_rate": 1.7979015896254724e-07,
      "loss": 0.4584,
      "step": 122250
    },
    {
      "epoch": 2.8680307205252973,
      "grad_norm": 0.33776232600212097,
      "learning_rate": 1.7664497117311496e-07,
      "loss": 0.4565,
      "step": 122300
    },
    {
      "epoch": 2.869203259658791,
      "grad_norm": 0.3797738552093506,
      "learning_rate": 1.7352737416013287e-07,
      "loss": 0.4419,
      "step": 122350
    },
    {
      "epoch": 2.870375798792285,
      "grad_norm": 0.3527529835700989,
      "learning_rate": 1.7043737372639757e-07,
      "loss": 0.4711,
      "step": 122400
    },
    {
      "epoch": 2.8715483379257782,
      "grad_norm": 0.23802822828292847,
      "learning_rate": 1.6737497562333837e-07,
      "loss": 0.4625,
      "step": 122450
    },
    {
      "epoch": 2.8727208770592716,
      "grad_norm": 0.310773104429245,
      "learning_rate": 1.6434018555101405e-07,
      "loss": 0.4808,
      "step": 122500
    },
    {
      "epoch": 2.8738934161927654,
      "grad_norm": 0.2999224364757538,
      "learning_rate": 1.6133300915808947e-07,
      "loss": 0.4841,
      "step": 122550
    },
    {
      "epoch": 2.875065955326259,
      "grad_norm": 0.4947707951068878,
      "learning_rate": 1.5835345204183894e-07,
      "loss": 0.4409,
      "step": 122600
    },
    {
      "epoch": 2.8762384944597525,
      "grad_norm": 0.3518710136413574,
      "learning_rate": 1.5540151974812288e-07,
      "loss": 0.432,
      "step": 122650
    },
    {
      "epoch": 2.877411033593246,
      "grad_norm": 0.3782366216182709,
      "learning_rate": 1.5247721777138456e-07,
      "loss": 0.4292,
      "step": 122700
    },
    {
      "epoch": 2.8785835727267397,
      "grad_norm": 0.3066783547401428,
      "learning_rate": 1.4958055155464335e-07,
      "loss": 0.4376,
      "step": 122750
    },
    {
      "epoch": 2.8797561118602335,
      "grad_norm": 0.5176613330841064,
      "learning_rate": 1.4671152648947472e-07,
      "loss": 0.4435,
      "step": 122800
    },
    {
      "epoch": 2.880928650993727,
      "grad_norm": 0.363637775182724,
      "learning_rate": 1.438701479160104e-07,
      "loss": 0.4998,
      "step": 122850
    },
    {
      "epoch": 2.8821011901272207,
      "grad_norm": 0.5243440866470337,
      "learning_rate": 1.4105642112291817e-07,
      "loss": 0.4635,
      "step": 122900
    },
    {
      "epoch": 2.883273729260714,
      "grad_norm": 0.3836916983127594,
      "learning_rate": 1.3827035134740208e-07,
      "loss": 0.4799,
      "step": 122950
    },
    {
      "epoch": 2.884446268394208,
      "grad_norm": 0.4384290277957916,
      "learning_rate": 1.3551194377518394e-07,
      "loss": 0.4799,
      "step": 123000
    },
    {
      "epoch": 2.885618807527701,
      "grad_norm": 0.4223952293395996,
      "learning_rate": 1.3278120354050183e-07,
      "loss": 0.4825,
      "step": 123050
    },
    {
      "epoch": 2.886791346661195,
      "grad_norm": 0.29896727204322815,
      "learning_rate": 1.300781357260933e-07,
      "loss": 0.4293,
      "step": 123100
    },
    {
      "epoch": 2.8879638857946883,
      "grad_norm": 0.4886283576488495,
      "learning_rate": 1.2740274536319208e-07,
      "loss": 0.4283,
      "step": 123150
    },
    {
      "epoch": 2.889136424928182,
      "grad_norm": 0.15394780039787292,
      "learning_rate": 1.2475503743151316e-07,
      "loss": 0.4275,
      "step": 123200
    },
    {
      "epoch": 2.8903089640616755,
      "grad_norm": 0.3866002857685089,
      "learning_rate": 1.2213501685924778e-07,
      "loss": 0.4532,
      "step": 123250
    },
    {
      "epoch": 2.8914815031951693,
      "grad_norm": 0.43494370579719543,
      "learning_rate": 1.1954268852304995e-07,
      "loss": 0.4515,
      "step": 123300
    },
    {
      "epoch": 2.8926540423286626,
      "grad_norm": 0.4415574073791504,
      "learning_rate": 1.16978057248035e-07,
      "loss": 0.475,
      "step": 123350
    },
    {
      "epoch": 2.8938265814621564,
      "grad_norm": 0.2593821585178375,
      "learning_rate": 1.144411278077595e-07,
      "loss": 0.4521,
      "step": 123400
    },
    {
      "epoch": 2.89499912059565,
      "grad_norm": 0.25892919301986694,
      "learning_rate": 1.1193190492422456e-07,
      "loss": 0.4554,
      "step": 123450
    },
    {
      "epoch": 2.8961716597291436,
      "grad_norm": 0.34151187539100647,
      "learning_rate": 1.0945039326785422e-07,
      "loss": 0.4551,
      "step": 123500
    },
    {
      "epoch": 2.897344198862637,
      "grad_norm": 0.333769828081131,
      "learning_rate": 1.069965974574988e-07,
      "loss": 0.4319,
      "step": 123550
    },
    {
      "epoch": 2.8985167379961307,
      "grad_norm": 0.30044350028038025,
      "learning_rate": 1.0457052206041984e-07,
      "loss": 0.447,
      "step": 123600
    },
    {
      "epoch": 2.899689277129624,
      "grad_norm": 0.3978623151779175,
      "learning_rate": 1.0217217159228353e-07,
      "loss": 0.4124,
      "step": 123650
    },
    {
      "epoch": 2.900861816263118,
      "grad_norm": 0.2261725664138794,
      "learning_rate": 9.980155051714567e-08,
      "loss": 0.4661,
      "step": 123700
    },
    {
      "epoch": 2.9020343553966113,
      "grad_norm": 0.2537979483604431,
      "learning_rate": 9.745866324745834e-08,
      "loss": 0.4091,
      "step": 123750
    },
    {
      "epoch": 2.903206894530105,
      "grad_norm": 0.47095057368278503,
      "learning_rate": 9.514351414404653e-08,
      "loss": 0.4314,
      "step": 123800
    },
    {
      "epoch": 2.9043794336635984,
      "grad_norm": 0.3426477015018463,
      "learning_rate": 9.285610751610662e-08,
      "loss": 0.4558,
      "step": 123850
    },
    {
      "epoch": 2.905551972797092,
      "grad_norm": 0.33218392729759216,
      "learning_rate": 9.059644762119956e-08,
      "loss": 0.4203,
      "step": 123900
    },
    {
      "epoch": 2.9067245119305856,
      "grad_norm": 0.22476454079151154,
      "learning_rate": 8.836453866524097e-08,
      "loss": 0.4456,
      "step": 123950
    },
    {
      "epoch": 2.9078970510640794,
      "grad_norm": 0.46973273158073425,
      "learning_rate": 8.616038480249111e-08,
      "loss": 0.4452,
      "step": 124000
    },
    {
      "epoch": 2.9078970510640794,
      "eval_loss": 0.760428249835968,
      "eval_runtime": 168.1106,
      "eval_samples_per_second": 59.485,
      "eval_steps_per_second": 14.871,
      "step": 124000
    },
    {
      "epoch": 2.2377187800181293,
      "grad_norm": 0.543333888053894,
      "learning_rate": 5.523665901965613e-06,
      "loss": 0.5837,
      "step": 124050
    },
    {
      "epoch": 2.23862073319774,
      "grad_norm": 0.4352588355541229,
      "learning_rate": 5.5114685417638185e-06,
      "loss": 0.5968,
      "step": 124100
    },
    {
      "epoch": 2.23952268637735,
      "grad_norm": 0.6230080127716064,
      "learning_rate": 5.499281631857645e-06,
      "loss": 0.6333,
      "step": 124150
    },
    {
      "epoch": 2.2404246395569607,
      "grad_norm": 0.25472554564476013,
      "learning_rate": 5.487105185669271e-06,
      "loss": 0.5651,
      "step": 124200
    },
    {
      "epoch": 2.241326592736571,
      "grad_norm": 0.44736823439598083,
      "learning_rate": 5.474939216609369e-06,
      "loss": 0.6318,
      "step": 124250
    },
    {
      "epoch": 2.2422285459161815,
      "grad_norm": 0.5430946946144104,
      "learning_rate": 5.462783738077052e-06,
      "loss": 0.6047,
      "step": 124300
    },
    {
      "epoch": 2.243130499095792,
      "grad_norm": 0.16816657781600952,
      "learning_rate": 5.450638763459879e-06,
      "loss": 0.6441,
      "step": 124350
    },
    {
      "epoch": 2.2440324522754023,
      "grad_norm": 0.5719034671783447,
      "learning_rate": 5.438504306133855e-06,
      "loss": 0.6494,
      "step": 124400
    },
    {
      "epoch": 2.244934405455013,
      "grad_norm": 0.5481187105178833,
      "learning_rate": 5.426380379463406e-06,
      "loss": 0.5928,
      "step": 124450
    },
    {
      "epoch": 2.245836358634623,
      "grad_norm": 0.21296076476573944,
      "learning_rate": 5.414266996801325e-06,
      "loss": 0.6195,
      "step": 124500
    },
    {
      "epoch": 2.2467383118142337,
      "grad_norm": 0.4287676513195038,
      "learning_rate": 5.402164171488828e-06,
      "loss": 0.6481,
      "step": 124550
    },
    {
      "epoch": 2.2476402649938443,
      "grad_norm": 0.405500203371048,
      "learning_rate": 5.390071916855494e-06,
      "loss": 0.6319,
      "step": 124600
    },
    {
      "epoch": 2.2485422181734545,
      "grad_norm": 0.26741671562194824,
      "learning_rate": 5.377990246219254e-06,
      "loss": 0.5241,
      "step": 124650
    },
    {
      "epoch": 2.249444171353065,
      "grad_norm": 0.26110175251960754,
      "learning_rate": 5.3659191728863796e-06,
      "loss": 0.6191,
      "step": 124700
    },
    {
      "epoch": 2.2503461245326757,
      "grad_norm": 0.36694765090942383,
      "learning_rate": 5.353858710151486e-06,
      "loss": 0.6217,
      "step": 124750
    },
    {
      "epoch": 2.251248077712286,
      "grad_norm": 0.6179211139678955,
      "learning_rate": 5.341808871297488e-06,
      "loss": 0.5804,
      "step": 124800
    },
    {
      "epoch": 2.2521500308918965,
      "grad_norm": 0.43830546736717224,
      "learning_rate": 5.3297696695956e-06,
      "loss": 0.5999,
      "step": 124850
    },
    {
      "epoch": 2.253051984071507,
      "grad_norm": 0.5851091742515564,
      "learning_rate": 5.317741118305334e-06,
      "loss": 0.6464,
      "step": 124900
    },
    {
      "epoch": 2.2539539372511173,
      "grad_norm": 0.2991151809692383,
      "learning_rate": 5.305723230674457e-06,
      "loss": 0.5756,
      "step": 124950
    },
    {
      "epoch": 2.254855890430728,
      "grad_norm": 0.44054490327835083,
      "learning_rate": 5.293716019939005e-06,
      "loss": 0.5565,
      "step": 125000
    },
    {
      "epoch": 2.255757843610338,
      "grad_norm": 0.4482937753200531,
      "learning_rate": 5.281719499323243e-06,
      "loss": 0.654,
      "step": 125050
    },
    {
      "epoch": 2.2566597967899487,
      "grad_norm": 0.3368072509765625,
      "learning_rate": 5.269733682039666e-06,
      "loss": 0.6067,
      "step": 125100
    },
    {
      "epoch": 2.2575617499695593,
      "grad_norm": 0.5441448092460632,
      "learning_rate": 5.257758581288987e-06,
      "loss": 0.6272,
      "step": 125150
    },
    {
      "epoch": 2.2584637031491694,
      "grad_norm": 0.3074299395084381,
      "learning_rate": 5.24579421026011e-06,
      "loss": 0.6173,
      "step": 125200
    },
    {
      "epoch": 2.25936565632878,
      "grad_norm": 0.6691204309463501,
      "learning_rate": 5.233840582130117e-06,
      "loss": 0.5888,
      "step": 125250
    },
    {
      "epoch": 2.2602676095083902,
      "grad_norm": 0.7824872136116028,
      "learning_rate": 5.221897710064266e-06,
      "loss": 0.5745,
      "step": 125300
    },
    {
      "epoch": 2.261169562688001,
      "grad_norm": 0.43261030316352844,
      "learning_rate": 5.209965607215984e-06,
      "loss": 0.5693,
      "step": 125350
    },
    {
      "epoch": 2.2620715158676115,
      "grad_norm": 0.7739987969398499,
      "learning_rate": 5.198044286726793e-06,
      "loss": 0.6102,
      "step": 125400
    },
    {
      "epoch": 2.2629734690472216,
      "grad_norm": 0.4251454472541809,
      "learning_rate": 5.18613376172638e-06,
      "loss": 0.5988,
      "step": 125450
    },
    {
      "epoch": 2.2638754222268322,
      "grad_norm": 0.4992808401584625,
      "learning_rate": 5.174234045332534e-06,
      "loss": 0.6342,
      "step": 125500
    },
    {
      "epoch": 2.2647773754064424,
      "grad_norm": 0.8163055777549744,
      "learning_rate": 5.162345150651131e-06,
      "loss": 0.6323,
      "step": 125550
    },
    {
      "epoch": 2.265679328586053,
      "grad_norm": 0.6207514405250549,
      "learning_rate": 5.150467090776126e-06,
      "loss": 0.6231,
      "step": 125600
    },
    {
      "epoch": 2.2665812817656636,
      "grad_norm": 0.6192798614501953,
      "learning_rate": 5.1385998787895524e-06,
      "loss": 0.5858,
      "step": 125650
    },
    {
      "epoch": 2.267483234945274,
      "grad_norm": 0.27999764680862427,
      "learning_rate": 5.1267435277615e-06,
      "loss": 0.588,
      "step": 125700
    },
    {
      "epoch": 2.2683851881248844,
      "grad_norm": 0.2890707552433014,
      "learning_rate": 5.114898050750071e-06,
      "loss": 0.6292,
      "step": 125750
    },
    {
      "epoch": 2.269287141304495,
      "grad_norm": 0.7163087725639343,
      "learning_rate": 5.103063460801415e-06,
      "loss": 0.6137,
      "step": 125800
    },
    {
      "epoch": 2.270189094484105,
      "grad_norm": 0.5870348811149597,
      "learning_rate": 5.0912397709496905e-06,
      "loss": 0.6474,
      "step": 125850
    },
    {
      "epoch": 2.271091047663716,
      "grad_norm": 0.464801549911499,
      "learning_rate": 5.07942699421704e-06,
      "loss": 0.5921,
      "step": 125900
    },
    {
      "epoch": 2.2719930008433264,
      "grad_norm": 0.6136177182197571,
      "learning_rate": 5.067625143613585e-06,
      "loss": 0.5715,
      "step": 125950
    },
    {
      "epoch": 2.2728949540229366,
      "grad_norm": 0.2735672891139984,
      "learning_rate": 5.055834232137433e-06,
      "loss": 0.5345,
      "step": 126000
    },
    {
      "epoch": 2.2728949540229366,
      "eval_loss": 0.763725996017456,
      "eval_runtime": 175.2519,
      "eval_samples_per_second": 57.061,
      "eval_steps_per_second": 14.265,
      "step": 126000
    },
    {
      "epoch": 2.2737969072025472,
      "grad_norm": 0.4399943947792053,
      "learning_rate": 5.044054272774622e-06,
      "loss": 0.6128,
      "step": 126050
    },
    {
      "epoch": 2.2746988603821574,
      "grad_norm": 0.3980908691883087,
      "learning_rate": 5.032285278499134e-06,
      "loss": 0.5794,
      "step": 126100
    },
    {
      "epoch": 2.275600813561768,
      "grad_norm": 0.25556471943855286,
      "learning_rate": 5.020527262272884e-06,
      "loss": 0.5566,
      "step": 126150
    },
    {
      "epoch": 2.2765027667413786,
      "grad_norm": 0.2983306050300598,
      "learning_rate": 5.0087802370456835e-06,
      "loss": 0.5988,
      "step": 126200
    },
    {
      "epoch": 2.277404719920989,
      "grad_norm": 0.9925840497016907,
      "learning_rate": 4.99704421575525e-06,
      "loss": 0.5889,
      "step": 126250
    },
    {
      "epoch": 2.2783066731005994,
      "grad_norm": 0.6178374290466309,
      "learning_rate": 4.985319211327171e-06,
      "loss": 0.589,
      "step": 126300
    },
    {
      "epoch": 2.2792086262802096,
      "grad_norm": 0.41955533623695374,
      "learning_rate": 4.973605236674905e-06,
      "loss": 0.658,
      "step": 126350
    },
    {
      "epoch": 2.28011057945982,
      "grad_norm": 0.4771277904510498,
      "learning_rate": 4.961902304699768e-06,
      "loss": 0.5575,
      "step": 126400
    },
    {
      "epoch": 2.281012532639431,
      "grad_norm": 0.6544424295425415,
      "learning_rate": 4.950210428290908e-06,
      "loss": 0.6323,
      "step": 126450
    },
    {
      "epoch": 2.281914485819041,
      "grad_norm": 0.7127678990364075,
      "learning_rate": 4.938529620325292e-06,
      "loss": 0.5346,
      "step": 126500
    },
    {
      "epoch": 2.2828164389986516,
      "grad_norm": 0.3656955659389496,
      "learning_rate": 4.926859893667708e-06,
      "loss": 0.5596,
      "step": 126550
    },
    {
      "epoch": 2.283718392178262,
      "grad_norm": 0.49246275424957275,
      "learning_rate": 4.9152012611707375e-06,
      "loss": 0.6019,
      "step": 126600
    },
    {
      "epoch": 2.2846203453578724,
      "grad_norm": 0.6084648370742798,
      "learning_rate": 4.9035537356747375e-06,
      "loss": 0.6183,
      "step": 126650
    },
    {
      "epoch": 2.285522298537483,
      "grad_norm": 0.38649797439575195,
      "learning_rate": 4.891917330007829e-06,
      "loss": 0.601,
      "step": 126700
    },
    {
      "epoch": 2.286424251717093,
      "grad_norm": 0.2840111255645752,
      "learning_rate": 4.880292056985901e-06,
      "loss": 0.5665,
      "step": 126750
    },
    {
      "epoch": 2.287326204896704,
      "grad_norm": 0.19556157290935516,
      "learning_rate": 4.868677929412569e-06,
      "loss": 0.6162,
      "step": 126800
    },
    {
      "epoch": 2.2882281580763144,
      "grad_norm": 0.6355746388435364,
      "learning_rate": 4.85707496007917e-06,
      "loss": 0.6174,
      "step": 126850
    },
    {
      "epoch": 2.2891301112559246,
      "grad_norm": 0.2229311168193817,
      "learning_rate": 4.84548316176477e-06,
      "loss": 0.5593,
      "step": 126900
    },
    {
      "epoch": 2.290032064435535,
      "grad_norm": 0.4400377869606018,
      "learning_rate": 4.833902547236108e-06,
      "loss": 0.5947,
      "step": 126950
    },
    {
      "epoch": 2.290934017615146,
      "grad_norm": 0.5635218620300293,
      "learning_rate": 4.822333129247631e-06,
      "loss": 0.6209,
      "step": 127000
    },
    {
      "epoch": 2.291835970794756,
      "grad_norm": 0.3684791922569275,
      "learning_rate": 4.810774920541432e-06,
      "loss": 0.6162,
      "step": 127050
    },
    {
      "epoch": 2.2927379239743666,
      "grad_norm": 0.5977340936660767,
      "learning_rate": 4.799227933847264e-06,
      "loss": 0.5972,
      "step": 127100
    },
    {
      "epoch": 2.2936398771539768,
      "grad_norm": 0.23261262476444244,
      "learning_rate": 4.787692181882538e-06,
      "loss": 0.5765,
      "step": 127150
    },
    {
      "epoch": 2.2945418303335874,
      "grad_norm": 0.44093647599220276,
      "learning_rate": 4.776167677352268e-06,
      "loss": 0.6129,
      "step": 127200
    },
    {
      "epoch": 2.295443783513198,
      "grad_norm": 0.598921537399292,
      "learning_rate": 4.764654432949089e-06,
      "loss": 0.5761,
      "step": 127250
    },
    {
      "epoch": 2.296345736692808,
      "grad_norm": 0.5434467792510986,
      "learning_rate": 4.753152461353238e-06,
      "loss": 0.5961,
      "step": 127300
    },
    {
      "epoch": 2.297247689872419,
      "grad_norm": 0.20731467008590698,
      "learning_rate": 4.741661775232541e-06,
      "loss": 0.555,
      "step": 127350
    },
    {
      "epoch": 2.298149643052029,
      "grad_norm": 0.6958674788475037,
      "learning_rate": 4.7301823872423825e-06,
      "loss": 0.6024,
      "step": 127400
    },
    {
      "epoch": 2.2990515962316396,
      "grad_norm": 0.5433995723724365,
      "learning_rate": 4.718714310025706e-06,
      "loss": 0.5438,
      "step": 127450
    },
    {
      "epoch": 2.29995354941125,
      "grad_norm": 0.3292297124862671,
      "learning_rate": 4.707257556213008e-06,
      "loss": 0.5886,
      "step": 127500
    },
    {
      "epoch": 2.3008555025908604,
      "grad_norm": 0.518819272518158,
      "learning_rate": 4.695812138422305e-06,
      "loss": 0.6176,
      "step": 127550
    },
    {
      "epoch": 2.301757455770471,
      "grad_norm": 0.6572323441505432,
      "learning_rate": 4.684378069259123e-06,
      "loss": 0.5795,
      "step": 127600
    },
    {
      "epoch": 2.3026594089500816,
      "grad_norm": 0.45056769251823425,
      "learning_rate": 4.6729553613165025e-06,
      "loss": 0.6004,
      "step": 127650
    },
    {
      "epoch": 2.3035613621296918,
      "grad_norm": 0.5295699238777161,
      "learning_rate": 4.661544027174974e-06,
      "loss": 0.5745,
      "step": 127700
    },
    {
      "epoch": 2.3044633153093024,
      "grad_norm": 0.5998559594154358,
      "learning_rate": 4.650144079402513e-06,
      "loss": 0.6277,
      "step": 127750
    },
    {
      "epoch": 2.305365268488913,
      "grad_norm": 1.0189812183380127,
      "learning_rate": 4.6387555305545844e-06,
      "loss": 0.6487,
      "step": 127800
    },
    {
      "epoch": 2.306267221668523,
      "grad_norm": 0.6057592630386353,
      "learning_rate": 4.627378393174093e-06,
      "loss": 0.5845,
      "step": 127850
    },
    {
      "epoch": 2.3071691748481338,
      "grad_norm": 0.2509079873561859,
      "learning_rate": 4.616012679791366e-06,
      "loss": 0.6113,
      "step": 127900
    },
    {
      "epoch": 2.308071128027744,
      "grad_norm": 0.3008737862110138,
      "learning_rate": 4.604658402924148e-06,
      "loss": 0.5794,
      "step": 127950
    },
    {
      "epoch": 2.3089730812073546,
      "grad_norm": 0.27673622965812683,
      "learning_rate": 4.5933155750776035e-06,
      "loss": 0.648,
      "step": 128000
    },
    {
      "epoch": 2.3089730812073546,
      "eval_loss": 0.7660305500030518,
      "eval_runtime": 190.8908,
      "eval_samples_per_second": 52.386,
      "eval_steps_per_second": 13.096,
      "step": 128000
    },
    {
      "epoch": 2.309875034386965,
      "grad_norm": 0.5736947059631348,
      "learning_rate": 4.581984208744272e-06,
      "loss": 0.6474,
      "step": 128050
    },
    {
      "epoch": 2.3107769875665753,
      "grad_norm": 0.21198159456253052,
      "learning_rate": 4.570664316404071e-06,
      "loss": 0.5732,
      "step": 128100
    },
    {
      "epoch": 2.311678940746186,
      "grad_norm": 0.5609583258628845,
      "learning_rate": 4.559355910524293e-06,
      "loss": 0.5951,
      "step": 128150
    },
    {
      "epoch": 2.312580893925796,
      "grad_norm": 0.510516345500946,
      "learning_rate": 4.548059003559564e-06,
      "loss": 0.6317,
      "step": 128200
    },
    {
      "epoch": 2.3134828471054067,
      "grad_norm": 0.3020458221435547,
      "learning_rate": 4.536773607951862e-06,
      "loss": 0.5558,
      "step": 128250
    },
    {
      "epoch": 2.3143848002850174,
      "grad_norm": 0.7422146797180176,
      "learning_rate": 4.525499736130474e-06,
      "loss": 0.5663,
      "step": 128300
    },
    {
      "epoch": 2.3152867534646275,
      "grad_norm": 0.42110496759414673,
      "learning_rate": 4.514237400511995e-06,
      "loss": 0.5832,
      "step": 128350
    },
    {
      "epoch": 2.316188706644238,
      "grad_norm": 0.4816484749317169,
      "learning_rate": 4.502986613500325e-06,
      "loss": 0.6356,
      "step": 128400
    },
    {
      "epoch": 2.3170906598238483,
      "grad_norm": 0.5702207088470459,
      "learning_rate": 4.491747387486638e-06,
      "loss": 0.5569,
      "step": 128450
    },
    {
      "epoch": 2.317992613003459,
      "grad_norm": 0.4656164050102234,
      "learning_rate": 4.480519734849369e-06,
      "loss": 0.5971,
      "step": 128500
    },
    {
      "epoch": 2.3188945661830695,
      "grad_norm": 0.5278236865997314,
      "learning_rate": 4.469303667954219e-06,
      "loss": 0.5493,
      "step": 128550
    },
    {
      "epoch": 2.3197965193626797,
      "grad_norm": 0.45073485374450684,
      "learning_rate": 4.458099199154131e-06,
      "loss": 0.558,
      "step": 128600
    },
    {
      "epoch": 2.3206984725422903,
      "grad_norm": 0.49451079964637756,
      "learning_rate": 4.4469063407892495e-06,
      "loss": 0.5981,
      "step": 128650
    },
    {
      "epoch": 2.321600425721901,
      "grad_norm": 0.39066290855407715,
      "learning_rate": 4.435725105186957e-06,
      "loss": 0.6019,
      "step": 128700
    },
    {
      "epoch": 2.322502378901511,
      "grad_norm": 0.5705121159553528,
      "learning_rate": 4.424555504661832e-06,
      "loss": 0.5979,
      "step": 128750
    },
    {
      "epoch": 2.3234043320811217,
      "grad_norm": 0.49987152218818665,
      "learning_rate": 4.413397551515631e-06,
      "loss": 0.5378,
      "step": 128800
    },
    {
      "epoch": 2.3243062852607324,
      "grad_norm": 0.8068012595176697,
      "learning_rate": 4.402251258037277e-06,
      "loss": 0.6139,
      "step": 128850
    },
    {
      "epoch": 2.3252082384403425,
      "grad_norm": 0.37897539138793945,
      "learning_rate": 4.391116636502871e-06,
      "loss": 0.5556,
      "step": 128900
    },
    {
      "epoch": 2.326110191619953,
      "grad_norm": 0.4724530279636383,
      "learning_rate": 4.379993699175642e-06,
      "loss": 0.5778,
      "step": 128950
    },
    {
      "epoch": 2.3270121447995633,
      "grad_norm": 0.4370051622390747,
      "learning_rate": 4.36888245830595e-06,
      "loss": 0.5786,
      "step": 129000
    },
    {
      "epoch": 2.327914097979174,
      "grad_norm": 0.41837918758392334,
      "learning_rate": 4.357782926131288e-06,
      "loss": 0.6039,
      "step": 129050
    },
    {
      "epoch": 2.3288160511587845,
      "grad_norm": 0.5044168829917908,
      "learning_rate": 4.346695114876244e-06,
      "loss": 0.6401,
      "step": 129100
    },
    {
      "epoch": 2.3297180043383947,
      "grad_norm": 0.6297003030776978,
      "learning_rate": 4.335619036752495e-06,
      "loss": 0.5992,
      "step": 129150
    },
    {
      "epoch": 2.3306199575180053,
      "grad_norm": 0.4150676131248474,
      "learning_rate": 4.324554703958796e-06,
      "loss": 0.5715,
      "step": 129200
    },
    {
      "epoch": 2.3315219106976155,
      "grad_norm": 0.4820016622543335,
      "learning_rate": 4.3135021286809725e-06,
      "loss": 0.5894,
      "step": 129250
    },
    {
      "epoch": 2.332423863877226,
      "grad_norm": 0.4259182810783386,
      "learning_rate": 4.302461323091898e-06,
      "loss": 0.6094,
      "step": 129300
    },
    {
      "epoch": 2.3333258170568367,
      "grad_norm": 0.4547434151172638,
      "learning_rate": 4.291432299351472e-06,
      "loss": 0.5769,
      "step": 129350
    },
    {
      "epoch": 2.334227770236447,
      "grad_norm": 0.5563770532608032,
      "learning_rate": 4.280415069606638e-06,
      "loss": 0.6159,
      "step": 129400
    },
    {
      "epoch": 2.3351297234160575,
      "grad_norm": 0.4399298131465912,
      "learning_rate": 4.269409645991333e-06,
      "loss": 0.5877,
      "step": 129450
    },
    {
      "epoch": 2.3360316765956677,
      "grad_norm": 0.4000059962272644,
      "learning_rate": 4.258416040626504e-06,
      "loss": 0.5625,
      "step": 129500
    },
    {
      "epoch": 2.3369336297752783,
      "grad_norm": 0.46029725670814514,
      "learning_rate": 4.247434265620074e-06,
      "loss": 0.5768,
      "step": 129550
    },
    {
      "epoch": 2.337835582954889,
      "grad_norm": 0.6207846403121948,
      "learning_rate": 4.236464333066933e-06,
      "loss": 0.6434,
      "step": 129600
    },
    {
      "epoch": 2.338737536134499,
      "grad_norm": 0.34787070751190186,
      "learning_rate": 4.2255062550489415e-06,
      "loss": 0.5966,
      "step": 129650
    },
    {
      "epoch": 2.3396394893141097,
      "grad_norm": 0.3120103180408478,
      "learning_rate": 4.214560043634893e-06,
      "loss": 0.5912,
      "step": 129700
    },
    {
      "epoch": 2.3405414424937203,
      "grad_norm": 0.33441752195358276,
      "learning_rate": 4.20362571088051e-06,
      "loss": 0.5969,
      "step": 129750
    },
    {
      "epoch": 2.3414433956733305,
      "grad_norm": 0.7376463413238525,
      "learning_rate": 4.192703268828439e-06,
      "loss": 0.6108,
      "step": 129800
    },
    {
      "epoch": 2.342345348852941,
      "grad_norm": 0.5847952365875244,
      "learning_rate": 4.181792729508241e-06,
      "loss": 0.6113,
      "step": 129850
    },
    {
      "epoch": 2.3432473020325517,
      "grad_norm": 0.6158472299575806,
      "learning_rate": 4.170894104936334e-06,
      "loss": 0.6363,
      "step": 129900
    },
    {
      "epoch": 2.344149255212162,
      "grad_norm": 0.55879807472229,
      "learning_rate": 4.1600074071160455e-06,
      "loss": 0.5871,
      "step": 129950
    },
    {
      "epoch": 2.3450512083917725,
      "grad_norm": 0.6560248732566833,
      "learning_rate": 4.149132648037562e-06,
      "loss": 0.5915,
      "step": 130000
    },
    {
      "epoch": 2.3450512083917725,
      "eval_loss": 0.7643374800682068,
      "eval_runtime": 173.4382,
      "eval_samples_per_second": 57.657,
      "eval_steps_per_second": 14.414,
      "step": 130000
    },
    {
      "epoch": 2.3459531615713827,
      "grad_norm": 0.6451669335365295,
      "learning_rate": 4.138269839677908e-06,
      "loss": 0.6078,
      "step": 130050
    },
    {
      "epoch": 2.3468551147509933,
      "grad_norm": 0.4528825581073761,
      "learning_rate": 4.127418994000952e-06,
      "loss": 0.5536,
      "step": 130100
    },
    {
      "epoch": 2.347757067930604,
      "grad_norm": 0.4374530017375946,
      "learning_rate": 4.1165801229573965e-06,
      "loss": 0.6184,
      "step": 130150
    },
    {
      "epoch": 2.348659021110214,
      "grad_norm": 0.30622920393943787,
      "learning_rate": 4.105753238484745e-06,
      "loss": 0.515,
      "step": 130200
    },
    {
      "epoch": 2.3495609742898247,
      "grad_norm": 0.42927074432373047,
      "learning_rate": 4.094938352507296e-06,
      "loss": 0.587,
      "step": 130250
    },
    {
      "epoch": 2.350462927469435,
      "grad_norm": 0.5009927153587341,
      "learning_rate": 4.084135476936151e-06,
      "loss": 0.6075,
      "step": 130300
    },
    {
      "epoch": 2.3513648806490455,
      "grad_norm": 1.0750420093536377,
      "learning_rate": 4.073344623669163e-06,
      "loss": 0.5689,
      "step": 130350
    },
    {
      "epoch": 2.352266833828656,
      "grad_norm": 0.6111161112785339,
      "learning_rate": 4.062565804590962e-06,
      "loss": 0.5918,
      "step": 130400
    },
    {
      "epoch": 2.3531687870082663,
      "grad_norm": 0.6635628938674927,
      "learning_rate": 4.0517990315729105e-06,
      "loss": 0.5322,
      "step": 130450
    },
    {
      "epoch": 2.354070740187877,
      "grad_norm": 0.25223061442375183,
      "learning_rate": 4.041044316473106e-06,
      "loss": 0.5633,
      "step": 130500
    },
    {
      "epoch": 2.354972693367487,
      "grad_norm": 0.604037344455719,
      "learning_rate": 4.030301671136379e-06,
      "loss": 0.6186,
      "step": 130550
    },
    {
      "epoch": 2.3558746465470977,
      "grad_norm": 0.39878585934638977,
      "learning_rate": 4.01957110739425e-06,
      "loss": 0.5797,
      "step": 130600
    },
    {
      "epoch": 2.3567765997267083,
      "grad_norm": 0.4076906144618988,
      "learning_rate": 4.008852637064935e-06,
      "loss": 0.6151,
      "step": 130650
    },
    {
      "epoch": 2.3576785529063184,
      "grad_norm": 0.29696765542030334,
      "learning_rate": 3.998146271953344e-06,
      "loss": 0.5967,
      "step": 130700
    },
    {
      "epoch": 2.358580506085929,
      "grad_norm": 0.5810902118682861,
      "learning_rate": 3.9874520238510535e-06,
      "loss": 0.5583,
      "step": 130750
    },
    {
      "epoch": 2.3594824592655397,
      "grad_norm": 0.6972004175186157,
      "learning_rate": 3.976769904536271e-06,
      "loss": 0.5997,
      "step": 130800
    },
    {
      "epoch": 2.36038441244515,
      "grad_norm": 0.4287399649620056,
      "learning_rate": 3.96609992577387e-06,
      "loss": 0.5628,
      "step": 130850
    },
    {
      "epoch": 2.3612863656247605,
      "grad_norm": 0.5105943083763123,
      "learning_rate": 3.955442099315353e-06,
      "loss": 0.5849,
      "step": 130900
    },
    {
      "epoch": 2.362188318804371,
      "grad_norm": 0.361066073179245,
      "learning_rate": 3.944796436898823e-06,
      "loss": 0.5931,
      "step": 130950
    },
    {
      "epoch": 2.3630902719839812,
      "grad_norm": 0.34371674060821533,
      "learning_rate": 3.934162950248994e-06,
      "loss": 0.5592,
      "step": 131000
    },
    {
      "epoch": 2.363992225163592,
      "grad_norm": 0.5034013390541077,
      "learning_rate": 3.923541651077172e-06,
      "loss": 0.6291,
      "step": 131050
    },
    {
      "epoch": 2.364894178343202,
      "grad_norm": 0.3341981768608093,
      "learning_rate": 3.912932551081247e-06,
      "loss": 0.5907,
      "step": 131100
    },
    {
      "epoch": 2.3657961315228127,
      "grad_norm": 0.497415155172348,
      "learning_rate": 3.902335661945648e-06,
      "loss": 0.6061,
      "step": 131150
    },
    {
      "epoch": 2.3666980847024233,
      "grad_norm": 0.3201215863227844,
      "learning_rate": 3.891750995341382e-06,
      "loss": 0.5608,
      "step": 131200
    },
    {
      "epoch": 2.3676000378820334,
      "grad_norm": 0.622157633304596,
      "learning_rate": 3.881178562925989e-06,
      "loss": 0.5567,
      "step": 131250
    },
    {
      "epoch": 2.368501991061644,
      "grad_norm": 0.4536997973918915,
      "learning_rate": 3.870618376343524e-06,
      "loss": 0.616,
      "step": 131300
    },
    {
      "epoch": 2.3694039442412542,
      "grad_norm": 0.2315579056739807,
      "learning_rate": 3.86007044722456e-06,
      "loss": 0.5741,
      "step": 131350
    },
    {
      "epoch": 2.370305897420865,
      "grad_norm": 0.45692765712738037,
      "learning_rate": 3.849534787186177e-06,
      "loss": 0.6031,
      "step": 131400
    },
    {
      "epoch": 2.3712078506004755,
      "grad_norm": 0.2710069715976715,
      "learning_rate": 3.839011407831936e-06,
      "loss": 0.5656,
      "step": 131450
    },
    {
      "epoch": 2.3721098037800856,
      "grad_norm": 0.3830969035625458,
      "learning_rate": 3.828500320751866e-06,
      "loss": 0.5547,
      "step": 131500
    },
    {
      "epoch": 2.3730117569596962,
      "grad_norm": 0.4193568527698517,
      "learning_rate": 3.818001537522475e-06,
      "loss": 0.6395,
      "step": 131550
    },
    {
      "epoch": 2.373913710139307,
      "grad_norm": 0.23558226227760315,
      "learning_rate": 3.8075150697067028e-06,
      "loss": 0.5606,
      "step": 131600
    },
    {
      "epoch": 2.374815663318917,
      "grad_norm": 0.5597228407859802,
      "learning_rate": 3.797040928853941e-06,
      "loss": 0.5494,
      "step": 131650
    },
    {
      "epoch": 2.3757176164985276,
      "grad_norm": 0.43132463097572327,
      "learning_rate": 3.7865791264999914e-06,
      "loss": 0.5677,
      "step": 131700
    },
    {
      "epoch": 2.3766195696781383,
      "grad_norm": 0.37271416187286377,
      "learning_rate": 3.776129674167068e-06,
      "loss": 0.5523,
      "step": 131750
    },
    {
      "epoch": 2.3775215228577484,
      "grad_norm": 0.5117822885513306,
      "learning_rate": 3.765692583363796e-06,
      "loss": 0.5321,
      "step": 131800
    },
    {
      "epoch": 2.378423476037359,
      "grad_norm": 0.2511914372444153,
      "learning_rate": 3.7552678655851724e-06,
      "loss": 0.6477,
      "step": 131850
    },
    {
      "epoch": 2.379325429216969,
      "grad_norm": 0.4335424602031708,
      "learning_rate": 3.744855532312566e-06,
      "loss": 0.575,
      "step": 131900
    },
    {
      "epoch": 2.38022738239658,
      "grad_norm": 0.43048256635665894,
      "learning_rate": 3.734455595013719e-06,
      "loss": 0.5965,
      "step": 131950
    },
    {
      "epoch": 2.3811293355761904,
      "grad_norm": 0.3254722058773041,
      "learning_rate": 3.724068065142715e-06,
      "loss": 0.5548,
      "step": 132000
    },
    {
      "epoch": 2.3811293355761904,
      "eval_loss": 0.7645427584648132,
      "eval_runtime": 182.0022,
      "eval_samples_per_second": 54.944,
      "eval_steps_per_second": 13.736,
      "step": 132000
    },
    {
      "epoch": 2.3820312887558006,
      "grad_norm": 0.5539864301681519,
      "learning_rate": 3.713692954139967e-06,
      "loss": 0.5883,
      "step": 132050
    },
    {
      "epoch": 2.3829332419354112,
      "grad_norm": 0.6863040924072266,
      "learning_rate": 3.7033302734322115e-06,
      "loss": 0.6153,
      "step": 132100
    },
    {
      "epoch": 2.3838351951150214,
      "grad_norm": 0.3189157545566559,
      "learning_rate": 3.6929800344325054e-06,
      "loss": 0.6445,
      "step": 132150
    },
    {
      "epoch": 2.384737148294632,
      "grad_norm": 0.4489203095436096,
      "learning_rate": 3.6826422485401928e-06,
      "loss": 0.5676,
      "step": 132200
    },
    {
      "epoch": 2.3856391014742426,
      "grad_norm": 0.4557484984397888,
      "learning_rate": 3.672316927140898e-06,
      "loss": 0.6144,
      "step": 132250
    },
    {
      "epoch": 2.386541054653853,
      "grad_norm": 0.45074647665023804,
      "learning_rate": 3.6620040816065325e-06,
      "loss": 0.5549,
      "step": 132300
    },
    {
      "epoch": 2.3874430078334634,
      "grad_norm": 0.38747429847717285,
      "learning_rate": 3.6517037232952543e-06,
      "loss": 0.6421,
      "step": 132350
    },
    {
      "epoch": 2.3883449610130736,
      "grad_norm": 0.45286285877227783,
      "learning_rate": 3.6414158635514774e-06,
      "loss": 0.5759,
      "step": 132400
    },
    {
      "epoch": 2.389246914192684,
      "grad_norm": 0.3702876567840576,
      "learning_rate": 3.631140513705844e-06,
      "loss": 0.6052,
      "step": 132450
    },
    {
      "epoch": 2.390148867372295,
      "grad_norm": 0.30940261483192444,
      "learning_rate": 3.620877685075216e-06,
      "loss": 0.5676,
      "step": 132500
    },
    {
      "epoch": 2.391050820551905,
      "grad_norm": 0.4094374179840088,
      "learning_rate": 3.610627388962677e-06,
      "loss": 0.5553,
      "step": 132550
    },
    {
      "epoch": 2.3919527737315156,
      "grad_norm": 0.487858384847641,
      "learning_rate": 3.6003896366574936e-06,
      "loss": 0.5497,
      "step": 132600
    },
    {
      "epoch": 2.392854726911126,
      "grad_norm": 0.6343607306480408,
      "learning_rate": 3.5901644394351312e-06,
      "loss": 0.6096,
      "step": 132650
    },
    {
      "epoch": 2.3937566800907364,
      "grad_norm": 0.5741158127784729,
      "learning_rate": 3.5799518085572127e-06,
      "loss": 0.5741,
      "step": 132700
    },
    {
      "epoch": 2.394658633270347,
      "grad_norm": 0.3403545916080475,
      "learning_rate": 3.5697517552715347e-06,
      "loss": 0.5358,
      "step": 132750
    },
    {
      "epoch": 2.3955605864499576,
      "grad_norm": 0.4857322871685028,
      "learning_rate": 3.5595642908120336e-06,
      "loss": 0.6394,
      "step": 132800
    },
    {
      "epoch": 2.396462539629568,
      "grad_norm": 0.4055195748806,
      "learning_rate": 3.5493894263987753e-06,
      "loss": 0.5962,
      "step": 132850
    },
    {
      "epoch": 2.3973644928091784,
      "grad_norm": 0.7474311590194702,
      "learning_rate": 3.539227173237966e-06,
      "loss": 0.557,
      "step": 132900
    },
    {
      "epoch": 2.3982664459887886,
      "grad_norm": 0.26138946413993835,
      "learning_rate": 3.5290775425219064e-06,
      "loss": 0.6084,
      "step": 132950
    },
    {
      "epoch": 2.399168399168399,
      "grad_norm": 0.8174405097961426,
      "learning_rate": 3.5189405454289975e-06,
      "loss": 0.6703,
      "step": 133000
    },
    {
      "epoch": 2.40007035234801,
      "grad_norm": 0.2639338970184326,
      "learning_rate": 3.508816193123734e-06,
      "loss": 0.5853,
      "step": 133050
    },
    {
      "epoch": 2.40097230552762,
      "grad_norm": 0.3126554489135742,
      "learning_rate": 3.498704496756686e-06,
      "loss": 0.5513,
      "step": 133100
    },
    {
      "epoch": 2.4018742587072306,
      "grad_norm": 0.4207853674888611,
      "learning_rate": 3.488605467464466e-06,
      "loss": 0.5801,
      "step": 133150
    },
    {
      "epoch": 2.4027762118868408,
      "grad_norm": 0.3174491822719574,
      "learning_rate": 3.4785191163697503e-06,
      "loss": 0.5627,
      "step": 133200
    },
    {
      "epoch": 2.4036781650664514,
      "grad_norm": 0.48728376626968384,
      "learning_rate": 3.4684454545812595e-06,
      "loss": 0.628,
      "step": 133250
    },
    {
      "epoch": 2.404580118246062,
      "grad_norm": 0.5136730670928955,
      "learning_rate": 3.458384493193721e-06,
      "loss": 0.581,
      "step": 133300
    },
    {
      "epoch": 2.405482071425672,
      "grad_norm": 0.5098397731781006,
      "learning_rate": 3.4483362432878793e-06,
      "loss": 0.6305,
      "step": 133350
    },
    {
      "epoch": 2.406384024605283,
      "grad_norm": 0.3078869581222534,
      "learning_rate": 3.4383007159304898e-06,
      "loss": 0.647,
      "step": 133400
    },
    {
      "epoch": 2.407285977784893,
      "grad_norm": 0.32239827513694763,
      "learning_rate": 3.428277922174285e-06,
      "loss": 0.6106,
      "step": 133450
    },
    {
      "epoch": 2.4081879309645036,
      "grad_norm": 0.6260040998458862,
      "learning_rate": 3.4182678730579686e-06,
      "loss": 0.5926,
      "step": 133500
    },
    {
      "epoch": 2.409089884144114,
      "grad_norm": 0.613584041595459,
      "learning_rate": 3.4082705796062284e-06,
      "loss": 0.6233,
      "step": 133550
    },
    {
      "epoch": 2.4099918373237244,
      "grad_norm": 0.32996541261672974,
      "learning_rate": 3.398286052829679e-06,
      "loss": 0.613,
      "step": 133600
    },
    {
      "epoch": 2.410893790503335,
      "grad_norm": 0.4358339309692383,
      "learning_rate": 3.388314303724893e-06,
      "loss": 0.5878,
      "step": 133650
    },
    {
      "epoch": 2.4117957436829456,
      "grad_norm": 0.5273149609565735,
      "learning_rate": 3.378355343274363e-06,
      "loss": 0.6034,
      "step": 133700
    },
    {
      "epoch": 2.4126976968625558,
      "grad_norm": 0.39934706687927246,
      "learning_rate": 3.368409182446488e-06,
      "loss": 0.6096,
      "step": 133750
    },
    {
      "epoch": 2.4135996500421664,
      "grad_norm": 0.3166221082210541,
      "learning_rate": 3.3584758321955906e-06,
      "loss": 0.6304,
      "step": 133800
    },
    {
      "epoch": 2.414501603221777,
      "grad_norm": 0.558922290802002,
      "learning_rate": 3.3485553034618644e-06,
      "loss": 0.5749,
      "step": 133850
    },
    {
      "epoch": 2.415403556401387,
      "grad_norm": 0.37317997217178345,
      "learning_rate": 3.3386476071713896e-06,
      "loss": 0.5728,
      "step": 133900
    },
    {
      "epoch": 2.4163055095809978,
      "grad_norm": 0.5885290503501892,
      "learning_rate": 3.3287527542361157e-06,
      "loss": 0.5652,
      "step": 133950
    },
    {
      "epoch": 2.417207462760608,
      "grad_norm": 1.0487887859344482,
      "learning_rate": 3.3188707555538524e-06,
      "loss": 0.6268,
      "step": 134000
    },
    {
      "epoch": 2.417207462760608,
      "eval_loss": 0.7641125321388245,
      "eval_runtime": 182.6811,
      "eval_samples_per_second": 54.74,
      "eval_steps_per_second": 13.685,
      "step": 134000
    },
    {
      "epoch": 2.4181094159402186,
      "grad_norm": 0.5497940182685852,
      "learning_rate": 3.30900162200823e-06,
      "loss": 0.6078,
      "step": 134050
    },
    {
      "epoch": 2.419011369119829,
      "grad_norm": 0.587719738483429,
      "learning_rate": 3.2991453644687304e-06,
      "loss": 0.6043,
      "step": 134100
    },
    {
      "epoch": 2.4199133222994393,
      "grad_norm": 0.5214515924453735,
      "learning_rate": 3.289301993790654e-06,
      "loss": 0.6354,
      "step": 134150
    },
    {
      "epoch": 2.42081527547905,
      "grad_norm": 0.354110985994339,
      "learning_rate": 3.2794715208150982e-06,
      "loss": 0.6292,
      "step": 134200
    },
    {
      "epoch": 2.42171722865866,
      "grad_norm": 0.4166155755519867,
      "learning_rate": 3.269653956368955e-06,
      "loss": 0.5733,
      "step": 134250
    },
    {
      "epoch": 2.4226191818382707,
      "grad_norm": 0.41200515627861023,
      "learning_rate": 3.2598493112649074e-06,
      "loss": 0.5647,
      "step": 134300
    },
    {
      "epoch": 2.4235211350178814,
      "grad_norm": 0.24112947285175323,
      "learning_rate": 3.2500575963014166e-06,
      "loss": 0.6081,
      "step": 134350
    },
    {
      "epoch": 2.4244230881974915,
      "grad_norm": 0.5094958543777466,
      "learning_rate": 3.2402788222626744e-06,
      "loss": 0.5601,
      "step": 134400
    },
    {
      "epoch": 2.425325041377102,
      "grad_norm": 0.5992413759231567,
      "learning_rate": 3.2305129999186454e-06,
      "loss": 0.6374,
      "step": 134450
    },
    {
      "epoch": 2.4262269945567123,
      "grad_norm": 0.48674988746643066,
      "learning_rate": 3.22076014002503e-06,
      "loss": 0.5598,
      "step": 134500
    },
    {
      "epoch": 2.427128947736323,
      "grad_norm": 0.7026817202568054,
      "learning_rate": 3.211020253323236e-06,
      "loss": 0.5398,
      "step": 134550
    },
    {
      "epoch": 2.4280309009159335,
      "grad_norm": 0.45567774772644043,
      "learning_rate": 3.201293350540393e-06,
      "loss": 0.5635,
      "step": 134600
    },
    {
      "epoch": 2.4289328540955437,
      "grad_norm": 0.5835490226745605,
      "learning_rate": 3.1915794423893334e-06,
      "loss": 0.528,
      "step": 134650
    },
    {
      "epoch": 2.4298348072751543,
      "grad_norm": 0.46291694045066833,
      "learning_rate": 3.181878539568572e-06,
      "loss": 0.5414,
      "step": 134700
    },
    {
      "epoch": 2.430736760454765,
      "grad_norm": 0.5089700818061829,
      "learning_rate": 3.1721906527622977e-06,
      "loss": 0.6034,
      "step": 134750
    },
    {
      "epoch": 2.431638713634375,
      "grad_norm": 0.21915189921855927,
      "learning_rate": 3.1625157926403754e-06,
      "loss": 0.5954,
      "step": 134800
    },
    {
      "epoch": 2.4325406668139857,
      "grad_norm": 0.31189748644828796,
      "learning_rate": 3.1528539698583088e-06,
      "loss": 0.6097,
      "step": 134850
    },
    {
      "epoch": 2.4334426199935963,
      "grad_norm": 0.4758426547050476,
      "learning_rate": 3.143205195057256e-06,
      "loss": 0.5459,
      "step": 134900
    },
    {
      "epoch": 2.4343445731732065,
      "grad_norm": 0.6077107787132263,
      "learning_rate": 3.133569478863995e-06,
      "loss": 0.5535,
      "step": 134950
    },
    {
      "epoch": 2.435246526352817,
      "grad_norm": 0.4180014133453369,
      "learning_rate": 3.123946831890922e-06,
      "loss": 0.5881,
      "step": 135000
    },
    {
      "epoch": 2.4361484795324273,
      "grad_norm": 0.2923101484775543,
      "learning_rate": 3.114337264736051e-06,
      "loss": 0.6659,
      "step": 135050
    },
    {
      "epoch": 2.437050432712038,
      "grad_norm": 0.5599722862243652,
      "learning_rate": 3.1047407879829753e-06,
      "loss": 0.5985,
      "step": 135100
    },
    {
      "epoch": 2.4379523858916485,
      "grad_norm": 0.5102499127388,
      "learning_rate": 3.095157412200873e-06,
      "loss": 0.6442,
      "step": 135150
    },
    {
      "epoch": 2.4388543390712587,
      "grad_norm": 0.2920812666416168,
      "learning_rate": 3.0855871479445042e-06,
      "loss": 0.6264,
      "step": 135200
    },
    {
      "epoch": 2.4397562922508693,
      "grad_norm": 0.3499562442302704,
      "learning_rate": 3.0760300057541873e-06,
      "loss": 0.5322,
      "step": 135250
    },
    {
      "epoch": 2.4406582454304795,
      "grad_norm": 0.37184789776802063,
      "learning_rate": 3.066485996155768e-06,
      "loss": 0.5875,
      "step": 135300
    },
    {
      "epoch": 2.44156019861009,
      "grad_norm": 0.5304048657417297,
      "learning_rate": 3.0569551296606525e-06,
      "loss": 0.6152,
      "step": 135350
    },
    {
      "epoch": 2.4424621517897007,
      "grad_norm": 0.6337208151817322,
      "learning_rate": 3.0474374167657644e-06,
      "loss": 0.6226,
      "step": 135400
    },
    {
      "epoch": 2.443364104969311,
      "grad_norm": 0.694110631942749,
      "learning_rate": 3.037932867953537e-06,
      "loss": 0.5641,
      "step": 135450
    },
    {
      "epoch": 2.4442660581489215,
      "grad_norm": 0.22033414244651794,
      "learning_rate": 3.0284414936919e-06,
      "loss": 0.5853,
      "step": 135500
    },
    {
      "epoch": 2.445168011328532,
      "grad_norm": 0.36909613013267517,
      "learning_rate": 3.0189633044342896e-06,
      "loss": 0.6021,
      "step": 135550
    },
    {
      "epoch": 2.4460699645081423,
      "grad_norm": 0.6461573839187622,
      "learning_rate": 3.0094983106196077e-06,
      "loss": 0.5965,
      "step": 135600
    },
    {
      "epoch": 2.446971917687753,
      "grad_norm": 0.781298816204071,
      "learning_rate": 3.0000465226722207e-06,
      "loss": 0.5581,
      "step": 135650
    },
    {
      "epoch": 2.4478738708673635,
      "grad_norm": 0.555682361125946,
      "learning_rate": 2.9906079510019646e-06,
      "loss": 0.5287,
      "step": 135700
    },
    {
      "epoch": 2.4487758240469737,
      "grad_norm": 0.6854458451271057,
      "learning_rate": 2.9811826060041057e-06,
      "loss": 0.6169,
      "step": 135750
    },
    {
      "epoch": 2.4496777772265843,
      "grad_norm": 0.5167728662490845,
      "learning_rate": 2.971770498059354e-06,
      "loss": 0.6289,
      "step": 135800
    },
    {
      "epoch": 2.4505797304061945,
      "grad_norm": 0.7327237129211426,
      "learning_rate": 2.962371637533832e-06,
      "loss": 0.6012,
      "step": 135850
    },
    {
      "epoch": 2.451481683585805,
      "grad_norm": 0.44387760758399963,
      "learning_rate": 2.9529860347790716e-06,
      "loss": 0.6024,
      "step": 135900
    },
    {
      "epoch": 2.4523836367654157,
      "grad_norm": 0.4354304373264313,
      "learning_rate": 2.9436137001320133e-06,
      "loss": 0.5614,
      "step": 135950
    },
    {
      "epoch": 2.453285589945026,
      "grad_norm": 0.4306255877017975,
      "learning_rate": 2.934254643914976e-06,
      "loss": 0.6288,
      "step": 136000
    },
    {
      "epoch": 2.453285589945026,
      "eval_loss": 0.7638341784477234,
      "eval_runtime": 182.5946,
      "eval_samples_per_second": 54.766,
      "eval_steps_per_second": 13.692,
      "step": 136000
    },
    {
      "epoch": 2.4541875431246365,
      "grad_norm": 0.8072941303253174,
      "learning_rate": 2.9249088764356517e-06,
      "loss": 0.5882,
      "step": 136050
    },
    {
      "epoch": 2.4550894963042467,
      "grad_norm": 0.3652213215827942,
      "learning_rate": 2.915576407987106e-06,
      "loss": 0.5443,
      "step": 136100
    },
    {
      "epoch": 2.4559914494838573,
      "grad_norm": 0.408907413482666,
      "learning_rate": 2.906257248847755e-06,
      "loss": 0.5749,
      "step": 136150
    },
    {
      "epoch": 2.456893402663468,
      "grad_norm": 0.6393935084342957,
      "learning_rate": 2.896951409281352e-06,
      "loss": 0.5943,
      "step": 136200
    },
    {
      "epoch": 2.457795355843078,
      "grad_norm": 0.40191152691841125,
      "learning_rate": 2.8876588995369803e-06,
      "loss": 0.6188,
      "step": 136250
    },
    {
      "epoch": 2.4586973090226887,
      "grad_norm": 0.3355206549167633,
      "learning_rate": 2.878379729849049e-06,
      "loss": 0.5268,
      "step": 136300
    },
    {
      "epoch": 2.459599262202299,
      "grad_norm": 0.24561789631843567,
      "learning_rate": 2.86911391043727e-06,
      "loss": 0.601,
      "step": 136350
    },
    {
      "epoch": 2.4605012153819095,
      "grad_norm": 0.3360491394996643,
      "learning_rate": 2.8598614515066468e-06,
      "loss": 0.5815,
      "step": 136400
    },
    {
      "epoch": 2.46140316856152,
      "grad_norm": 0.2388361096382141,
      "learning_rate": 2.8506223632474787e-06,
      "loss": 0.5937,
      "step": 136450
    },
    {
      "epoch": 2.4623051217411303,
      "grad_norm": 0.5490928292274475,
      "learning_rate": 2.841396655835343e-06,
      "loss": 0.6163,
      "step": 136500
    },
    {
      "epoch": 2.463207074920741,
      "grad_norm": 0.4728204309940338,
      "learning_rate": 2.832184339431056e-06,
      "loss": 0.6052,
      "step": 136550
    },
    {
      "epoch": 2.4641090281003515,
      "grad_norm": 0.36887261271476746,
      "learning_rate": 2.8229854241807056e-06,
      "loss": 0.5901,
      "step": 136600
    },
    {
      "epoch": 2.4650109812799617,
      "grad_norm": 0.2573980689048767,
      "learning_rate": 2.813799920215623e-06,
      "loss": 0.6116,
      "step": 136650
    },
    {
      "epoch": 2.4659129344595723,
      "grad_norm": 0.31651633977890015,
      "learning_rate": 2.804627837652355e-06,
      "loss": 0.5431,
      "step": 136700
    },
    {
      "epoch": 2.466814887639183,
      "grad_norm": 0.45686429738998413,
      "learning_rate": 2.79546918659267e-06,
      "loss": 0.584,
      "step": 136750
    },
    {
      "epoch": 2.467716840818793,
      "grad_norm": 0.4356815814971924,
      "learning_rate": 2.786323977123555e-06,
      "loss": 0.6328,
      "step": 136800
    },
    {
      "epoch": 2.4686187939984037,
      "grad_norm": 0.6161597967147827,
      "learning_rate": 2.777192219317177e-06,
      "loss": 0.6738,
      "step": 136850
    },
    {
      "epoch": 2.469520747178014,
      "grad_norm": 0.26330021023750305,
      "learning_rate": 2.768073923230895e-06,
      "loss": 0.6183,
      "step": 136900
    },
    {
      "epoch": 2.4704227003576245,
      "grad_norm": 0.24629034101963043,
      "learning_rate": 2.758969098907248e-06,
      "loss": 0.6182,
      "step": 136950
    },
    {
      "epoch": 2.471324653537235,
      "grad_norm": 0.2127850502729416,
      "learning_rate": 2.749877756373925e-06,
      "loss": 0.5751,
      "step": 137000
    },
    {
      "epoch": 2.4722266067168452,
      "grad_norm": 0.3578275144100189,
      "learning_rate": 2.7407999056437807e-06,
      "loss": 0.595,
      "step": 137050
    },
    {
      "epoch": 2.473128559896456,
      "grad_norm": 0.6383450627326965,
      "learning_rate": 2.731735556714796e-06,
      "loss": 0.5911,
      "step": 137100
    },
    {
      "epoch": 2.474030513076066,
      "grad_norm": 0.3830726742744446,
      "learning_rate": 2.72268471957009e-06,
      "loss": 0.5627,
      "step": 137150
    },
    {
      "epoch": 2.4749324662556766,
      "grad_norm": 0.5871719717979431,
      "learning_rate": 2.713647404177903e-06,
      "loss": 0.5717,
      "step": 137200
    },
    {
      "epoch": 2.4758344194352873,
      "grad_norm": 0.45786869525909424,
      "learning_rate": 2.704623620491577e-06,
      "loss": 0.5695,
      "step": 137250
    },
    {
      "epoch": 2.4767363726148974,
      "grad_norm": 0.41516298055648804,
      "learning_rate": 2.6956133784495484e-06,
      "loss": 0.6192,
      "step": 137300
    },
    {
      "epoch": 2.477638325794508,
      "grad_norm": 0.5618734955787659,
      "learning_rate": 2.686616687975346e-06,
      "loss": 0.6048,
      "step": 137350
    },
    {
      "epoch": 2.478540278974118,
      "grad_norm": 0.7984836101531982,
      "learning_rate": 2.6776335589775776e-06,
      "loss": 0.6214,
      "step": 137400
    },
    {
      "epoch": 2.479442232153729,
      "grad_norm": 0.6889433860778809,
      "learning_rate": 2.668664001349903e-06,
      "loss": 0.596,
      "step": 137450
    },
    {
      "epoch": 2.4803441853333394,
      "grad_norm": 0.787418782711029,
      "learning_rate": 2.659708024971036e-06,
      "loss": 0.6096,
      "step": 137500
    },
    {
      "epoch": 2.4812461385129496,
      "grad_norm": 0.45707640051841736,
      "learning_rate": 2.650765639704746e-06,
      "loss": 0.6154,
      "step": 137550
    },
    {
      "epoch": 2.4821480916925602,
      "grad_norm": 0.47249653935432434,
      "learning_rate": 2.6418368553998197e-06,
      "loss": 0.5791,
      "step": 137600
    },
    {
      "epoch": 2.483050044872171,
      "grad_norm": 0.7047451138496399,
      "learning_rate": 2.6329216818900677e-06,
      "loss": 0.5722,
      "step": 137650
    },
    {
      "epoch": 2.483951998051781,
      "grad_norm": 0.523823082447052,
      "learning_rate": 2.6240201289943154e-06,
      "loss": 0.5206,
      "step": 137700
    },
    {
      "epoch": 2.4848539512313916,
      "grad_norm": 0.37809041142463684,
      "learning_rate": 2.6151322065163796e-06,
      "loss": 0.5976,
      "step": 137750
    },
    {
      "epoch": 2.4857559044110022,
      "grad_norm": 0.3235393762588501,
      "learning_rate": 2.6062579242450748e-06,
      "loss": 0.6449,
      "step": 137800
    },
    {
      "epoch": 2.4866578575906124,
      "grad_norm": 0.6541820168495178,
      "learning_rate": 2.597397291954177e-06,
      "loss": 0.6226,
      "step": 137850
    },
    {
      "epoch": 2.487559810770223,
      "grad_norm": 0.49727940559387207,
      "learning_rate": 2.5885503194024495e-06,
      "loss": 0.6056,
      "step": 137900
    },
    {
      "epoch": 2.488461763949833,
      "grad_norm": 0.496163934469223,
      "learning_rate": 2.579717016333594e-06,
      "loss": 0.5872,
      "step": 137950
    },
    {
      "epoch": 2.489363717129444,
      "grad_norm": 0.4826795756816864,
      "learning_rate": 2.5708973924762626e-06,
      "loss": 0.6236,
      "step": 138000
    },
    {
      "epoch": 2.489363717129444,
      "eval_loss": 0.7643290758132935,
      "eval_runtime": 150.5641,
      "eval_samples_per_second": 66.417,
      "eval_steps_per_second": 16.604,
      "step": 138000
    },
    {
      "epoch": 2.4902656703090544,
      "grad_norm": 0.6207000613212585,
      "learning_rate": 2.5620914575440457e-06,
      "loss": 0.5591,
      "step": 138050
    },
    {
      "epoch": 2.4911676234886646,
      "grad_norm": 0.6973081231117249,
      "learning_rate": 2.5532992212354495e-06,
      "loss": 0.562,
      "step": 138100
    },
    {
      "epoch": 2.492069576668275,
      "grad_norm": 0.47181710600852966,
      "learning_rate": 2.5445206932339026e-06,
      "loss": 0.6124,
      "step": 138150
    },
    {
      "epoch": 2.4929715298478854,
      "grad_norm": 0.5913037657737732,
      "learning_rate": 2.53575588320773e-06,
      "loss": 0.6199,
      "step": 138200
    },
    {
      "epoch": 2.493873483027496,
      "grad_norm": 0.3385108709335327,
      "learning_rate": 2.5270048008101427e-06,
      "loss": 0.5855,
      "step": 138250
    },
    {
      "epoch": 2.4947754362071066,
      "grad_norm": 0.3592880964279175,
      "learning_rate": 2.518267455679246e-06,
      "loss": 0.5753,
      "step": 138300
    },
    {
      "epoch": 2.495677389386717,
      "grad_norm": 0.29369309544563293,
      "learning_rate": 2.509543857438008e-06,
      "loss": 0.6216,
      "step": 138350
    },
    {
      "epoch": 2.4965793425663274,
      "grad_norm": 0.4993029832839966,
      "learning_rate": 2.5008340156942495e-06,
      "loss": 0.5486,
      "step": 138400
    },
    {
      "epoch": 2.4974812957459376,
      "grad_norm": 0.2315986156463623,
      "learning_rate": 2.4921379400406536e-06,
      "loss": 0.5429,
      "step": 138450
    },
    {
      "epoch": 2.498383248925548,
      "grad_norm": 0.7259171605110168,
      "learning_rate": 2.483455640054744e-06,
      "loss": 0.5828,
      "step": 138500
    },
    {
      "epoch": 2.499285202105159,
      "grad_norm": 0.22909699380397797,
      "learning_rate": 2.474787125298849e-06,
      "loss": 0.6141,
      "step": 138550
    },
    {
      "epoch": 2.5001871552847694,
      "grad_norm": 0.2740038335323334,
      "learning_rate": 2.466132405320138e-06,
      "loss": 0.514,
      "step": 138600
    },
    {
      "epoch": 2.5010891084643796,
      "grad_norm": 0.414127379655838,
      "learning_rate": 2.4574914896505824e-06,
      "loss": 0.6536,
      "step": 138650
    },
    {
      "epoch": 2.50199106164399,
      "grad_norm": 0.4939754903316498,
      "learning_rate": 2.4488643878069434e-06,
      "loss": 0.5949,
      "step": 138700
    },
    {
      "epoch": 2.5028930148236004,
      "grad_norm": 0.4386933445930481,
      "learning_rate": 2.4402511092907688e-06,
      "loss": 0.5782,
      "step": 138750
    },
    {
      "epoch": 2.503794968003211,
      "grad_norm": 0.5374831557273865,
      "learning_rate": 2.431651663588392e-06,
      "loss": 0.5799,
      "step": 138800
    },
    {
      "epoch": 2.5046969211828216,
      "grad_norm": 0.42154455184936523,
      "learning_rate": 2.4230660601708998e-06,
      "loss": 0.55,
      "step": 138850
    },
    {
      "epoch": 2.505598874362432,
      "grad_norm": 0.44877198338508606,
      "learning_rate": 2.4144943084941374e-06,
      "loss": 0.5694,
      "step": 138900
    },
    {
      "epoch": 2.5065008275420424,
      "grad_norm": 0.5425922274589539,
      "learning_rate": 2.405936417998701e-06,
      "loss": 0.5769,
      "step": 138950
    },
    {
      "epoch": 2.5074027807216526,
      "grad_norm": 0.3803519308567047,
      "learning_rate": 2.397392398109908e-06,
      "loss": 0.5904,
      "step": 139000
    },
    {
      "epoch": 2.508304733901263,
      "grad_norm": 0.3133341670036316,
      "learning_rate": 2.3888622582378123e-06,
      "loss": 0.6078,
      "step": 139050
    },
    {
      "epoch": 2.509206687080874,
      "grad_norm": 0.5647991299629211,
      "learning_rate": 2.380346007777171e-06,
      "loss": 0.5597,
      "step": 139100
    },
    {
      "epoch": 2.510108640260484,
      "grad_norm": 0.471080482006073,
      "learning_rate": 2.3718436561074447e-06,
      "loss": 0.5904,
      "step": 139150
    },
    {
      "epoch": 2.5110105934400946,
      "grad_norm": 0.5082967281341553,
      "learning_rate": 2.3633552125927975e-06,
      "loss": 0.5929,
      "step": 139200
    },
    {
      "epoch": 2.5119125466197048,
      "grad_norm": 0.3389081656932831,
      "learning_rate": 2.3548806865820634e-06,
      "loss": 0.5996,
      "step": 139250
    },
    {
      "epoch": 2.5128144997993154,
      "grad_norm": 0.707694947719574,
      "learning_rate": 2.346420087408747e-06,
      "loss": 0.6185,
      "step": 139300
    },
    {
      "epoch": 2.513716452978926,
      "grad_norm": 0.3534972369670868,
      "learning_rate": 2.3379734243910256e-06,
      "loss": 0.5575,
      "step": 139350
    },
    {
      "epoch": 2.514618406158536,
      "grad_norm": 0.5030869841575623,
      "learning_rate": 2.329540706831727e-06,
      "loss": 0.5818,
      "step": 139400
    },
    {
      "epoch": 2.5155203593381468,
      "grad_norm": 0.5516932010650635,
      "learning_rate": 2.3211219440183023e-06,
      "loss": 0.6261,
      "step": 139450
    },
    {
      "epoch": 2.516422312517757,
      "grad_norm": 0.41761651635169983,
      "learning_rate": 2.312717145222853e-06,
      "loss": 0.587,
      "step": 139500
    },
    {
      "epoch": 2.5173242656973676,
      "grad_norm": 0.6663625836372375,
      "learning_rate": 2.304326319702096e-06,
      "loss": 0.5955,
      "step": 139550
    },
    {
      "epoch": 2.518226218876978,
      "grad_norm": 0.5844576358795166,
      "learning_rate": 2.295949476697354e-06,
      "loss": 0.5637,
      "step": 139600
    },
    {
      "epoch": 2.519128172056589,
      "grad_norm": 0.33922675251960754,
      "learning_rate": 2.28758662543455e-06,
      "loss": 0.5745,
      "step": 139650
    },
    {
      "epoch": 2.520030125236199,
      "grad_norm": 0.5177701711654663,
      "learning_rate": 2.279237775124202e-06,
      "loss": 0.6076,
      "step": 139700
    },
    {
      "epoch": 2.5209320784158096,
      "grad_norm": 0.33643025159835815,
      "learning_rate": 2.2709029349614146e-06,
      "loss": 0.5769,
      "step": 139750
    },
    {
      "epoch": 2.5218340315954197,
      "grad_norm": 0.3329255282878876,
      "learning_rate": 2.262582114125838e-06,
      "loss": 0.6177,
      "step": 139800
    },
    {
      "epoch": 2.5227359847750304,
      "grad_norm": 0.357241690158844,
      "learning_rate": 2.254275321781703e-06,
      "loss": 0.5425,
      "step": 139850
    },
    {
      "epoch": 2.523637937954641,
      "grad_norm": 0.4908055067062378,
      "learning_rate": 2.2459825670777883e-06,
      "loss": 0.5492,
      "step": 139900
    },
    {
      "epoch": 2.524539891134251,
      "grad_norm": 0.4387037456035614,
      "learning_rate": 2.2377038591474065e-06,
      "loss": 0.6058,
      "step": 139950
    },
    {
      "epoch": 2.5254418443138618,
      "grad_norm": 0.7840875387191772,
      "learning_rate": 2.229439207108395e-06,
      "loss": 0.5616,
      "step": 140000
    },
    {
      "epoch": 2.5254418443138618,
      "eval_loss": 0.7634694576263428,
      "eval_runtime": 151.3151,
      "eval_samples_per_second": 66.087,
      "eval_steps_per_second": 16.522,
      "step": 140000
    },
    {
      "epoch": 2.526343797493472,
      "grad_norm": 0.6071146130561829,
      "learning_rate": 2.2211886200631254e-06,
      "loss": 0.5829,
      "step": 140050
    },
    {
      "epoch": 2.5272457506730825,
      "grad_norm": 0.38915908336639404,
      "learning_rate": 2.212952107098465e-06,
      "loss": 0.5476,
      "step": 140100
    },
    {
      "epoch": 2.528147703852693,
      "grad_norm": 0.5435897707939148,
      "learning_rate": 2.204729677285784e-06,
      "loss": 0.5936,
      "step": 140150
    },
    {
      "epoch": 2.5290496570323033,
      "grad_norm": 0.6065568327903748,
      "learning_rate": 2.1965213396809485e-06,
      "loss": 0.596,
      "step": 140200
    },
    {
      "epoch": 2.529951610211914,
      "grad_norm": 0.6107475161552429,
      "learning_rate": 2.1883271033242925e-06,
      "loss": 0.6438,
      "step": 140250
    },
    {
      "epoch": 2.530853563391524,
      "grad_norm": 0.15120959281921387,
      "learning_rate": 2.1801469772406346e-06,
      "loss": 0.5754,
      "step": 140300
    },
    {
      "epoch": 2.5317555165711347,
      "grad_norm": 0.26422765851020813,
      "learning_rate": 2.171980970439237e-06,
      "loss": 0.6015,
      "step": 140350
    },
    {
      "epoch": 2.5326574697507453,
      "grad_norm": 0.3375273048877716,
      "learning_rate": 2.16382909191382e-06,
      "loss": 0.5584,
      "step": 140400
    },
    {
      "epoch": 2.5335594229303555,
      "grad_norm": 0.3625580668449402,
      "learning_rate": 2.1556913506425437e-06,
      "loss": 0.6019,
      "step": 140450
    },
    {
      "epoch": 2.534461376109966,
      "grad_norm": 0.4220735430717468,
      "learning_rate": 2.1475677555879984e-06,
      "loss": 0.5596,
      "step": 140500
    },
    {
      "epoch": 2.5353633292895763,
      "grad_norm": 0.2656625509262085,
      "learning_rate": 2.139458315697185e-06,
      "loss": 0.5747,
      "step": 140550
    },
    {
      "epoch": 2.536265282469187,
      "grad_norm": 0.344159334897995,
      "learning_rate": 2.131363039901526e-06,
      "loss": 0.5535,
      "step": 140600
    },
    {
      "epoch": 2.5371672356487975,
      "grad_norm": 0.5189008712768555,
      "learning_rate": 2.1232819371168487e-06,
      "loss": 0.5593,
      "step": 140650
    },
    {
      "epoch": 2.538069188828408,
      "grad_norm": 0.45415276288986206,
      "learning_rate": 2.115215016243347e-06,
      "loss": 0.6387,
      "step": 140700
    },
    {
      "epoch": 2.5389711420080183,
      "grad_norm": 0.17955875396728516,
      "learning_rate": 2.1071622861656188e-06,
      "loss": 0.5921,
      "step": 140750
    },
    {
      "epoch": 2.539873095187629,
      "grad_norm": 0.31710922718048096,
      "learning_rate": 2.099123755752626e-06,
      "loss": 0.5023,
      "step": 140800
    },
    {
      "epoch": 2.540775048367239,
      "grad_norm": 0.24994120001792908,
      "learning_rate": 2.091099433857687e-06,
      "loss": 0.5608,
      "step": 140850
    },
    {
      "epoch": 2.5416770015468497,
      "grad_norm": 0.5651916861534119,
      "learning_rate": 2.083089329318471e-06,
      "loss": 0.579,
      "step": 140900
    },
    {
      "epoch": 2.5425789547264603,
      "grad_norm": 0.5476647615432739,
      "learning_rate": 2.0750934509570007e-06,
      "loss": 0.5598,
      "step": 140950
    },
    {
      "epoch": 2.5434809079060705,
      "grad_norm": 0.49658119678497314,
      "learning_rate": 2.067111807579617e-06,
      "loss": 0.5723,
      "step": 141000
    },
    {
      "epoch": 2.544382861085681,
      "grad_norm": 0.5420461893081665,
      "learning_rate": 2.059144407976986e-06,
      "loss": 0.6418,
      "step": 141050
    },
    {
      "epoch": 2.5452848142652913,
      "grad_norm": 0.42805221676826477,
      "learning_rate": 2.0511912609240956e-06,
      "loss": 0.5738,
      "step": 141100
    },
    {
      "epoch": 2.546186767444902,
      "grad_norm": 0.4822250306606293,
      "learning_rate": 2.0432523751802208e-06,
      "loss": 0.5899,
      "step": 141150
    },
    {
      "epoch": 2.5470887206245125,
      "grad_norm": 0.6639997959136963,
      "learning_rate": 2.035327759488947e-06,
      "loss": 0.5929,
      "step": 141200
    },
    {
      "epoch": 2.5479906738041227,
      "grad_norm": 0.4948303699493408,
      "learning_rate": 2.0274174225781294e-06,
      "loss": 0.5488,
      "step": 141250
    },
    {
      "epoch": 2.5488926269837333,
      "grad_norm": 0.40180355310440063,
      "learning_rate": 2.019521373159901e-06,
      "loss": 0.5692,
      "step": 141300
    },
    {
      "epoch": 2.5497945801633435,
      "grad_norm": 0.4994036555290222,
      "learning_rate": 2.0116396199306654e-06,
      "loss": 0.5903,
      "step": 141350
    },
    {
      "epoch": 2.550696533342954,
      "grad_norm": 0.4937199354171753,
      "learning_rate": 2.0037721715710685e-06,
      "loss": 0.5679,
      "step": 141400
    },
    {
      "epoch": 2.5515984865225647,
      "grad_norm": 0.572460412979126,
      "learning_rate": 1.9959190367460173e-06,
      "loss": 0.6017,
      "step": 141450
    },
    {
      "epoch": 2.5525004397021753,
      "grad_norm": 0.559445858001709,
      "learning_rate": 1.988080224104637e-06,
      "loss": 0.564,
      "step": 141500
    },
    {
      "epoch": 2.5534023928817855,
      "grad_norm": 0.5924715995788574,
      "learning_rate": 1.980255742280298e-06,
      "loss": 0.5641,
      "step": 141550
    },
    {
      "epoch": 2.554304346061396,
      "grad_norm": 0.46145761013031006,
      "learning_rate": 1.9724455998905686e-06,
      "loss": 0.6317,
      "step": 141600
    },
    {
      "epoch": 2.5552062992410063,
      "grad_norm": 0.3897159993648529,
      "learning_rate": 1.9646498055372335e-06,
      "loss": 0.5897,
      "step": 141650
    },
    {
      "epoch": 2.556108252420617,
      "grad_norm": 0.4173280596733093,
      "learning_rate": 1.956868367806279e-06,
      "loss": 0.5825,
      "step": 141700
    },
    {
      "epoch": 2.5570102056002275,
      "grad_norm": 0.5771593451499939,
      "learning_rate": 1.949101295267874e-06,
      "loss": 0.618,
      "step": 141750
    },
    {
      "epoch": 2.5579121587798377,
      "grad_norm": 0.4818532466888428,
      "learning_rate": 1.9413485964763595e-06,
      "loss": 0.5884,
      "step": 141800
    },
    {
      "epoch": 2.5588141119594483,
      "grad_norm": 0.37348297238349915,
      "learning_rate": 1.9336102799702594e-06,
      "loss": 0.5265,
      "step": 141850
    },
    {
      "epoch": 2.5597160651390585,
      "grad_norm": 0.4741799533367157,
      "learning_rate": 1.925886354272253e-06,
      "loss": 0.5985,
      "step": 141900
    },
    {
      "epoch": 2.560618018318669,
      "grad_norm": 0.6350852251052856,
      "learning_rate": 1.9181768278891665e-06,
      "loss": 0.5989,
      "step": 141950
    },
    {
      "epoch": 2.5615199714982797,
      "grad_norm": 0.7729712128639221,
      "learning_rate": 1.9104817093119654e-06,
      "loss": 0.5669,
      "step": 142000
    },
    {
      "epoch": 2.5615199714982797,
      "eval_loss": 0.7644696235656738,
      "eval_runtime": 174.077,
      "eval_samples_per_second": 57.446,
      "eval_steps_per_second": 14.361,
      "step": 142000
    },
    {
      "epoch": 2.56242192467789,
      "grad_norm": 0.342607319355011,
      "learning_rate": 1.9028010070157582e-06,
      "loss": 0.6029,
      "step": 142050
    },
    {
      "epoch": 2.5633238778575005,
      "grad_norm": 0.5920534133911133,
      "learning_rate": 1.895134729459763e-06,
      "loss": 0.6247,
      "step": 142100
    },
    {
      "epoch": 2.5642258310371107,
      "grad_norm": 0.4925214648246765,
      "learning_rate": 1.8874828850873166e-06,
      "loss": 0.5688,
      "step": 142150
    },
    {
      "epoch": 2.5651277842167213,
      "grad_norm": 0.3967146873474121,
      "learning_rate": 1.8798454823258637e-06,
      "loss": 0.6114,
      "step": 142200
    },
    {
      "epoch": 2.566029737396332,
      "grad_norm": 0.4188070595264435,
      "learning_rate": 1.8722225295869345e-06,
      "loss": 0.5867,
      "step": 142250
    },
    {
      "epoch": 2.566931690575942,
      "grad_norm": 0.7461904287338257,
      "learning_rate": 1.8646140352661546e-06,
      "loss": 0.5942,
      "step": 142300
    },
    {
      "epoch": 2.5678336437555527,
      "grad_norm": 0.3654079735279083,
      "learning_rate": 1.8570200077432193e-06,
      "loss": 0.6142,
      "step": 142350
    },
    {
      "epoch": 2.568735596935163,
      "grad_norm": 0.6951687932014465,
      "learning_rate": 1.8494404553818866e-06,
      "loss": 0.6116,
      "step": 142400
    },
    {
      "epoch": 2.5696375501147735,
      "grad_norm": 0.7327207922935486,
      "learning_rate": 1.8418753865299842e-06,
      "loss": 0.5487,
      "step": 142450
    },
    {
      "epoch": 2.570539503294384,
      "grad_norm": 0.23254691064357758,
      "learning_rate": 1.8343248095193803e-06,
      "loss": 0.5986,
      "step": 142500
    },
    {
      "epoch": 2.5714414564739947,
      "grad_norm": 0.5144098401069641,
      "learning_rate": 1.8267887326659799e-06,
      "loss": 0.6114,
      "step": 142550
    },
    {
      "epoch": 2.572343409653605,
      "grad_norm": 0.3097574710845947,
      "learning_rate": 1.8192671642697234e-06,
      "loss": 0.601,
      "step": 142600
    },
    {
      "epoch": 2.5732453628332155,
      "grad_norm": 0.487928181886673,
      "learning_rate": 1.8117601126145795e-06,
      "loss": 0.6245,
      "step": 142650
    },
    {
      "epoch": 2.5741473160128256,
      "grad_norm": 0.4464204013347626,
      "learning_rate": 1.804267585968506e-06,
      "loss": 0.5652,
      "step": 142700
    },
    {
      "epoch": 2.5750492691924363,
      "grad_norm": 0.28331393003463745,
      "learning_rate": 1.796789592583482e-06,
      "loss": 0.55,
      "step": 142750
    },
    {
      "epoch": 2.575951222372047,
      "grad_norm": 0.25004369020462036,
      "learning_rate": 1.7893261406954798e-06,
      "loss": 0.5674,
      "step": 142800
    },
    {
      "epoch": 2.576853175551657,
      "grad_norm": 0.42784038186073303,
      "learning_rate": 1.7818772385244502e-06,
      "loss": 0.6324,
      "step": 142850
    },
    {
      "epoch": 2.5777551287312677,
      "grad_norm": 0.38246968388557434,
      "learning_rate": 1.7744428942743163e-06,
      "loss": 0.6174,
      "step": 142900
    },
    {
      "epoch": 2.578657081910878,
      "grad_norm": 0.3872022032737732,
      "learning_rate": 1.7670231161329814e-06,
      "loss": 0.5766,
      "step": 142950
    },
    {
      "epoch": 2.5795590350904885,
      "grad_norm": 0.45683926343917847,
      "learning_rate": 1.7596179122722904e-06,
      "loss": 0.5684,
      "step": 143000
    },
    {
      "epoch": 2.580460988270099,
      "grad_norm": 0.22029702365398407,
      "learning_rate": 1.7522272908480448e-06,
      "loss": 0.6118,
      "step": 143050
    },
    {
      "epoch": 2.5813629414497092,
      "grad_norm": 0.4829256236553192,
      "learning_rate": 1.7448512599999844e-06,
      "loss": 0.5624,
      "step": 143100
    },
    {
      "epoch": 2.58226489462932,
      "grad_norm": 0.25297021865844727,
      "learning_rate": 1.7374898278517847e-06,
      "loss": 0.5521,
      "step": 143150
    },
    {
      "epoch": 2.58316684780893,
      "grad_norm": 0.4276270568370819,
      "learning_rate": 1.7301430025110337e-06,
      "loss": 0.5231,
      "step": 143200
    },
    {
      "epoch": 2.5840688009885406,
      "grad_norm": 0.6568397283554077,
      "learning_rate": 1.7228107920692337e-06,
      "loss": 0.5916,
      "step": 143250
    },
    {
      "epoch": 2.5849707541681513,
      "grad_norm": 0.5766966342926025,
      "learning_rate": 1.7154932046018002e-06,
      "loss": 0.572,
      "step": 143300
    },
    {
      "epoch": 2.5858727073477614,
      "grad_norm": 0.40604475140571594,
      "learning_rate": 1.7081902481680312e-06,
      "loss": 0.6016,
      "step": 143350
    },
    {
      "epoch": 2.586774660527372,
      "grad_norm": 0.3906913995742798,
      "learning_rate": 1.700901930811113e-06,
      "loss": 0.5884,
      "step": 143400
    },
    {
      "epoch": 2.587676613706982,
      "grad_norm": 0.6699939966201782,
      "learning_rate": 1.6936282605581193e-06,
      "loss": 0.6107,
      "step": 143450
    },
    {
      "epoch": 2.588578566886593,
      "grad_norm": 0.3278249502182007,
      "learning_rate": 1.6863692454199776e-06,
      "loss": 0.5473,
      "step": 143500
    },
    {
      "epoch": 2.5894805200662034,
      "grad_norm": 0.39136597514152527,
      "learning_rate": 1.6791248933914888e-06,
      "loss": 0.604,
      "step": 143550
    },
    {
      "epoch": 2.590382473245814,
      "grad_norm": 0.37507614493370056,
      "learning_rate": 1.6718952124512943e-06,
      "loss": 0.6673,
      "step": 143600
    },
    {
      "epoch": 2.5912844264254242,
      "grad_norm": 0.3167797029018402,
      "learning_rate": 1.664680210561878e-06,
      "loss": 0.5394,
      "step": 143650
    },
    {
      "epoch": 2.592186379605035,
      "grad_norm": 0.45170313119888306,
      "learning_rate": 1.6574798956695637e-06,
      "loss": 0.5697,
      "step": 143700
    },
    {
      "epoch": 2.593088332784645,
      "grad_norm": 0.4166909456253052,
      "learning_rate": 1.6502942757044925e-06,
      "loss": 0.5807,
      "step": 143750
    },
    {
      "epoch": 2.5939902859642556,
      "grad_norm": 0.3023279011249542,
      "learning_rate": 1.6431233585806227e-06,
      "loss": 0.5494,
      "step": 143800
    },
    {
      "epoch": 2.5948922391438662,
      "grad_norm": 0.38187670707702637,
      "learning_rate": 1.6359671521957215e-06,
      "loss": 0.5778,
      "step": 143850
    },
    {
      "epoch": 2.5957941923234764,
      "grad_norm": 0.5198872089385986,
      "learning_rate": 1.628825664431361e-06,
      "loss": 0.6088,
      "step": 143900
    },
    {
      "epoch": 2.596696145503087,
      "grad_norm": 0.4194987714290619,
      "learning_rate": 1.621698903152883e-06,
      "loss": 0.5856,
      "step": 143950
    },
    {
      "epoch": 2.597598098682697,
      "grad_norm": 0.4225956201553345,
      "learning_rate": 1.6145868762094268e-06,
      "loss": 0.5682,
      "step": 144000
    },
    {
      "epoch": 2.597598098682697,
      "eval_loss": 0.7635864019393921,
      "eval_runtime": 122.6497,
      "eval_samples_per_second": 81.533,
      "eval_steps_per_second": 20.383,
      "step": 144000
    },
    {
      "epoch": 2.598500051862308,
      "grad_norm": 0.34509366750717163,
      "learning_rate": 1.6074895914339037e-06,
      "loss": 0.5479,
      "step": 144050
    },
    {
      "epoch": 2.5994020050419184,
      "grad_norm": 0.24754537642002106,
      "learning_rate": 1.6004070566429797e-06,
      "loss": 0.5992,
      "step": 144100
    },
    {
      "epoch": 2.6003039582215286,
      "grad_norm": 0.5037423968315125,
      "learning_rate": 1.5933392796370795e-06,
      "loss": 0.6069,
      "step": 144150
    },
    {
      "epoch": 2.601205911401139,
      "grad_norm": 0.3618796467781067,
      "learning_rate": 1.5862862682003797e-06,
      "loss": 0.5152,
      "step": 144200
    },
    {
      "epoch": 2.6021078645807494,
      "grad_norm": 0.4335566759109497,
      "learning_rate": 1.5792480301007855e-06,
      "loss": 0.5433,
      "step": 144250
    },
    {
      "epoch": 2.60300981776036,
      "grad_norm": 0.3015955686569214,
      "learning_rate": 1.5722245730899359e-06,
      "loss": 0.631,
      "step": 144300
    },
    {
      "epoch": 2.6039117709399706,
      "grad_norm": 0.46158406138420105,
      "learning_rate": 1.5652159049031933e-06,
      "loss": 0.5823,
      "step": 144350
    },
    {
      "epoch": 2.604813724119581,
      "grad_norm": 0.6655365228652954,
      "learning_rate": 1.5582220332596247e-06,
      "loss": 0.5984,
      "step": 144400
    },
    {
      "epoch": 2.6057156772991914,
      "grad_norm": 0.44420215487480164,
      "learning_rate": 1.551242965862012e-06,
      "loss": 0.6236,
      "step": 144450
    },
    {
      "epoch": 2.6066176304788016,
      "grad_norm": 0.5080975294113159,
      "learning_rate": 1.5442787103968231e-06,
      "loss": 0.5897,
      "step": 144500
    },
    {
      "epoch": 2.607519583658412,
      "grad_norm": 0.47766098380088806,
      "learning_rate": 1.5373292745342093e-06,
      "loss": 0.5697,
      "step": 144550
    },
    {
      "epoch": 2.608421536838023,
      "grad_norm": 0.6013014912605286,
      "learning_rate": 1.5303946659280165e-06,
      "loss": 0.5915,
      "step": 144600
    },
    {
      "epoch": 2.6093234900176334,
      "grad_norm": 0.33277660608291626,
      "learning_rate": 1.5234748922157438e-06,
      "loss": 0.6355,
      "step": 144650
    },
    {
      "epoch": 2.6102254431972436,
      "grad_norm": 0.38805481791496277,
      "learning_rate": 1.5165699610185547e-06,
      "loss": 0.5683,
      "step": 144700
    },
    {
      "epoch": 2.611127396376854,
      "grad_norm": 0.43465396761894226,
      "learning_rate": 1.5096798799412741e-06,
      "loss": 0.5907,
      "step": 144750
    },
    {
      "epoch": 2.6120293495564644,
      "grad_norm": 0.5539859533309937,
      "learning_rate": 1.5028046565723723e-06,
      "loss": 0.5833,
      "step": 144800
    },
    {
      "epoch": 2.612931302736075,
      "grad_norm": 0.4338020980358124,
      "learning_rate": 1.4959442984839371e-06,
      "loss": 0.5442,
      "step": 144850
    },
    {
      "epoch": 2.6138332559156856,
      "grad_norm": 0.4068832993507385,
      "learning_rate": 1.489098813231703e-06,
      "loss": 0.5435,
      "step": 144900
    },
    {
      "epoch": 2.6147352090952958,
      "grad_norm": 0.271906316280365,
      "learning_rate": 1.4822682083550238e-06,
      "loss": 0.558,
      "step": 144950
    },
    {
      "epoch": 2.6156371622749064,
      "grad_norm": 0.285663902759552,
      "learning_rate": 1.475452491376852e-06,
      "loss": 0.5881,
      "step": 145000
    },
    {
      "epoch": 2.6165391154545166,
      "grad_norm": 0.5845991969108582,
      "learning_rate": 1.4686516698037512e-06,
      "loss": 0.6608,
      "step": 145050
    },
    {
      "epoch": 2.617441068634127,
      "grad_norm": 0.7682216763496399,
      "learning_rate": 1.4618657511258815e-06,
      "loss": 0.5601,
      "step": 145100
    },
    {
      "epoch": 2.618343021813738,
      "grad_norm": 0.5232943892478943,
      "learning_rate": 1.4550947428169915e-06,
      "loss": 0.6213,
      "step": 145150
    },
    {
      "epoch": 2.619244974993348,
      "grad_norm": 0.26218658685684204,
      "learning_rate": 1.4483386523343923e-06,
      "loss": 0.6594,
      "step": 145200
    },
    {
      "epoch": 2.6201469281729586,
      "grad_norm": 0.386810839176178,
      "learning_rate": 1.4415974871189819e-06,
      "loss": 0.6322,
      "step": 145250
    },
    {
      "epoch": 2.6210488813525687,
      "grad_norm": 0.4550589919090271,
      "learning_rate": 1.4348712545952197e-06,
      "loss": 0.6267,
      "step": 145300
    },
    {
      "epoch": 2.6219508345321794,
      "grad_norm": 0.23667468130588531,
      "learning_rate": 1.4281599621711082e-06,
      "loss": 0.585,
      "step": 145350
    },
    {
      "epoch": 2.62285278771179,
      "grad_norm": 1.0433375835418701,
      "learning_rate": 1.4214636172382e-06,
      "loss": 0.6312,
      "step": 145400
    },
    {
      "epoch": 2.6237547408914006,
      "grad_norm": 0.4860326051712036,
      "learning_rate": 1.4147822271715905e-06,
      "loss": 0.6161,
      "step": 145450
    },
    {
      "epoch": 2.6246566940710108,
      "grad_norm": 0.6891937255859375,
      "learning_rate": 1.4081157993298998e-06,
      "loss": 0.5992,
      "step": 145500
    },
    {
      "epoch": 2.6255586472506214,
      "grad_norm": 0.3236865699291229,
      "learning_rate": 1.4014643410552636e-06,
      "loss": 0.5661,
      "step": 145550
    },
    {
      "epoch": 2.6264606004302316,
      "grad_norm": 0.34108689427375793,
      "learning_rate": 1.394827859673345e-06,
      "loss": 0.5892,
      "step": 145600
    },
    {
      "epoch": 2.627362553609842,
      "grad_norm": 0.3277568817138672,
      "learning_rate": 1.3882063624932956e-06,
      "loss": 0.5938,
      "step": 145650
    },
    {
      "epoch": 2.628264506789453,
      "grad_norm": 0.66484135389328,
      "learning_rate": 1.3815998568077803e-06,
      "loss": 0.5899,
      "step": 145700
    },
    {
      "epoch": 2.629166459969063,
      "grad_norm": 0.26737546920776367,
      "learning_rate": 1.3750083498929405e-06,
      "loss": 0.5742,
      "step": 145750
    },
    {
      "epoch": 2.6300684131486736,
      "grad_norm": 0.5975849032402039,
      "learning_rate": 1.368431849008399e-06,
      "loss": 0.5551,
      "step": 145800
    },
    {
      "epoch": 2.6309703663282837,
      "grad_norm": 0.24138139188289642,
      "learning_rate": 1.3618703613972637e-06,
      "loss": 0.5977,
      "step": 145850
    },
    {
      "epoch": 2.6318723195078944,
      "grad_norm": 0.3592613935470581,
      "learning_rate": 1.3553238942860958e-06,
      "loss": 0.6006,
      "step": 145900
    },
    {
      "epoch": 2.632774272687505,
      "grad_norm": 0.5922103524208069,
      "learning_rate": 1.3487924548849106e-06,
      "loss": 0.6426,
      "step": 145950
    },
    {
      "epoch": 2.633676225867115,
      "grad_norm": 0.3358805179595947,
      "learning_rate": 1.3422760503871862e-06,
      "loss": 0.5849,
      "step": 146000
    },
    {
      "epoch": 2.633676225867115,
      "eval_loss": 0.7640622854232788,
      "eval_runtime": 121.1567,
      "eval_samples_per_second": 82.538,
      "eval_steps_per_second": 20.634,
      "step": 146000
    },
    {
      "epoch": 2.6345781790467258,
      "grad_norm": 1.0219449996948242,
      "learning_rate": 1.3357746879698379e-06,
      "loss": 0.5647,
      "step": 146050
    },
    {
      "epoch": 2.635480132226336,
      "grad_norm": 0.341564804315567,
      "learning_rate": 1.3292883747931984e-06,
      "loss": 0.5785,
      "step": 146100
    },
    {
      "epoch": 2.6363820854059465,
      "grad_norm": 0.4754592776298523,
      "learning_rate": 1.3228171180010462e-06,
      "loss": 0.6482,
      "step": 146150
    },
    {
      "epoch": 2.637284038585557,
      "grad_norm": 0.5812873840332031,
      "learning_rate": 1.3163609247205704e-06,
      "loss": 0.5494,
      "step": 146200
    },
    {
      "epoch": 2.6381859917651673,
      "grad_norm": 0.3060818314552307,
      "learning_rate": 1.3099198020623666e-06,
      "loss": 0.6103,
      "step": 146250
    },
    {
      "epoch": 2.639087944944778,
      "grad_norm": 0.4327227473258972,
      "learning_rate": 1.3034937571204297e-06,
      "loss": 0.5685,
      "step": 146300
    },
    {
      "epoch": 2.639989898124388,
      "grad_norm": 0.3437652587890625,
      "learning_rate": 1.2970827969721606e-06,
      "loss": 0.5973,
      "step": 146350
    },
    {
      "epoch": 2.6408918513039987,
      "grad_norm": 0.6505715847015381,
      "learning_rate": 1.290686928678334e-06,
      "loss": 0.5982,
      "step": 146400
    },
    {
      "epoch": 2.6417938044836093,
      "grad_norm": 0.2995924949645996,
      "learning_rate": 1.2843061592831073e-06,
      "loss": 0.5544,
      "step": 146450
    },
    {
      "epoch": 2.64269575766322,
      "grad_norm": 0.7071792483329773,
      "learning_rate": 1.277940495814014e-06,
      "loss": 0.6103,
      "step": 146500
    },
    {
      "epoch": 2.64359771084283,
      "grad_norm": 0.4494824707508087,
      "learning_rate": 1.271589945281939e-06,
      "loss": 0.5491,
      "step": 146550
    },
    {
      "epoch": 2.6444996640224407,
      "grad_norm": 0.6523396372795105,
      "learning_rate": 1.2652545146811345e-06,
      "loss": 0.6241,
      "step": 146600
    },
    {
      "epoch": 2.645401617202051,
      "grad_norm": 0.4432896673679352,
      "learning_rate": 1.2589342109891921e-06,
      "loss": 0.5707,
      "step": 146650
    },
    {
      "epoch": 2.6463035703816615,
      "grad_norm": 0.34421876072883606,
      "learning_rate": 1.2526290411670487e-06,
      "loss": 0.5781,
      "step": 146700
    },
    {
      "epoch": 2.647205523561272,
      "grad_norm": 0.23897890746593475,
      "learning_rate": 1.2463390121589679e-06,
      "loss": 0.5913,
      "step": 146750
    },
    {
      "epoch": 2.6481074767408823,
      "grad_norm": 0.5793823003768921,
      "learning_rate": 1.2400641308925382e-06,
      "loss": 0.514,
      "step": 146800
    },
    {
      "epoch": 2.649009429920493,
      "grad_norm": 0.45278990268707275,
      "learning_rate": 1.2338044042786728e-06,
      "loss": 0.5605,
      "step": 146850
    },
    {
      "epoch": 2.649911383100103,
      "grad_norm": 0.2959030866622925,
      "learning_rate": 1.227559839211584e-06,
      "loss": 0.6189,
      "step": 146900
    },
    {
      "epoch": 2.6508133362797137,
      "grad_norm": 0.4584338366985321,
      "learning_rate": 1.2213304425687949e-06,
      "loss": 0.5825,
      "step": 146950
    },
    {
      "epoch": 2.6517152894593243,
      "grad_norm": 0.7765092849731445,
      "learning_rate": 1.2151162212111144e-06,
      "loss": 0.6122,
      "step": 147000
    },
    {
      "epoch": 2.6526172426389345,
      "grad_norm": 0.406610906124115,
      "learning_rate": 1.2089171819826395e-06,
      "loss": 0.5282,
      "step": 147050
    },
    {
      "epoch": 2.653519195818545,
      "grad_norm": 0.8786035776138306,
      "learning_rate": 1.2027333317107553e-06,
      "loss": 0.6015,
      "step": 147100
    },
    {
      "epoch": 2.6544211489981553,
      "grad_norm": 0.4096296727657318,
      "learning_rate": 1.1965646772061072e-06,
      "loss": 0.5792,
      "step": 147150
    },
    {
      "epoch": 2.655323102177766,
      "grad_norm": 0.4295187294483185,
      "learning_rate": 1.1904112252626059e-06,
      "loss": 0.5913,
      "step": 147200
    },
    {
      "epoch": 2.6562250553573765,
      "grad_norm": 0.33223652839660645,
      "learning_rate": 1.1842729826574266e-06,
      "loss": 0.5701,
      "step": 147250
    },
    {
      "epoch": 2.6571270085369867,
      "grad_norm": 0.5879573225975037,
      "learning_rate": 1.1781499561509884e-06,
      "loss": 0.5891,
      "step": 147300
    },
    {
      "epoch": 2.6580289617165973,
      "grad_norm": 0.664929986000061,
      "learning_rate": 1.1720421524869501e-06,
      "loss": 0.5746,
      "step": 147350
    },
    {
      "epoch": 2.6589309148962075,
      "grad_norm": 0.40634119510650635,
      "learning_rate": 1.1659495783922042e-06,
      "loss": 0.5975,
      "step": 147400
    },
    {
      "epoch": 2.659832868075818,
      "grad_norm": 0.7746046781539917,
      "learning_rate": 1.159872240576878e-06,
      "loss": 0.5655,
      "step": 147450
    },
    {
      "epoch": 2.6607348212554287,
      "grad_norm": 0.37878575921058655,
      "learning_rate": 1.1538101457343093e-06,
      "loss": 0.5856,
      "step": 147500
    },
    {
      "epoch": 2.6616367744350393,
      "grad_norm": 0.3991892337799072,
      "learning_rate": 1.1477633005410488e-06,
      "loss": 0.5656,
      "step": 147550
    },
    {
      "epoch": 2.6625387276146495,
      "grad_norm": 1.1035449504852295,
      "learning_rate": 1.1417317116568594e-06,
      "loss": 0.5938,
      "step": 147600
    },
    {
      "epoch": 2.66344068079426,
      "grad_norm": 0.6442781090736389,
      "learning_rate": 1.1357153857246905e-06,
      "loss": 0.6146,
      "step": 147650
    },
    {
      "epoch": 2.6643426339738703,
      "grad_norm": 0.3615952432155609,
      "learning_rate": 1.1297143293706935e-06,
      "loss": 0.5931,
      "step": 147700
    },
    {
      "epoch": 2.665244587153481,
      "grad_norm": 0.23007741570472717,
      "learning_rate": 1.1237285492041898e-06,
      "loss": 0.574,
      "step": 147750
    },
    {
      "epoch": 2.6661465403330915,
      "grad_norm": 0.5243186354637146,
      "learning_rate": 1.1177580518176844e-06,
      "loss": 0.6311,
      "step": 147800
    },
    {
      "epoch": 2.6670484935127017,
      "grad_norm": 0.7341928482055664,
      "learning_rate": 1.1118028437868522e-06,
      "loss": 0.6379,
      "step": 147850
    },
    {
      "epoch": 2.6679504466923123,
      "grad_norm": 0.2726113498210907,
      "learning_rate": 1.1058629316705216e-06,
      "loss": 0.5689,
      "step": 147900
    },
    {
      "epoch": 2.6688523998719225,
      "grad_norm": 0.4806053340435028,
      "learning_rate": 1.0999383220106747e-06,
      "loss": 0.5939,
      "step": 147950
    },
    {
      "epoch": 2.669754353051533,
      "grad_norm": 0.8111446499824524,
      "learning_rate": 1.09402902133245e-06,
      "loss": 0.5709,
      "step": 148000
    },
    {
      "epoch": 2.669754353051533,
      "eval_loss": 0.7641708254814148,
      "eval_runtime": 121.4163,
      "eval_samples_per_second": 82.361,
      "eval_steps_per_second": 20.59,
      "step": 148000
    },
    {
      "epoch": 2.6706563062311437,
      "grad_norm": 0.20406894385814667,
      "learning_rate": 1.08813503614412e-06,
      "loss": 0.5835,
      "step": 148050
    },
    {
      "epoch": 2.671558259410754,
      "grad_norm": 1.0396184921264648,
      "learning_rate": 1.0822563729370822e-06,
      "loss": 0.5777,
      "step": 148100
    },
    {
      "epoch": 2.6724602125903645,
      "grad_norm": 0.26219621300697327,
      "learning_rate": 1.0763930381858671e-06,
      "loss": 0.5977,
      "step": 148150
    },
    {
      "epoch": 2.6733621657699747,
      "grad_norm": 0.2851204574108124,
      "learning_rate": 1.0705450383481247e-06,
      "loss": 0.5765,
      "step": 148200
    },
    {
      "epoch": 2.6742641189495853,
      "grad_norm": 0.46870720386505127,
      "learning_rate": 1.0647123798646092e-06,
      "loss": 0.5922,
      "step": 148250
    },
    {
      "epoch": 2.675166072129196,
      "grad_norm": 0.3833552300930023,
      "learning_rate": 1.0588950691591803e-06,
      "loss": 0.6057,
      "step": 148300
    },
    {
      "epoch": 2.6760680253088065,
      "grad_norm": 0.47362345457077026,
      "learning_rate": 1.0530931126387966e-06,
      "loss": 0.5847,
      "step": 148350
    },
    {
      "epoch": 2.6769699784884167,
      "grad_norm": 0.3822884261608124,
      "learning_rate": 1.0473065166935114e-06,
      "loss": 0.5655,
      "step": 148400
    },
    {
      "epoch": 2.677871931668027,
      "grad_norm": 0.39940527081489563,
      "learning_rate": 1.0415352876964418e-06,
      "loss": 0.6208,
      "step": 148450
    },
    {
      "epoch": 2.6787738848476375,
      "grad_norm": 0.6044803261756897,
      "learning_rate": 1.0357794320038e-06,
      "loss": 0.565,
      "step": 148500
    },
    {
      "epoch": 2.679675838027248,
      "grad_norm": 0.37890875339508057,
      "learning_rate": 1.03003895595486e-06,
      "loss": 0.5704,
      "step": 148550
    },
    {
      "epoch": 2.6805777912068587,
      "grad_norm": 0.3572554588317871,
      "learning_rate": 1.0243138658719547e-06,
      "loss": 0.5438,
      "step": 148600
    },
    {
      "epoch": 2.681479744386469,
      "grad_norm": 0.21678844094276428,
      "learning_rate": 1.0186041680604703e-06,
      "loss": 0.591,
      "step": 148650
    },
    {
      "epoch": 2.6823816975660795,
      "grad_norm": 0.408159464597702,
      "learning_rate": 1.0129098688088494e-06,
      "loss": 0.5862,
      "step": 148700
    },
    {
      "epoch": 2.6832836507456896,
      "grad_norm": 0.4561843276023865,
      "learning_rate": 1.007230974388566e-06,
      "loss": 0.6833,
      "step": 148750
    },
    {
      "epoch": 2.6841856039253003,
      "grad_norm": 0.4983280599117279,
      "learning_rate": 1.00156749105413e-06,
      "loss": 0.5772,
      "step": 148800
    },
    {
      "epoch": 2.685087557104911,
      "grad_norm": 0.4758172333240509,
      "learning_rate": 9.959194250430836e-07,
      "loss": 0.5456,
      "step": 148850
    },
    {
      "epoch": 2.685989510284521,
      "grad_norm": 0.4056675434112549,
      "learning_rate": 9.902867825759792e-07,
      "loss": 0.6462,
      "step": 148900
    },
    {
      "epoch": 2.6868914634641317,
      "grad_norm": 0.4742913842201233,
      "learning_rate": 9.84669569856393e-07,
      "loss": 0.5826,
      "step": 148950
    },
    {
      "epoch": 2.687793416643742,
      "grad_norm": 0.41205140948295593,
      "learning_rate": 9.790677930709007e-07,
      "loss": 0.5959,
      "step": 149000
    },
    {
      "epoch": 2.6886953698233524,
      "grad_norm": 0.3487699627876282,
      "learning_rate": 9.73481458389076e-07,
      "loss": 0.6395,
      "step": 149050
    },
    {
      "epoch": 2.689597323002963,
      "grad_norm": 0.5172886848449707,
      "learning_rate": 9.679105719634924e-07,
      "loss": 0.5838,
      "step": 149100
    },
    {
      "epoch": 2.6904992761825732,
      "grad_norm": 0.6740286946296692,
      "learning_rate": 9.623551399297047e-07,
      "loss": 0.6443,
      "step": 149150
    },
    {
      "epoch": 2.691401229362184,
      "grad_norm": 0.5978158116340637,
      "learning_rate": 9.568151684062443e-07,
      "loss": 0.5563,
      "step": 149200
    },
    {
      "epoch": 2.692303182541794,
      "grad_norm": 0.5098536014556885,
      "learning_rate": 9.512906634946183e-07,
      "loss": 0.6009,
      "step": 149250
    },
    {
      "epoch": 2.6932051357214046,
      "grad_norm": 0.5830556750297546,
      "learning_rate": 9.457816312793082e-07,
      "loss": 0.6068,
      "step": 149300
    },
    {
      "epoch": 2.6941070889010152,
      "grad_norm": 0.4746991693973541,
      "learning_rate": 9.402880778277323e-07,
      "loss": 0.6202,
      "step": 149350
    },
    {
      "epoch": 2.695009042080626,
      "grad_norm": 0.43432706594467163,
      "learning_rate": 9.348100091902806e-07,
      "loss": 0.5992,
      "step": 149400
    },
    {
      "epoch": 2.695910995260236,
      "grad_norm": 0.43129515647888184,
      "learning_rate": 9.29347431400287e-07,
      "loss": 0.6152,
      "step": 149450
    },
    {
      "epoch": 2.6968129484398466,
      "grad_norm": 0.5998139381408691,
      "learning_rate": 9.23900350474014e-07,
      "loss": 0.6614,
      "step": 149500
    },
    {
      "epoch": 2.697714901619457,
      "grad_norm": 0.4390670657157898,
      "learning_rate": 9.18468772410661e-07,
      "loss": 0.5432,
      "step": 149550
    },
    {
      "epoch": 2.6986168547990674,
      "grad_norm": 0.41670259833335876,
      "learning_rate": 9.130527031923613e-07,
      "loss": 0.5948,
      "step": 149600
    },
    {
      "epoch": 2.699518807978678,
      "grad_norm": 0.42154085636138916,
      "learning_rate": 9.076521487841583e-07,
      "loss": 0.595,
      "step": 149650
    },
    {
      "epoch": 2.700420761158288,
      "grad_norm": 0.41615843772888184,
      "learning_rate": 9.022671151340057e-07,
      "loss": 0.5922,
      "step": 149700
    },
    {
      "epoch": 2.701322714337899,
      "grad_norm": 0.3399973213672638,
      "learning_rate": 8.968976081727725e-07,
      "loss": 0.6099,
      "step": 149750
    },
    {
      "epoch": 2.702224667517509,
      "grad_norm": 0.47067123651504517,
      "learning_rate": 8.915436338142202e-07,
      "loss": 0.5935,
      "step": 149800
    },
    {
      "epoch": 2.7031266206971196,
      "grad_norm": 0.31939706206321716,
      "learning_rate": 8.8620519795501e-07,
      "loss": 0.5862,
      "step": 149850
    },
    {
      "epoch": 2.7040285738767302,
      "grad_norm": 0.3378719091415405,
      "learning_rate": 8.808823064746807e-07,
      "loss": 0.6008,
      "step": 149900
    },
    {
      "epoch": 2.7049305270563404,
      "grad_norm": 0.6005184054374695,
      "learning_rate": 8.755749652356577e-07,
      "loss": 0.6645,
      "step": 149950
    },
    {
      "epoch": 2.705832480235951,
      "grad_norm": 0.2527937591075897,
      "learning_rate": 8.702831800832384e-07,
      "loss": 0.614,
      "step": 150000
    },
    {
      "epoch": 2.705832480235951,
      "eval_loss": 0.7631095051765442,
      "eval_runtime": 121.8957,
      "eval_samples_per_second": 82.037,
      "eval_steps_per_second": 20.509,
      "step": 150000
    },
    {
      "epoch": 2.706734433415561,
      "grad_norm": 0.5161011815071106,
      "learning_rate": 8.650069568455859e-07,
      "loss": 0.5993,
      "step": 150050
    },
    {
      "epoch": 2.707636386595172,
      "grad_norm": 0.43472012877464294,
      "learning_rate": 8.597463013337237e-07,
      "loss": 0.6257,
      "step": 150100
    },
    {
      "epoch": 2.7085383397747824,
      "grad_norm": 0.41224443912506104,
      "learning_rate": 8.545012193415335e-07,
      "loss": 0.6179,
      "step": 150150
    },
    {
      "epoch": 2.7094402929543926,
      "grad_norm": 0.26602640748023987,
      "learning_rate": 8.492717166457447e-07,
      "loss": 0.5873,
      "step": 150200
    },
    {
      "epoch": 2.710342246134003,
      "grad_norm": 0.36515966057777405,
      "learning_rate": 8.440577990059217e-07,
      "loss": 0.5931,
      "step": 150250
    },
    {
      "epoch": 2.7112441993136134,
      "grad_norm": 0.36738520860671997,
      "learning_rate": 8.388594721644693e-07,
      "loss": 0.6237,
      "step": 150300
    },
    {
      "epoch": 2.712146152493224,
      "grad_norm": 0.5061389803886414,
      "learning_rate": 8.336767418466213e-07,
      "loss": 0.589,
      "step": 150350
    },
    {
      "epoch": 2.7130481056728346,
      "grad_norm": 0.5082541108131409,
      "learning_rate": 8.285096137604348e-07,
      "loss": 0.5851,
      "step": 150400
    },
    {
      "epoch": 2.7139500588524452,
      "grad_norm": 0.3879912197589874,
      "learning_rate": 8.233580935967761e-07,
      "loss": 0.5593,
      "step": 150450
    },
    {
      "epoch": 2.7148520120320554,
      "grad_norm": 0.7487857937812805,
      "learning_rate": 8.182221870293316e-07,
      "loss": 0.6047,
      "step": 150500
    },
    {
      "epoch": 2.715753965211666,
      "grad_norm": 0.2544458508491516,
      "learning_rate": 8.1310189971459e-07,
      "loss": 0.5767,
      "step": 150550
    },
    {
      "epoch": 2.716655918391276,
      "grad_norm": 0.24167513847351074,
      "learning_rate": 8.079972372918271e-07,
      "loss": 0.5518,
      "step": 150600
    },
    {
      "epoch": 2.717557871570887,
      "grad_norm": 0.22050918638706207,
      "learning_rate": 8.029082053831188e-07,
      "loss": 0.5981,
      "step": 150650
    },
    {
      "epoch": 2.7184598247504974,
      "grad_norm": 0.6379926204681396,
      "learning_rate": 7.978348095933302e-07,
      "loss": 0.5877,
      "step": 150700
    },
    {
      "epoch": 2.7193617779301076,
      "grad_norm": 0.3219042122364044,
      "learning_rate": 7.927770555100983e-07,
      "loss": 0.5708,
      "step": 150750
    },
    {
      "epoch": 2.720263731109718,
      "grad_norm": 0.5516225099563599,
      "learning_rate": 7.877349487038288e-07,
      "loss": 0.6176,
      "step": 150800
    },
    {
      "epoch": 2.7211656842893284,
      "grad_norm": 0.4269293546676636,
      "learning_rate": 7.827084947277064e-07,
      "loss": 0.6267,
      "step": 150850
    },
    {
      "epoch": 2.722067637468939,
      "grad_norm": 0.42968758940696716,
      "learning_rate": 7.776976991176693e-07,
      "loss": 0.5543,
      "step": 150900
    },
    {
      "epoch": 2.7229695906485496,
      "grad_norm": 0.451336532831192,
      "learning_rate": 7.727025673924066e-07,
      "loss": 0.5504,
      "step": 150950
    },
    {
      "epoch": 2.7238715438281598,
      "grad_norm": 0.2989214062690735,
      "learning_rate": 7.67723105053364e-07,
      "loss": 0.579,
      "step": 151000
    },
    {
      "epoch": 2.7247734970077704,
      "grad_norm": 0.3457917869091034,
      "learning_rate": 7.627593175847231e-07,
      "loss": 0.6412,
      "step": 151050
    },
    {
      "epoch": 2.7256754501873806,
      "grad_norm": 0.37533509731292725,
      "learning_rate": 7.578112104534074e-07,
      "loss": 0.5869,
      "step": 151100
    },
    {
      "epoch": 2.726577403366991,
      "grad_norm": 0.3921019434928894,
      "learning_rate": 7.52878789109066e-07,
      "loss": 0.5359,
      "step": 151150
    },
    {
      "epoch": 2.727479356546602,
      "grad_norm": 0.6912386417388916,
      "learning_rate": 7.479620589840718e-07,
      "loss": 0.6158,
      "step": 151200
    },
    {
      "epoch": 2.728381309726212,
      "grad_norm": 0.27301856875419617,
      "learning_rate": 7.430610254935216e-07,
      "loss": 0.5942,
      "step": 151250
    },
    {
      "epoch": 2.7292832629058226,
      "grad_norm": 0.2422960102558136,
      "learning_rate": 7.381756940352208e-07,
      "loss": 0.6168,
      "step": 151300
    },
    {
      "epoch": 2.7301852160854327,
      "grad_norm": 0.3375106453895569,
      "learning_rate": 7.333060699896771e-07,
      "loss": 0.6065,
      "step": 151350
    },
    {
      "epoch": 2.7310871692650434,
      "grad_norm": 0.5503575801849365,
      "learning_rate": 7.284521587201071e-07,
      "loss": 0.6276,
      "step": 151400
    },
    {
      "epoch": 2.731989122444654,
      "grad_norm": 0.5214979648590088,
      "learning_rate": 7.236139655724211e-07,
      "loss": 0.5603,
      "step": 151450
    },
    {
      "epoch": 2.7328910756242646,
      "grad_norm": 0.5144887566566467,
      "learning_rate": 7.187914958752051e-07,
      "loss": 0.5435,
      "step": 151500
    },
    {
      "epoch": 2.7337930288038748,
      "grad_norm": 0.47152361273765564,
      "learning_rate": 7.139847549397438e-07,
      "loss": 0.598,
      "step": 151550
    },
    {
      "epoch": 2.7346949819834854,
      "grad_norm": 0.6152244806289673,
      "learning_rate": 7.091937480599958e-07,
      "loss": 0.6087,
      "step": 151600
    },
    {
      "epoch": 2.7355969351630955,
      "grad_norm": 0.4935590326786041,
      "learning_rate": 7.044184805125836e-07,
      "loss": 0.5455,
      "step": 151650
    },
    {
      "epoch": 2.736498888342706,
      "grad_norm": 0.3672466278076172,
      "learning_rate": 6.996589575567968e-07,
      "loss": 0.5337,
      "step": 151700
    },
    {
      "epoch": 2.7374008415223168,
      "grad_norm": 0.5290188193321228,
      "learning_rate": 6.949151844345924e-07,
      "loss": 0.5299,
      "step": 151750
    },
    {
      "epoch": 2.738302794701927,
      "grad_norm": 0.5047609806060791,
      "learning_rate": 6.901871663705745e-07,
      "loss": 0.6074,
      "step": 151800
    },
    {
      "epoch": 2.7392047478815376,
      "grad_norm": 0.2266191989183426,
      "learning_rate": 6.854749085719908e-07,
      "loss": 0.5567,
      "step": 151850
    },
    {
      "epoch": 2.7401067010611477,
      "grad_norm": 0.7459871172904968,
      "learning_rate": 6.807784162287433e-07,
      "loss": 0.6113,
      "step": 151900
    },
    {
      "epoch": 2.7410086542407583,
      "grad_norm": 0.6305160522460938,
      "learning_rate": 6.760976945133629e-07,
      "loss": 0.588,
      "step": 151950
    },
    {
      "epoch": 2.741910607420369,
      "grad_norm": 0.5841235518455505,
      "learning_rate": 6.714327485810106e-07,
      "loss": 0.5267,
      "step": 152000
    },
    {
      "epoch": 2.741910607420369,
      "eval_loss": 0.763450026512146,
      "eval_runtime": 121.2414,
      "eval_samples_per_second": 82.48,
      "eval_steps_per_second": 20.62,
      "step": 152000
    },
    {
      "epoch": 2.742812560599979,
      "grad_norm": 0.42334404587745667,
      "learning_rate": 6.667835835694753e-07,
      "loss": 0.5087,
      "step": 152050
    },
    {
      "epoch": 2.7437145137795897,
      "grad_norm": 0.4663621187210083,
      "learning_rate": 6.621502045991657e-07,
      "loss": 0.5701,
      "step": 152100
    },
    {
      "epoch": 2.7446164669592,
      "grad_norm": 0.34226253628730774,
      "learning_rate": 6.575326167731033e-07,
      "loss": 0.586,
      "step": 152150
    },
    {
      "epoch": 2.7455184201388105,
      "grad_norm": 0.3855193257331848,
      "learning_rate": 6.529308251769134e-07,
      "loss": 0.5763,
      "step": 152200
    },
    {
      "epoch": 2.746420373318421,
      "grad_norm": 0.5142358541488647,
      "learning_rate": 6.483448348788351e-07,
      "loss": 0.6227,
      "step": 152250
    },
    {
      "epoch": 2.7473223264980318,
      "grad_norm": 0.3268384337425232,
      "learning_rate": 6.437746509296932e-07,
      "loss": 0.5268,
      "step": 152300
    },
    {
      "epoch": 2.748224279677642,
      "grad_norm": 0.5800861120223999,
      "learning_rate": 6.392202783629114e-07,
      "loss": 0.6122,
      "step": 152350
    },
    {
      "epoch": 2.749126232857252,
      "grad_norm": 0.2953871786594391,
      "learning_rate": 6.346817221944956e-07,
      "loss": 0.5863,
      "step": 152400
    },
    {
      "epoch": 2.7500281860368627,
      "grad_norm": 0.41656139492988586,
      "learning_rate": 6.301589874230307e-07,
      "loss": 0.6367,
      "step": 152450
    },
    {
      "epoch": 2.7509301392164733,
      "grad_norm": 0.563866913318634,
      "learning_rate": 6.256520790296822e-07,
      "loss": 0.5937,
      "step": 152500
    },
    {
      "epoch": 2.751832092396084,
      "grad_norm": 0.2303651124238968,
      "learning_rate": 6.211610019781827e-07,
      "loss": 0.5437,
      "step": 152550
    },
    {
      "epoch": 2.752734045575694,
      "grad_norm": 0.3537217378616333,
      "learning_rate": 6.166857612148242e-07,
      "loss": 0.5567,
      "step": 152600
    },
    {
      "epoch": 2.7536359987553047,
      "grad_norm": 0.3829764723777771,
      "learning_rate": 6.12226361668462e-07,
      "loss": 0.5658,
      "step": 152650
    },
    {
      "epoch": 2.754537951934915,
      "grad_norm": 0.4299771189689636,
      "learning_rate": 6.077828082505077e-07,
      "loss": 0.5938,
      "step": 152700
    },
    {
      "epoch": 2.7554399051145255,
      "grad_norm": 0.6213420033454895,
      "learning_rate": 6.033551058549114e-07,
      "loss": 0.6069,
      "step": 152750
    },
    {
      "epoch": 2.756341858294136,
      "grad_norm": 0.6725137233734131,
      "learning_rate": 5.989432593581723e-07,
      "loss": 0.549,
      "step": 152800
    },
    {
      "epoch": 2.7572438114737463,
      "grad_norm": 0.5343939661979675,
      "learning_rate": 5.945472736193252e-07,
      "loss": 0.5896,
      "step": 152850
    },
    {
      "epoch": 2.758145764653357,
      "grad_norm": 0.6375294327735901,
      "learning_rate": 5.901671534799375e-07,
      "loss": 0.6309,
      "step": 152900
    },
    {
      "epoch": 2.759047717832967,
      "grad_norm": 0.8667322397232056,
      "learning_rate": 5.858029037640966e-07,
      "loss": 0.6405,
      "step": 152950
    },
    {
      "epoch": 2.7599496710125777,
      "grad_norm": 0.2947884500026703,
      "learning_rate": 5.814545292784196e-07,
      "loss": 0.6198,
      "step": 153000
    },
    {
      "epoch": 2.7608516241921883,
      "grad_norm": 0.37271714210510254,
      "learning_rate": 5.771220348120321e-07,
      "loss": 0.5573,
      "step": 153050
    },
    {
      "epoch": 2.7617535773717985,
      "grad_norm": 0.5218619704246521,
      "learning_rate": 5.728054251365772e-07,
      "loss": 0.5852,
      "step": 153100
    },
    {
      "epoch": 2.762655530551409,
      "grad_norm": 0.23048803210258484,
      "learning_rate": 5.685047050061986e-07,
      "loss": 0.5654,
      "step": 153150
    },
    {
      "epoch": 2.7635574837310193,
      "grad_norm": 0.4250175654888153,
      "learning_rate": 5.642198791575359e-07,
      "loss": 0.5468,
      "step": 153200
    },
    {
      "epoch": 2.76445943691063,
      "grad_norm": 0.7004806995391846,
      "learning_rate": 5.599509523097324e-07,
      "loss": 0.6434,
      "step": 153250
    },
    {
      "epoch": 2.7653613900902405,
      "grad_norm": 0.2253650575876236,
      "learning_rate": 5.556979291644171e-07,
      "loss": 0.5363,
      "step": 153300
    },
    {
      "epoch": 2.766263343269851,
      "grad_norm": 0.30338796973228455,
      "learning_rate": 5.514608144056965e-07,
      "loss": 0.5198,
      "step": 153350
    },
    {
      "epoch": 2.7671652964494613,
      "grad_norm": 0.5650489926338196,
      "learning_rate": 5.472396127001661e-07,
      "loss": 0.6022,
      "step": 153400
    },
    {
      "epoch": 2.768067249629072,
      "grad_norm": 0.40005001425743103,
      "learning_rate": 5.430343286968953e-07,
      "loss": 0.6051,
      "step": 153450
    },
    {
      "epoch": 2.768969202808682,
      "grad_norm": 0.4242197275161743,
      "learning_rate": 5.388449670274126e-07,
      "loss": 0.5306,
      "step": 153500
    },
    {
      "epoch": 2.7698711559882927,
      "grad_norm": 0.4528696835041046,
      "learning_rate": 5.346715323057172e-07,
      "loss": 0.578,
      "step": 153550
    },
    {
      "epoch": 2.7707731091679033,
      "grad_norm": 0.64620441198349,
      "learning_rate": 5.305140291282707e-07,
      "loss": 0.6358,
      "step": 153600
    },
    {
      "epoch": 2.7716750623475135,
      "grad_norm": 0.3706359267234802,
      "learning_rate": 5.263724620739785e-07,
      "loss": 0.6219,
      "step": 153650
    },
    {
      "epoch": 2.772577015527124,
      "grad_norm": 0.6579363346099854,
      "learning_rate": 5.22246835704202e-07,
      "loss": 0.566,
      "step": 153700
    },
    {
      "epoch": 2.7734789687067343,
      "grad_norm": 0.4141217768192291,
      "learning_rate": 5.181371545627416e-07,
      "loss": 0.5267,
      "step": 153750
    },
    {
      "epoch": 2.774380921886345,
      "grad_norm": 0.2601492404937744,
      "learning_rate": 5.14043423175845e-07,
      "loss": 0.6102,
      "step": 153800
    },
    {
      "epoch": 2.7752828750659555,
      "grad_norm": 0.44200852513313293,
      "learning_rate": 5.099656460521806e-07,
      "loss": 0.6219,
      "step": 153850
    },
    {
      "epoch": 2.7761848282455657,
      "grad_norm": 0.48663023114204407,
      "learning_rate": 5.059038276828526e-07,
      "loss": 0.5826,
      "step": 153900
    },
    {
      "epoch": 2.7770867814251763,
      "grad_norm": 0.36564841866493225,
      "learning_rate": 5.018579725413908e-07,
      "loss": 0.6164,
      "step": 153950
    },
    {
      "epoch": 2.7779887346047865,
      "grad_norm": 0.24930277466773987,
      "learning_rate": 4.978280850837408e-07,
      "loss": 0.618,
      "step": 154000
    },
    {
      "epoch": 2.7779887346047865,
      "eval_loss": 0.76336669921875,
      "eval_runtime": 121.4339,
      "eval_samples_per_second": 82.349,
      "eval_steps_per_second": 20.587,
      "step": 154000
    },
    {
      "epoch": 2.778890687784397,
      "grad_norm": 0.7167271971702576,
      "learning_rate": 4.93814169748259e-07,
      "loss": 0.6357,
      "step": 154050
    },
    {
      "epoch": 2.7797926409640077,
      "grad_norm": 0.38517382740974426,
      "learning_rate": 4.898162309557175e-07,
      "loss": 0.572,
      "step": 154100
    },
    {
      "epoch": 2.780694594143618,
      "grad_norm": 0.5858197212219238,
      "learning_rate": 4.858342731092874e-07,
      "loss": 0.5339,
      "step": 154150
    },
    {
      "epoch": 2.7815965473232285,
      "grad_norm": 0.38702040910720825,
      "learning_rate": 4.818683005945407e-07,
      "loss": 0.64,
      "step": 154200
    },
    {
      "epoch": 2.7824985005028386,
      "grad_norm": 0.846900999546051,
      "learning_rate": 4.779183177794433e-07,
      "loss": 0.5977,
      "step": 154250
    },
    {
      "epoch": 2.7834004536824493,
      "grad_norm": 0.7808001637458801,
      "learning_rate": 4.7398432901435196e-07,
      "loss": 0.5679,
      "step": 154300
    },
    {
      "epoch": 2.78430240686206,
      "grad_norm": 0.6760207414627075,
      "learning_rate": 4.700663386320075e-07,
      "loss": 0.569,
      "step": 154350
    },
    {
      "epoch": 2.7852043600416705,
      "grad_norm": 1.1248608827590942,
      "learning_rate": 4.6616435094752996e-07,
      "loss": 0.5834,
      "step": 154400
    },
    {
      "epoch": 2.7861063132212807,
      "grad_norm": 0.4871489703655243,
      "learning_rate": 4.6227837025841323e-07,
      "loss": 0.6055,
      "step": 154450
    },
    {
      "epoch": 2.7870082664008913,
      "grad_norm": 0.32082390785217285,
      "learning_rate": 4.5840840084452717e-07,
      "loss": 0.5927,
      "step": 154500
    },
    {
      "epoch": 2.7879102195805014,
      "grad_norm": 0.5073158144950867,
      "learning_rate": 4.545544469681023e-07,
      "loss": 0.5925,
      "step": 154550
    },
    {
      "epoch": 2.788812172760112,
      "grad_norm": 0.6314542889595032,
      "learning_rate": 4.5071651287373005e-07,
      "loss": 0.5776,
      "step": 154600
    },
    {
      "epoch": 2.7897141259397227,
      "grad_norm": 0.630329430103302,
      "learning_rate": 4.468946027883625e-07,
      "loss": 0.5984,
      "step": 154650
    },
    {
      "epoch": 2.790616079119333,
      "grad_norm": 0.26157474517822266,
      "learning_rate": 4.430887209213041e-07,
      "loss": 0.5773,
      "step": 154700
    },
    {
      "epoch": 2.7915180322989435,
      "grad_norm": 0.4544777274131775,
      "learning_rate": 4.3929887146419714e-07,
      "loss": 0.6149,
      "step": 154750
    },
    {
      "epoch": 2.7924199854785536,
      "grad_norm": 0.6423981189727783,
      "learning_rate": 4.355250585910375e-07,
      "loss": 0.6131,
      "step": 154800
    },
    {
      "epoch": 2.7933219386581642,
      "grad_norm": 0.5355026125907898,
      "learning_rate": 4.3176728645815235e-07,
      "loss": 0.5497,
      "step": 154850
    },
    {
      "epoch": 2.794223891837775,
      "grad_norm": 0.6608246564865112,
      "learning_rate": 4.280255592042059e-07,
      "loss": 0.5893,
      "step": 154900
    },
    {
      "epoch": 2.795125845017385,
      "grad_norm": 0.5267322063446045,
      "learning_rate": 4.242998809501869e-07,
      "loss": 0.5659,
      "step": 154950
    },
    {
      "epoch": 2.7960277981969957,
      "grad_norm": 0.39500489830970764,
      "learning_rate": 4.2059025579941456e-07,
      "loss": 0.562,
      "step": 155000
    },
    {
      "epoch": 2.796929751376606,
      "grad_norm": 0.37431904673576355,
      "learning_rate": 4.1689668783752075e-07,
      "loss": 0.5771,
      "step": 155050
    },
    {
      "epoch": 2.7978317045562164,
      "grad_norm": 0.3459465801715851,
      "learning_rate": 4.132191811324565e-07,
      "loss": 0.6255,
      "step": 155100
    },
    {
      "epoch": 2.798733657735827,
      "grad_norm": 0.39795953035354614,
      "learning_rate": 4.0955773973448516e-07,
      "loss": 0.6564,
      "step": 155150
    },
    {
      "epoch": 2.7996356109154372,
      "grad_norm": 0.4702698886394501,
      "learning_rate": 4.0591236767617277e-07,
      "loss": 0.5652,
      "step": 155200
    },
    {
      "epoch": 2.800537564095048,
      "grad_norm": 0.2416340559720993,
      "learning_rate": 4.022830689723894e-07,
      "loss": 0.5309,
      "step": 155250
    },
    {
      "epoch": 2.801439517274658,
      "grad_norm": 0.5011290311813354,
      "learning_rate": 3.986698476203027e-07,
      "loss": 0.5684,
      "step": 155300
    },
    {
      "epoch": 2.8023414704542686,
      "grad_norm": 0.6053065657615662,
      "learning_rate": 3.9507270759937097e-07,
      "loss": 0.6349,
      "step": 155350
    },
    {
      "epoch": 2.8032434236338792,
      "grad_norm": 0.2547326982021332,
      "learning_rate": 3.9149165287134526e-07,
      "loss": 0.5729,
      "step": 155400
    },
    {
      "epoch": 2.80414537681349,
      "grad_norm": 0.21205030381679535,
      "learning_rate": 3.879266873802556e-07,
      "loss": 0.5891,
      "step": 155450
    },
    {
      "epoch": 2.8050473299931,
      "grad_norm": 0.5664137601852417,
      "learning_rate": 3.843778150524163e-07,
      "loss": 0.6246,
      "step": 155500
    },
    {
      "epoch": 2.8059492831727106,
      "grad_norm": 0.35089820623397827,
      "learning_rate": 3.808450397964158e-07,
      "loss": 0.619,
      "step": 155550
    },
    {
      "epoch": 2.806851236352321,
      "grad_norm": 0.5351978540420532,
      "learning_rate": 3.7732836550311513e-07,
      "loss": 0.6352,
      "step": 155600
    },
    {
      "epoch": 2.8077531895319314,
      "grad_norm": 0.6236354112625122,
      "learning_rate": 3.738277960456393e-07,
      "loss": 0.608,
      "step": 155650
    },
    {
      "epoch": 2.808655142711542,
      "grad_norm": 0.5574516654014587,
      "learning_rate": 3.703433352793778e-07,
      "loss": 0.5375,
      "step": 155700
    },
    {
      "epoch": 2.809557095891152,
      "grad_norm": 0.4112381339073181,
      "learning_rate": 3.6687498704198243e-07,
      "loss": 0.5622,
      "step": 155750
    },
    {
      "epoch": 2.810459049070763,
      "grad_norm": 0.3570616841316223,
      "learning_rate": 3.6342275515335086e-07,
      "loss": 0.5591,
      "step": 155800
    },
    {
      "epoch": 2.811361002250373,
      "grad_norm": 0.5595508217811584,
      "learning_rate": 3.599866434156368e-07,
      "loss": 0.6105,
      "step": 155850
    },
    {
      "epoch": 2.8122629554299836,
      "grad_norm": 0.6844111680984497,
      "learning_rate": 3.5656665561323964e-07,
      "loss": 0.5407,
      "step": 155900
    },
    {
      "epoch": 2.8131649086095942,
      "grad_norm": 0.37246477603912354,
      "learning_rate": 3.531627955128014e-07,
      "loss": 0.5438,
      "step": 155950
    },
    {
      "epoch": 2.8140668617892044,
      "grad_norm": 0.48227521777153015,
      "learning_rate": 3.497750668631949e-07,
      "loss": 0.5237,
      "step": 156000
    },
    {
      "epoch": 2.8140668617892044,
      "eval_loss": 0.7635586857795715,
      "eval_runtime": 121.7636,
      "eval_samples_per_second": 82.126,
      "eval_steps_per_second": 20.532,
      "step": 156000
    },
    {
      "epoch": 2.814968814968815,
      "grad_norm": 0.4943109154701233,
      "learning_rate": 3.4640347339553403e-07,
      "loss": 0.5409,
      "step": 156050
    },
    {
      "epoch": 2.815870768148425,
      "grad_norm": 0.5267258286476135,
      "learning_rate": 3.430480188231616e-07,
      "loss": 0.6253,
      "step": 156100
    },
    {
      "epoch": 2.816772721328036,
      "grad_norm": 0.5434197783470154,
      "learning_rate": 3.3970870684164156e-07,
      "loss": 0.5882,
      "step": 156150
    },
    {
      "epoch": 2.8176746745076464,
      "grad_norm": 0.6326027512550354,
      "learning_rate": 3.363855411287586e-07,
      "loss": 0.5924,
      "step": 156200
    },
    {
      "epoch": 2.818576627687257,
      "grad_norm": 0.5712598562240601,
      "learning_rate": 3.3307852534452013e-07,
      "loss": 0.5412,
      "step": 156250
    },
    {
      "epoch": 2.819478580866867,
      "grad_norm": 0.27631422877311707,
      "learning_rate": 3.2978766313114085e-07,
      "loss": 0.5833,
      "step": 156300
    },
    {
      "epoch": 2.8203805340464774,
      "grad_norm": 0.3159058392047882,
      "learning_rate": 3.2651295811304826e-07,
      "loss": 0.5942,
      "step": 156350
    },
    {
      "epoch": 2.821282487226088,
      "grad_norm": 0.38315656781196594,
      "learning_rate": 3.232544138968757e-07,
      "loss": 0.5948,
      "step": 156400
    },
    {
      "epoch": 2.8221844404056986,
      "grad_norm": 0.48592668771743774,
      "learning_rate": 3.2001203407145076e-07,
      "loss": 0.5828,
      "step": 156450
    },
    {
      "epoch": 2.823086393585309,
      "grad_norm": 0.46097201108932495,
      "learning_rate": 3.167858222078085e-07,
      "loss": 0.5664,
      "step": 156500
    },
    {
      "epoch": 2.8239883467649194,
      "grad_norm": 0.43528199195861816,
      "learning_rate": 3.1357578185916836e-07,
      "loss": 0.6032,
      "step": 156550
    },
    {
      "epoch": 2.82489029994453,
      "grad_norm": 0.4031277000904083,
      "learning_rate": 3.1038191656094394e-07,
      "loss": 0.5572,
      "step": 156600
    },
    {
      "epoch": 2.82579225312414,
      "grad_norm": 0.26338911056518555,
      "learning_rate": 3.0720422983073316e-07,
      "loss": 0.5813,
      "step": 156650
    },
    {
      "epoch": 2.826694206303751,
      "grad_norm": 0.34392112493515015,
      "learning_rate": 3.0404272516831476e-07,
      "loss": 0.6122,
      "step": 156700
    },
    {
      "epoch": 2.8275961594833614,
      "grad_norm": 0.25973647832870483,
      "learning_rate": 3.008974060556452e-07,
      "loss": 0.5474,
      "step": 156750
    },
    {
      "epoch": 2.8284981126629716,
      "grad_norm": 0.4005388021469116,
      "learning_rate": 2.977682759568551e-07,
      "loss": 0.5706,
      "step": 156800
    },
    {
      "epoch": 2.829400065842582,
      "grad_norm": 0.3489411473274231,
      "learning_rate": 2.9465533831824933e-07,
      "loss": 0.5808,
      "step": 156850
    },
    {
      "epoch": 2.8303020190221924,
      "grad_norm": 0.39033618569374084,
      "learning_rate": 2.9155859656828874e-07,
      "loss": 0.5661,
      "step": 156900
    },
    {
      "epoch": 2.831203972201803,
      "grad_norm": 0.34531867504119873,
      "learning_rate": 2.8847805411760685e-07,
      "loss": 0.5665,
      "step": 156950
    },
    {
      "epoch": 2.8321059253814136,
      "grad_norm": 0.45970863103866577,
      "learning_rate": 2.85413714358993e-07,
      "loss": 0.5599,
      "step": 157000
    },
    {
      "epoch": 2.8330078785610238,
      "grad_norm": 0.4472312033176422,
      "learning_rate": 2.823655806673875e-07,
      "loss": 0.5434,
      "step": 157050
    },
    {
      "epoch": 2.8339098317406344,
      "grad_norm": 0.4394817054271698,
      "learning_rate": 2.793336563998866e-07,
      "loss": 0.5261,
      "step": 157100
    },
    {
      "epoch": 2.8348117849202445,
      "grad_norm": 0.3869335949420929,
      "learning_rate": 2.763179448957309e-07,
      "loss": 0.5953,
      "step": 157150
    },
    {
      "epoch": 2.835713738099855,
      "grad_norm": 0.37042734026908875,
      "learning_rate": 2.733184494763119e-07,
      "loss": 0.5815,
      "step": 157200
    },
    {
      "epoch": 2.836615691279466,
      "grad_norm": 0.47556981444358826,
      "learning_rate": 2.7033517344515037e-07,
      "loss": 0.5508,
      "step": 157250
    },
    {
      "epoch": 2.8375176444590764,
      "grad_norm": 0.4146934449672699,
      "learning_rate": 2.673681200879097e-07,
      "loss": 0.5802,
      "step": 157300
    },
    {
      "epoch": 2.8384195976386866,
      "grad_norm": 0.48371100425720215,
      "learning_rate": 2.6441729267238757e-07,
      "loss": 0.5742,
      "step": 157350
    },
    {
      "epoch": 2.839321550818297,
      "grad_norm": 0.5862330198287964,
      "learning_rate": 2.6148269444850936e-07,
      "loss": 0.5972,
      "step": 157400
    },
    {
      "epoch": 2.8402235039979074,
      "grad_norm": 0.3056010901927948,
      "learning_rate": 2.5856432864832636e-07,
      "loss": 0.6031,
      "step": 157450
    },
    {
      "epoch": 2.841125457177518,
      "grad_norm": 0.29706740379333496,
      "learning_rate": 2.556621984860108e-07,
      "loss": 0.5672,
      "step": 157500
    },
    {
      "epoch": 2.8420274103571286,
      "grad_norm": 0.22612763941287994,
      "learning_rate": 2.527763071578543e-07,
      "loss": 0.5762,
      "step": 157550
    },
    {
      "epoch": 2.8429293635367388,
      "grad_norm": 0.38601577281951904,
      "learning_rate": 2.4990665784226594e-07,
      "loss": 0.5766,
      "step": 157600
    },
    {
      "epoch": 2.8438313167163494,
      "grad_norm": 0.24749279022216797,
      "learning_rate": 2.470532536997644e-07,
      "loss": 0.5719,
      "step": 157650
    },
    {
      "epoch": 2.8447332698959595,
      "grad_norm": 0.6499431729316711,
      "learning_rate": 2.4421609787297574e-07,
      "loss": 0.5994,
      "step": 157700
    },
    {
      "epoch": 2.84563522307557,
      "grad_norm": 0.2631090581417084,
      "learning_rate": 2.413951934866321e-07,
      "loss": 0.5702,
      "step": 157750
    },
    {
      "epoch": 2.8465371762551808,
      "grad_norm": 0.4450616240501404,
      "learning_rate": 2.3859054364756826e-07,
      "loss": 0.6184,
      "step": 157800
    },
    {
      "epoch": 2.847439129434791,
      "grad_norm": 0.23415127396583557,
      "learning_rate": 2.358021514447134e-07,
      "loss": 0.5983,
      "step": 157850
    },
    {
      "epoch": 2.8483410826144016,
      "grad_norm": 0.616943895816803,
      "learning_rate": 2.3303001994909423e-07,
      "loss": 0.579,
      "step": 157900
    },
    {
      "epoch": 2.8492430357940117,
      "grad_norm": 0.5633931756019592,
      "learning_rate": 2.3027415221382686e-07,
      "loss": 0.5727,
      "step": 157950
    },
    {
      "epoch": 2.8501449889736223,
      "grad_norm": 0.45848631858825684,
      "learning_rate": 2.2753455127411505e-07,
      "loss": 0.6488,
      "step": 158000
    },
    {
      "epoch": 2.8501449889736223,
      "eval_loss": 0.7633211016654968,
      "eval_runtime": 123.0389,
      "eval_samples_per_second": 81.275,
      "eval_steps_per_second": 20.319,
      "step": 158000
    },
    {
      "epoch": 2.851046942153233,
      "grad_norm": 0.5587520003318787,
      "learning_rate": 2.2481122014724854e-07,
      "loss": 0.5729,
      "step": 158050
    },
    {
      "epoch": 2.851948895332843,
      "grad_norm": 0.514302670955658,
      "learning_rate": 2.2210416183259807e-07,
      "loss": 0.6396,
      "step": 158100
    },
    {
      "epoch": 2.8528508485124537,
      "grad_norm": 0.5787414908409119,
      "learning_rate": 2.1941337931161043e-07,
      "loss": 0.5439,
      "step": 158150
    },
    {
      "epoch": 2.853752801692064,
      "grad_norm": 0.5758398175239563,
      "learning_rate": 2.167388755478067e-07,
      "loss": 0.5962,
      "step": 158200
    },
    {
      "epoch": 2.8546547548716745,
      "grad_norm": 0.4128352701663971,
      "learning_rate": 2.140806534867823e-07,
      "loss": 0.5811,
      "step": 158250
    },
    {
      "epoch": 2.855556708051285,
      "grad_norm": 0.45616504549980164,
      "learning_rate": 2.1143871605619703e-07,
      "loss": 0.6485,
      "step": 158300
    },
    {
      "epoch": 2.8564586612308958,
      "grad_norm": 0.26135125756263733,
      "learning_rate": 2.0881306616577666e-07,
      "loss": 0.5916,
      "step": 158350
    },
    {
      "epoch": 2.857360614410506,
      "grad_norm": 0.27624356746673584,
      "learning_rate": 2.062037067073097e-07,
      "loss": 0.5423,
      "step": 158400
    },
    {
      "epoch": 2.8582625675901165,
      "grad_norm": 0.5334121584892273,
      "learning_rate": 2.0361064055464062e-07,
      "loss": 0.6434,
      "step": 158450
    },
    {
      "epoch": 2.8591645207697267,
      "grad_norm": 0.5960031151771545,
      "learning_rate": 2.0103387056367327e-07,
      "loss": 0.6172,
      "step": 158500
    },
    {
      "epoch": 2.8600664739493373,
      "grad_norm": 0.47055697441101074,
      "learning_rate": 1.9847339957235922e-07,
      "loss": 0.6218,
      "step": 158550
    },
    {
      "epoch": 2.860968427128948,
      "grad_norm": 0.3093930184841156,
      "learning_rate": 1.95929230400701e-07,
      "loss": 0.6177,
      "step": 158600
    },
    {
      "epoch": 2.861870380308558,
      "grad_norm": 0.34687361121177673,
      "learning_rate": 1.9340136585074552e-07,
      "loss": 0.6391,
      "step": 158650
    },
    {
      "epoch": 2.8627723334881687,
      "grad_norm": 0.6192762851715088,
      "learning_rate": 1.9088980870658411e-07,
      "loss": 0.5477,
      "step": 158700
    },
    {
      "epoch": 2.863674286667779,
      "grad_norm": 0.5922924876213074,
      "learning_rate": 1.8839456173434743e-07,
      "loss": 0.5641,
      "step": 158750
    },
    {
      "epoch": 2.8645762398473895,
      "grad_norm": 0.44406530261039734,
      "learning_rate": 1.859156276822005e-07,
      "loss": 0.6358,
      "step": 158800
    },
    {
      "epoch": 2.865478193027,
      "grad_norm": 0.3991154432296753,
      "learning_rate": 1.8345300928034935e-07,
      "loss": 0.6246,
      "step": 158850
    },
    {
      "epoch": 2.8663801462066103,
      "grad_norm": 0.4427239000797272,
      "learning_rate": 1.810067092410178e-07,
      "loss": 0.5698,
      "step": 158900
    },
    {
      "epoch": 2.867282099386221,
      "grad_norm": 0.44422367215156555,
      "learning_rate": 1.7857673025846734e-07,
      "loss": 0.6751,
      "step": 158950
    },
    {
      "epoch": 2.868184052565831,
      "grad_norm": 0.559597909450531,
      "learning_rate": 1.7616307500898376e-07,
      "loss": 0.5512,
      "step": 159000
    },
    {
      "epoch": 2.8690860057454417,
      "grad_norm": 0.47394859790802,
      "learning_rate": 1.737657461508707e-07,
      "loss": 0.6059,
      "step": 159050
    },
    {
      "epoch": 2.8699879589250523,
      "grad_norm": 0.380755752325058,
      "learning_rate": 1.713847463244511e-07,
      "loss": 0.5982,
      "step": 159100
    },
    {
      "epoch": 2.8708899121046625,
      "grad_norm": 0.4609614610671997,
      "learning_rate": 1.69020078152064e-07,
      "loss": 0.6409,
      "step": 159150
    },
    {
      "epoch": 2.871791865284273,
      "grad_norm": 0.3522436022758484,
      "learning_rate": 1.6667174423806607e-07,
      "loss": 0.6297,
      "step": 159200
    },
    {
      "epoch": 2.8726938184638833,
      "grad_norm": 0.2701968848705292,
      "learning_rate": 1.643397471688135e-07,
      "loss": 0.5758,
      "step": 159250
    },
    {
      "epoch": 2.873595771643494,
      "grad_norm": 0.48428502678871155,
      "learning_rate": 1.620240895126801e-07,
      "loss": 0.6456,
      "step": 159300
    },
    {
      "epoch": 2.8744977248231045,
      "grad_norm": 0.9241882562637329,
      "learning_rate": 1.597247738200408e-07,
      "loss": 0.56,
      "step": 159350
    },
    {
      "epoch": 2.875399678002715,
      "grad_norm": 0.24748089909553528,
      "learning_rate": 1.5744180262327158e-07,
      "loss": 0.5652,
      "step": 159400
    },
    {
      "epoch": 2.8763016311823253,
      "grad_norm": 0.3463568389415741,
      "learning_rate": 1.5517517843674278e-07,
      "loss": 0.5725,
      "step": 159450
    },
    {
      "epoch": 2.877203584361936,
      "grad_norm": 0.3639220893383026,
      "learning_rate": 1.529249037568309e-07,
      "loss": 0.554,
      "step": 159500
    },
    {
      "epoch": 2.878105537541546,
      "grad_norm": 0.2357233166694641,
      "learning_rate": 1.5069098106189838e-07,
      "loss": 0.5945,
      "step": 159550
    },
    {
      "epoch": 2.8790074907211567,
      "grad_norm": 0.5073149800300598,
      "learning_rate": 1.484734128122972e-07,
      "loss": 0.6125,
      "step": 159600
    },
    {
      "epoch": 2.8799094439007673,
      "grad_norm": 0.41618743538856506,
      "learning_rate": 1.46272201450372e-07,
      "loss": 0.5463,
      "step": 159650
    },
    {
      "epoch": 2.8808113970803775,
      "grad_norm": 0.5120654702186584,
      "learning_rate": 1.4408734940045188e-07,
      "loss": 0.6059,
      "step": 159700
    },
    {
      "epoch": 2.881713350259988,
      "grad_norm": 0.7317198514938354,
      "learning_rate": 1.4191885906884362e-07,
      "loss": 0.5961,
      "step": 159750
    },
    {
      "epoch": 2.8826153034395983,
      "grad_norm": 0.3516511619091034,
      "learning_rate": 1.3976673284384013e-07,
      "loss": 0.5776,
      "step": 159800
    },
    {
      "epoch": 2.883517256619209,
      "grad_norm": 0.325512558221817,
      "learning_rate": 1.3763097309570705e-07,
      "loss": 0.6224,
      "step": 159850
    },
    {
      "epoch": 2.8844192097988195,
      "grad_norm": 0.3972969651222229,
      "learning_rate": 1.3551158217668614e-07,
      "loss": 0.5436,
      "step": 159900
    },
    {
      "epoch": 2.8853211629784297,
      "grad_norm": 0.3285348117351532,
      "learning_rate": 1.3340856242099353e-07,
      "loss": 0.6476,
      "step": 159950
    },
    {
      "epoch": 2.8862231161580403,
      "grad_norm": 0.522241473197937,
      "learning_rate": 1.3132191614480981e-07,
      "loss": 0.6064,
      "step": 160000
    },
    {
      "epoch": 2.8862231161580403,
      "eval_loss": 0.7631866931915283,
      "eval_runtime": 123.1505,
      "eval_samples_per_second": 81.201,
      "eval_steps_per_second": 20.3,
      "step": 160000
    },
    {
      "epoch": 2.8871250693376505,
      "grad_norm": 0.6393049359321594,
      "learning_rate": 1.29251645646285e-07,
      "loss": 0.6112,
      "step": 160050
    },
    {
      "epoch": 2.888027022517261,
      "grad_norm": 0.41615426540374756,
      "learning_rate": 1.2719775320553684e-07,
      "loss": 0.552,
      "step": 160100
    },
    {
      "epoch": 2.8889289756968717,
      "grad_norm": 0.3748292922973633,
      "learning_rate": 1.251602410846392e-07,
      "loss": 0.5766,
      "step": 160150
    },
    {
      "epoch": 2.8898309288764823,
      "grad_norm": 0.32058265805244446,
      "learning_rate": 1.2313911152762702e-07,
      "loss": 0.6159,
      "step": 160200
    },
    {
      "epoch": 2.8907328820560925,
      "grad_norm": 0.3465631306171417,
      "learning_rate": 1.211343667604964e-07,
      "loss": 0.5385,
      "step": 160250
    },
    {
      "epoch": 2.8916348352357026,
      "grad_norm": 0.4213442802429199,
      "learning_rate": 1.191460089911911e-07,
      "loss": 0.5991,
      "step": 160300
    },
    {
      "epoch": 2.8925367884153133,
      "grad_norm": 0.4677978754043579,
      "learning_rate": 1.171740404096111e-07,
      "loss": 0.6045,
      "step": 160350
    },
    {
      "epoch": 2.893438741594924,
      "grad_norm": 0.39200544357299805,
      "learning_rate": 1.1521846318760743e-07,
      "loss": 0.6115,
      "step": 160400
    },
    {
      "epoch": 2.8943406947745345,
      "grad_norm": 0.6558505892753601,
      "learning_rate": 1.1327927947897221e-07,
      "loss": 0.5523,
      "step": 160450
    },
    {
      "epoch": 2.8952426479541447,
      "grad_norm": 0.4966178238391876,
      "learning_rate": 1.113564914194487e-07,
      "loss": 0.5678,
      "step": 160500
    },
    {
      "epoch": 2.8961446011337553,
      "grad_norm": 0.3322864770889282,
      "learning_rate": 1.0945010112671961e-07,
      "loss": 0.5446,
      "step": 160550
    },
    {
      "epoch": 2.8970465543133654,
      "grad_norm": 0.5549584627151489,
      "learning_rate": 1.0756011070040705e-07,
      "loss": 0.6098,
      "step": 160600
    },
    {
      "epoch": 2.897948507492976,
      "grad_norm": 0.31831446290016174,
      "learning_rate": 1.0568652222207432e-07,
      "loss": 0.5558,
      "step": 160650
    },
    {
      "epoch": 2.8988504606725867,
      "grad_norm": 0.45280903577804565,
      "learning_rate": 1.0382933775521574e-07,
      "loss": 0.5453,
      "step": 160700
    },
    {
      "epoch": 2.899752413852197,
      "grad_norm": 0.3324834108352661,
      "learning_rate": 1.0198855934526519e-07,
      "loss": 0.5946,
      "step": 160750
    },
    {
      "epoch": 2.9006543670318075,
      "grad_norm": 0.48741415143013,
      "learning_rate": 1.0016418901957924e-07,
      "loss": 0.577,
      "step": 160800
    },
    {
      "epoch": 2.9015563202114176,
      "grad_norm": 0.5140404105186462,
      "learning_rate": 9.835622878744899e-08,
      "loss": 0.5207,
      "step": 160850
    },
    {
      "epoch": 2.9024582733910282,
      "grad_norm": 0.36531341075897217,
      "learning_rate": 9.656468064009161e-08,
      "loss": 0.5797,
      "step": 160900
    },
    {
      "epoch": 2.903360226570639,
      "grad_norm": 0.4857596158981323,
      "learning_rate": 9.478954655064709e-08,
      "loss": 0.5893,
      "step": 160950
    },
    {
      "epoch": 2.904262179750249,
      "grad_norm": 0.4604131281375885,
      "learning_rate": 9.303082847417988e-08,
      "loss": 0.5875,
      "step": 161000
    },
    {
      "epoch": 2.9051641329298596,
      "grad_norm": 1.0051010847091675,
      "learning_rate": 9.12885283476722e-08,
      "loss": 0.6079,
      "step": 161050
    },
    {
      "epoch": 2.90606608610947,
      "grad_norm": 0.34815993905067444,
      "learning_rate": 8.956264809002246e-08,
      "loss": 0.5881,
      "step": 161100
    },
    {
      "epoch": 2.9069680392890804,
      "grad_norm": 0.41178765892982483,
      "learning_rate": 8.785318960205013e-08,
      "loss": 0.6298,
      "step": 161150
    },
    {
      "epoch": 2.907869992468691,
      "grad_norm": 0.5871095657348633,
      "learning_rate": 8.616015476648586e-08,
      "loss": 0.6291,
      "step": 161200
    },
    {
      "epoch": 2.9087719456483017,
      "grad_norm": 0.25435829162597656,
      "learning_rate": 8.448354544796977e-08,
      "loss": 0.5384,
      "step": 161250
    },
    {
      "epoch": 2.909673898827912,
      "grad_norm": 0.5019157528877258,
      "learning_rate": 8.282336349305475e-08,
      "loss": 0.5515,
      "step": 161300
    },
    {
      "epoch": 2.9105758520075224,
      "grad_norm": 0.27006590366363525,
      "learning_rate": 8.117961073019986e-08,
      "loss": 0.5935,
      "step": 161350
    },
    {
      "epoch": 2.9114778051871326,
      "grad_norm": 0.3491097092628479,
      "learning_rate": 7.955228896976863e-08,
      "loss": 0.5494,
      "step": 161400
    },
    {
      "epoch": 2.9123797583667432,
      "grad_norm": 0.32620182633399963,
      "learning_rate": 7.794140000403072e-08,
      "loss": 0.5289,
      "step": 161450
    },
    {
      "epoch": 2.913281711546354,
      "grad_norm": 0.5381113886833191,
      "learning_rate": 7.634694560715527e-08,
      "loss": 0.5969,
      "step": 161500
    },
    {
      "epoch": 2.914183664725964,
      "grad_norm": 0.24036966264247894,
      "learning_rate": 7.476892753521258e-08,
      "loss": 0.5946,
      "step": 161550
    },
    {
      "epoch": 2.9150856179055746,
      "grad_norm": 0.651648223400116,
      "learning_rate": 7.320734752616742e-08,
      "loss": 0.5964,
      "step": 161600
    },
    {
      "epoch": 2.915987571085185,
      "grad_norm": 0.42697253823280334,
      "learning_rate": 7.166220729988571e-08,
      "loss": 0.5878,
      "step": 161650
    },
    {
      "epoch": 2.9168895242647954,
      "grad_norm": 0.42153117060661316,
      "learning_rate": 7.013350855812283e-08,
      "loss": 0.6336,
      "step": 161700
    },
    {
      "epoch": 2.917791477444406,
      "grad_norm": 0.40134674310684204,
      "learning_rate": 6.862125298452538e-08,
      "loss": 0.5693,
      "step": 161750
    },
    {
      "epoch": 2.918693430624016,
      "grad_norm": 0.33222833275794983,
      "learning_rate": 6.712544224463601e-08,
      "loss": 0.5729,
      "step": 161800
    },
    {
      "epoch": 2.919595383803627,
      "grad_norm": 0.4405971169471741,
      "learning_rate": 6.564607798587863e-08,
      "loss": 0.574,
      "step": 161850
    },
    {
      "epoch": 2.920497336983237,
      "grad_norm": 0.44897228479385376,
      "learning_rate": 6.418316183756823e-08,
      "loss": 0.5807,
      "step": 161900
    },
    {
      "epoch": 2.9213992901628476,
      "grad_norm": 0.326312780380249,
      "learning_rate": 6.273669541090432e-08,
      "loss": 0.5609,
      "step": 161950
    },
    {
      "epoch": 2.922301243342458,
      "grad_norm": 0.5797123908996582,
      "learning_rate": 6.130668029896592e-08,
      "loss": 0.6251,
      "step": 162000
    },
    {
      "epoch": 2.922301243342458,
      "eval_loss": 0.7632912993431091,
      "eval_runtime": 124.2975,
      "eval_samples_per_second": 80.452,
      "eval_steps_per_second": 20.113,
      "step": 162000
    },
    {
      "epoch": 2.9232031965220684,
      "grad_norm": 0.3778602182865143,
      "learning_rate": 5.989311807671815e-08,
      "loss": 0.5413,
      "step": 162050
    },
    {
      "epoch": 2.924105149701679,
      "grad_norm": 0.6992141604423523,
      "learning_rate": 5.8496010301000715e-08,
      "loss": 0.5882,
      "step": 162100
    },
    {
      "epoch": 2.925007102881289,
      "grad_norm": 0.1708364486694336,
      "learning_rate": 5.711535851053273e-08,
      "loss": 0.5699,
      "step": 162150
    },
    {
      "epoch": 2.9259090560609,
      "grad_norm": 0.6272441148757935,
      "learning_rate": 5.57511642259112e-08,
      "loss": 0.6395,
      "step": 162200
    },
    {
      "epoch": 2.9268110092405104,
      "grad_norm": 0.3427466154098511,
      "learning_rate": 5.440342894960593e-08,
      "loss": 0.6235,
      "step": 162250
    },
    {
      "epoch": 2.927712962420121,
      "grad_norm": 0.4746146500110626,
      "learning_rate": 5.3072154165959566e-08,
      "loss": 0.601,
      "step": 162300
    },
    {
      "epoch": 2.928614915599731,
      "grad_norm": 0.477620929479599,
      "learning_rate": 5.175734134118593e-08,
      "loss": 0.623,
      "step": 162350
    },
    {
      "epoch": 2.929516868779342,
      "grad_norm": 0.39987677335739136,
      "learning_rate": 5.045899192336833e-08,
      "loss": 0.6516,
      "step": 162400
    },
    {
      "epoch": 2.930418821958952,
      "grad_norm": 0.49674156308174133,
      "learning_rate": 4.917710734245795e-08,
      "loss": 0.6203,
      "step": 162450
    },
    {
      "epoch": 2.9313207751385626,
      "grad_norm": 0.3855142295360565,
      "learning_rate": 4.791168901027043e-08,
      "loss": 0.601,
      "step": 162500
    },
    {
      "epoch": 2.932222728318173,
      "grad_norm": 0.4676473140716553,
      "learning_rate": 4.6662738320487616e-08,
      "loss": 0.5971,
      "step": 162550
    },
    {
      "epoch": 2.9331246814977834,
      "grad_norm": 0.48667845129966736,
      "learning_rate": 4.543025664865752e-08,
      "loss": 0.6036,
      "step": 162600
    },
    {
      "epoch": 2.934026634677394,
      "grad_norm": 0.45903831720352173,
      "learning_rate": 4.4214245352185986e-08,
      "loss": 0.6368,
      "step": 162650
    },
    {
      "epoch": 2.934928587857004,
      "grad_norm": 0.41882628202438354,
      "learning_rate": 4.301470577033839e-08,
      "loss": 0.5872,
      "step": 162700
    },
    {
      "epoch": 2.935830541036615,
      "grad_norm": 0.5657764673233032,
      "learning_rate": 4.1831639224242934e-08,
      "loss": 0.6131,
      "step": 162750
    },
    {
      "epoch": 2.9367324942162254,
      "grad_norm": 0.34607088565826416,
      "learning_rate": 4.066504701688068e-08,
      "loss": 0.5424,
      "step": 162800
    },
    {
      "epoch": 2.9376344473958356,
      "grad_norm": 0.2567366659641266,
      "learning_rate": 3.951493043309218e-08,
      "loss": 0.5599,
      "step": 162850
    },
    {
      "epoch": 2.938536400575446,
      "grad_norm": 0.6940194964408875,
      "learning_rate": 3.83812907395692e-08,
      "loss": 0.5644,
      "step": 162900
    },
    {
      "epoch": 2.9394383537550564,
      "grad_norm": 0.28258007764816284,
      "learning_rate": 3.726412918485633e-08,
      "loss": 0.5677,
      "step": 162950
    },
    {
      "epoch": 2.940340306934667,
      "grad_norm": 0.3248981833457947,
      "learning_rate": 3.6163446999356034e-08,
      "loss": 0.5575,
      "step": 163000
    },
    {
      "epoch": 2.9412422601142776,
      "grad_norm": 0.34032556414604187,
      "learning_rate": 3.507924539531193e-08,
      "loss": 0.6148,
      "step": 163050
    },
    {
      "epoch": 2.9421442132938878,
      "grad_norm": 0.36997008323669434,
      "learning_rate": 3.401152556682385e-08,
      "loss": 0.604,
      "step": 163100
    },
    {
      "epoch": 2.9430461664734984,
      "grad_norm": 0.3012199103832245,
      "learning_rate": 3.296028868983447e-08,
      "loss": 0.65,
      "step": 163150
    },
    {
      "epoch": 2.9439481196531085,
      "grad_norm": 0.3357280194759369,
      "learning_rate": 3.192553592213765e-08,
      "loss": 0.5444,
      "step": 163200
    },
    {
      "epoch": 2.944850072832719,
      "grad_norm": 0.5515990853309631,
      "learning_rate": 3.090726840336511e-08,
      "loss": 0.5313,
      "step": 163250
    },
    {
      "epoch": 2.9457520260123298,
      "grad_norm": 0.7588536739349365,
      "learning_rate": 2.9905487254999754e-08,
      "loss": 0.6311,
      "step": 163300
    },
    {
      "epoch": 2.9466539791919404,
      "grad_norm": 0.34464383125305176,
      "learning_rate": 2.8920193580365683e-08,
      "loss": 0.589,
      "step": 163350
    },
    {
      "epoch": 2.9475559323715506,
      "grad_norm": 0.468563050031662,
      "learning_rate": 2.795138846462153e-08,
      "loss": 0.5605,
      "step": 163400
    },
    {
      "epoch": 2.948457885551161,
      "grad_norm": 0.24377645552158356,
      "learning_rate": 2.699907297477544e-08,
      "loss": 0.5524,
      "step": 163450
    },
    {
      "epoch": 2.9493598387307713,
      "grad_norm": 0.5200637578964233,
      "learning_rate": 2.606324815966843e-08,
      "loss": 0.5726,
      "step": 163500
    },
    {
      "epoch": 2.950261791910382,
      "grad_norm": 0.41880226135253906,
      "learning_rate": 2.5143915049981038e-08,
      "loss": 0.6164,
      "step": 163550
    },
    {
      "epoch": 2.9511637450899926,
      "grad_norm": 0.48317256569862366,
      "learning_rate": 2.4241074658231665e-08,
      "loss": 0.5288,
      "step": 163600
    },
    {
      "epoch": 2.9520656982696027,
      "grad_norm": 0.3401990830898285,
      "learning_rate": 2.3354727978771583e-08,
      "loss": 0.5546,
      "step": 163650
    },
    {
      "epoch": 2.9529676514492134,
      "grad_norm": 0.35508424043655396,
      "learning_rate": 2.248487598778992e-08,
      "loss": 0.5905,
      "step": 163700
    },
    {
      "epoch": 2.9538696046288235,
      "grad_norm": 0.4261009097099304,
      "learning_rate": 2.163151964330701e-08,
      "loss": 0.6276,
      "step": 163750
    },
    {
      "epoch": 2.954771557808434,
      "grad_norm": 0.4882018566131592,
      "learning_rate": 2.0794659885176038e-08,
      "loss": 0.547,
      "step": 163800
    },
    {
      "epoch": 2.9556735109880448,
      "grad_norm": 0.5419556498527527,
      "learning_rate": 1.997429763507974e-08,
      "loss": 0.5956,
      "step": 163850
    },
    {
      "epoch": 2.956575464167655,
      "grad_norm": 0.6523439884185791,
      "learning_rate": 1.9170433796535382e-08,
      "loss": 0.6039,
      "step": 163900
    },
    {
      "epoch": 2.9574774173472655,
      "grad_norm": 0.38665613532066345,
      "learning_rate": 1.838306925488642e-08,
      "loss": 0.609,
      "step": 163950
    },
    {
      "epoch": 2.9583793705268757,
      "grad_norm": 0.5257670879364014,
      "learning_rate": 1.7612204877304195e-08,
      "loss": 0.6378,
      "step": 164000
    },
    {
      "epoch": 2.9583793705268757,
      "eval_loss": 0.7634154558181763,
      "eval_runtime": 164.6184,
      "eval_samples_per_second": 60.747,
      "eval_steps_per_second": 15.187,
      "step": 164000
    },
    {
      "epoch": 2.9592813237064863,
      "grad_norm": 0.24895763397216797,
      "learning_rate": 1.685784151278791e-08,
      "loss": 0.5785,
      "step": 164050
    },
    {
      "epoch": 2.960183276886097,
      "grad_norm": 0.5067369937896729,
      "learning_rate": 1.61199799921663e-08,
      "loss": 0.5634,
      "step": 164100
    },
    {
      "epoch": 2.9610852300657076,
      "grad_norm": 0.38286155462265015,
      "learning_rate": 1.5398621128089318e-08,
      "loss": 0.5903,
      "step": 164150
    },
    {
      "epoch": 2.9619871832453177,
      "grad_norm": 0.32016950845718384,
      "learning_rate": 1.4693765715031449e-08,
      "loss": 0.6031,
      "step": 164200
    },
    {
      "epoch": 2.962889136424928,
      "grad_norm": 0.4308640956878662,
      "learning_rate": 1.4005414529296712e-08,
      "loss": 0.6119,
      "step": 164250
    },
    {
      "epoch": 2.9637910896045385,
      "grad_norm": 0.30142226815223694,
      "learning_rate": 1.3333568329007006e-08,
      "loss": 0.6194,
      "step": 164300
    },
    {
      "epoch": 2.964693042784149,
      "grad_norm": 0.45405277609825134,
      "learning_rate": 1.2678227854103774e-08,
      "loss": 0.5716,
      "step": 164350
    },
    {
      "epoch": 2.9655949959637598,
      "grad_norm": 0.35057660937309265,
      "learning_rate": 1.2039393826357992e-08,
      "loss": 0.6021,
      "step": 164400
    },
    {
      "epoch": 2.96649694914337,
      "grad_norm": 0.6146546006202698,
      "learning_rate": 1.1417066949353517e-08,
      "loss": 0.5529,
      "step": 164450
    },
    {
      "epoch": 2.9673989023229805,
      "grad_norm": 0.40633469820022583,
      "learning_rate": 1.081124790849708e-08,
      "loss": 0.5651,
      "step": 164500
    },
    {
      "epoch": 2.9683008555025907,
      "grad_norm": 0.42290163040161133,
      "learning_rate": 1.0221937371014955e-08,
      "loss": 0.5947,
      "step": 164550
    },
    {
      "epoch": 2.9692028086822013,
      "grad_norm": 0.3841950595378876,
      "learning_rate": 9.64913598594963e-09,
      "loss": 0.5928,
      "step": 164600
    },
    {
      "epoch": 2.970104761861812,
      "grad_norm": 0.5872297286987305,
      "learning_rate": 9.092844384163135e-09,
      "loss": 0.6467,
      "step": 164650
    },
    {
      "epoch": 2.971006715041422,
      "grad_norm": 0.4284951984882355,
      "learning_rate": 8.553063178330378e-09,
      "loss": 0.5888,
      "step": 164700
    },
    {
      "epoch": 2.9719086682210327,
      "grad_norm": 0.2845149338245392,
      "learning_rate": 8.029792962949144e-09,
      "loss": 0.5943,
      "step": 164750
    },
    {
      "epoch": 2.972810621400643,
      "grad_norm": 0.21972930431365967,
      "learning_rate": 7.523034314326771e-09,
      "loss": 0.6105,
      "step": 164800
    },
    {
      "epoch": 2.9737125745802535,
      "grad_norm": 0.3333016633987427,
      "learning_rate": 7.032787790586803e-09,
      "loss": 0.5947,
      "step": 164850
    },
    {
      "epoch": 2.974614527759864,
      "grad_norm": 0.24726730585098267,
      "learning_rate": 6.559053931667336e-09,
      "loss": 0.5814,
      "step": 164900
    },
    {
      "epoch": 2.9755164809394743,
      "grad_norm": 0.5452300906181335,
      "learning_rate": 6.10183325932101e-09,
      "loss": 0.5991,
      "step": 164950
    },
    {
      "epoch": 2.976418434119085,
      "grad_norm": 0.5631106495857239,
      "learning_rate": 5.661126277113348e-09,
      "loss": 0.5406,
      "step": 165000
    },
    {
      "epoch": 2.977320387298695,
      "grad_norm": 0.4615643322467804,
      "learning_rate": 5.23693347042109e-09,
      "loss": 0.5334,
      "step": 165050
    },
    {
      "epoch": 2.9782223404783057,
      "grad_norm": 0.42804041504859924,
      "learning_rate": 4.82925530643219e-09,
      "loss": 0.5627,
      "step": 165100
    },
    {
      "epoch": 2.9791242936579163,
      "grad_norm": 0.32831084728240967,
      "learning_rate": 4.438092234149149e-09,
      "loss": 0.6023,
      "step": 165150
    },
    {
      "epoch": 2.980026246837527,
      "grad_norm": 0.3777186870574951,
      "learning_rate": 4.063444684384021e-09,
      "loss": 0.6098,
      "step": 165200
    },
    {
      "epoch": 2.980928200017137,
      "grad_norm": 0.2604227662086487,
      "learning_rate": 3.7053130697567437e-09,
      "loss": 0.6252,
      "step": 165250
    },
    {
      "epoch": 2.9818301531967477,
      "grad_norm": 0.29726341366767883,
      "learning_rate": 3.363697784700137e-09,
      "loss": 0.5672,
      "step": 165300
    },
    {
      "epoch": 2.982732106376358,
      "grad_norm": 0.4255467355251312,
      "learning_rate": 3.0385992054565713e-09,
      "loss": 0.6007,
      "step": 165350
    },
    {
      "epoch": 2.9836340595559685,
      "grad_norm": 0.5897331833839417,
      "learning_rate": 2.7300176900779683e-09,
      "loss": 0.5775,
      "step": 165400
    },
    {
      "epoch": 2.984536012735579,
      "grad_norm": 0.32219213247299194,
      "learning_rate": 2.437953578420804e-09,
      "loss": 0.6024,
      "step": 165450
    },
    {
      "epoch": 2.9854379659151893,
      "grad_norm": 0.36259540915489197,
      "learning_rate": 2.1624071921544364e-09,
      "loss": 0.6143,
      "step": 165500
    },
    {
      "epoch": 2.9863399190948,
      "grad_norm": 0.512961745262146,
      "learning_rate": 1.9033788347561087e-09,
      "loss": 0.5837,
      "step": 165550
    },
    {
      "epoch": 2.98724187227441,
      "grad_norm": 0.2131374180316925,
      "learning_rate": 1.6608687915076192e-09,
      "loss": 0.5943,
      "step": 165600
    },
    {
      "epoch": 2.9881438254540207,
      "grad_norm": 0.8153584003448486,
      "learning_rate": 1.434877329500317e-09,
      "loss": 0.5629,
      "step": 165650
    },
    {
      "epoch": 2.9890457786336313,
      "grad_norm": 0.9198395013809204,
      "learning_rate": 1.2254046976334364e-09,
      "loss": 0.5812,
      "step": 165700
    },
    {
      "epoch": 2.9899477318132415,
      "grad_norm": 0.5647199749946594,
      "learning_rate": 1.0324511266107672e-09,
      "loss": 0.5917,
      "step": 165750
    },
    {
      "epoch": 2.990849684992852,
      "grad_norm": 0.5616106986999512,
      "learning_rate": 8.560168289439841e-10,
      "loss": 0.5383,
      "step": 165800
    },
    {
      "epoch": 2.9917516381724623,
      "grad_norm": 0.3993832767009735,
      "learning_rate": 6.961019989526474e-10,
      "loss": 0.5827,
      "step": 165850
    },
    {
      "epoch": 2.992653591352073,
      "grad_norm": 1.1234815120697021,
      "learning_rate": 5.527068127575418e-10,
      "loss": 0.5648,
      "step": 165900
    },
    {
      "epoch": 2.9935555445316835,
      "grad_norm": 0.5265148282051086,
      "learning_rate": 4.2583142829066836e-10,
      "loss": 0.6019,
      "step": 165950
    },
    {
      "epoch": 2.9944574977112937,
      "grad_norm": 0.4233652949333191,
      "learning_rate": 3.1547598528691714e-10,
      "loss": 0.5441,
      "step": 166000
    },
    {
      "epoch": 2.9944574977112937,
      "eval_loss": 0.7633609175682068,
      "eval_runtime": 184.9876,
      "eval_samples_per_second": 54.058,
      "eval_steps_per_second": 13.514,
      "step": 166000
    }
  ],
  "logging_steps": 50,
  "max_steps": 166308,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 2000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9.270628024182374e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
