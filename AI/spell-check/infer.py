import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel

# ===== C·∫§U H√åNH =====
BASE_MODEL = "google/flan-t5-small"  # model g·ªëc
LORA_PATH = "checkpoints-t5-laptop/checkpoint-166000"      # ƒë∆∞·ªùng d·∫´n ƒë·∫øn adapter ƒë√£ train
PREFIX = "fix: "                     # prefix gi·ªëng l√∫c train
MAX_LEN = 128

# ===== LOAD MODEL & TOKENIZER =====
print("üîÑ Loading model...")
tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)

# Load base model
base_model = AutoModelForSeq2SeqLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Load LoRA adapter
model = PeftModel.from_pretrained(base_model, LORA_PATH)
model.eval()
print("‚úÖ Model loaded!\n")

# ===== H√ÄM INFERENCE =====
def correct_text(text, num_beams=4, max_length=MAX_LEN):
    """
    S·ª≠a l·ªói ch√≠nh t·∫£/ng·ªØ ph√°p cho text ƒë·∫ßu v√†o
    
    Args:
        text: c√¢u c·∫ßn s·ª≠a
        num_beams: s·ªë beam cho beam search (c√†ng cao c√†ng ch√≠nh x√°c nh∆∞ng ch·∫≠m)
        max_length: ƒë·ªô d√†i t·ªëi ƒëa output
    
    Returns:
        c√¢u ƒë√£ ƒë∆∞·ª£c s·ª≠a
    """
    # Th√™m prefix
    input_text = PREFIX + text
    
    # Tokenize
    inputs = tokenizer(
        input_text,
        max_length=max_length,
        truncation=True,
        return_tensors="pt"
    ).to(model.device)
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_beams=num_beams,
            early_stopping=True,
            do_sample=False  # deterministic ƒë·ªÉ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh
        )
    
    # Decode
    corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return corrected

# ===== H√ÄM INFERENCE BATCH =====
def correct_batch(texts, batch_size=16, num_beams=4, max_length=MAX_LEN):
    """
    S·ª≠a nhi·ªÅu c√¢u c√πng l√∫c (nhanh h∆°n)
    
    Args:
        texts: list c√°c c√¢u c·∫ßn s·ª≠a
        batch_size: s·ªë c√¢u x·ª≠ l√Ω m·ªói l·∫ßn
        num_beams: s·ªë beam cho beam search
        max_length: ƒë·ªô d√†i t·ªëi ƒëa output
    
    Returns:
        list c√°c c√¢u ƒë√£ s·ª≠a
    """
    results = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        input_texts = [PREFIX + text for text in batch]
        
        # Tokenize batch
        inputs = tokenizer(
            input_texts,
            max_length=max_length,
            truncation=True,
            padding=True,
            return_tensors="pt"
        ).to(model.device)
        
        # Generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                num_beams=num_beams,
                early_stopping=True,
                do_sample=False
            )
        
        # Decode
        corrected = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        results.extend(corrected)
    
    return results

# ===== DEMO =====
if __name__ == "__main__":
    # Test v·ªõi 1 c√¢u
    test_sentences = [
        "I has a cat and two dog .",
        "She dont like apple .",
        "They was going to the store yesterday .",
        "He can plays guitar very good .",
    ]
    
    print("=" * 60)
    print("SINGLE SENTENCE INFERENCE")
    print("=" * 60)
    
    for sent in test_sentences:
        corrected = correct_text(sent, num_beams=4)
        print(f"Input:  {sent}")
        print(f"Output: {corrected}")
        print("-" * 60)
    
    print("\n" + "=" * 60)
    print("BATCH INFERENCE (faster for multiple sentences)")
    print("=" * 60)
    
    corrected_batch = correct_batch(test_sentences, batch_size=4, num_beams=4)
    for orig, corr in zip(test_sentences, corrected_batch):
        print(f"Input:  {orig}")
        print(f"Output: {corr}")
        print("-" * 60)
    
    # Interactive mode
    print("\n" + "=" * 60)
    print("INTERACTIVE MODE (type 'quit' to exit)")
    print("=" * 60)
    
    while True:
        text = input("\nEnter text to correct: ").strip()
        if text.lower() == 'quit':
            break
        if text:
            corrected = correct_text(text)
            print(f"‚úÖ Corrected: {corrected}")